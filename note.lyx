#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass scrbook
\begin_preamble
\usepackage{indentfirst}
\usepackage{helvet}

\renewcommand\[{\begin{equation}}
\renewcommand\]{\end{equation}}
\end_preamble
\use_default_options true
\begin_modules
enumitem
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format pdf4
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Reading Notes of
\begin_inset Newline newline
\end_inset

Pattern Classification and
\begin_inset Newline newline
\end_inset

Machine Learning
\end_layout

\begin_layout Author
Tianyi Cui
\end_layout

\begin_layout Date
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Standard
Different kinds of tasks of machine learning:
\end_layout

\begin_layout Itemize
supervised learning: known input and target vectors
\end_layout

\begin_layout Itemize
classification: output is one of a finite number of discrete categories
\end_layout

\begin_deeper
\begin_layout Itemize
regression: output is one or more continuous variables
\end_layout

\end_deeper
\begin_layout Itemize
unsupervised learning: no corresponding target values
\end_layout

\begin_deeper
\begin_layout Itemize
clustering: discover groups of similar examples within the data
\end_layout

\begin_layout Itemize
density estimation: determine the distribution of data within the input
 space
\end_layout

\begin_layout Itemize
dimension reduction
\end_layout

\end_deeper
\begin_layout Itemize
reinforcement learning: finding suitable actions to take in a given situation
 in order to maximize a reward
\end_layout

\begin_layout Section
Example: Polynomial Curve Fitting
\end_layout

\begin_layout Standard
In regression problems, we can use a polynomial function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
y(x,\mathbf{w})=w_{0}+w_{1}+w_{2}x^{2}+\ldots+w_{M}x^{M}=\sum_{j=0}^{M}w_{j}x^{j}\label{eq:y(x,w)}
\end{equation}

\end_inset

 to fit the underlying function.
\end_layout

\begin_layout Standard
We need to minimize the 
\emph on
error function
\emph default
 
\begin_inset Formula 
\begin{equation}
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\{y(x_{n},\mathbf{w})-t_{n}\}^{2}\label{eq:E(w)}
\end{equation}

\end_inset

in which unique solution 
\begin_inset Formula $\mathbf{w^{*}}$
\end_inset

 can be found in closed form.
\end_layout

\begin_layout Standard
The root-mean-square (RMS) error is defined by 
\begin_inset Formula 
\[
E_{RMS}=\sqrt{2E(\mathbf{w^{*})/N}}
\]

\end_inset


\end_layout

\begin_layout Standard
When 
\begin_inset Formula $M$
\end_inset

 is large, 
\emph on
over-fitting
\emph default
 occurs, i.e.
 
\begin_inset Formula $E_{RMS}$
\end_inset

 against test data becomes large.
 One technique to control over-fitting is 
\emph on
regularization
\emph default
, by adding a penalty term to the error function 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:E(w)"

\end_inset

 in order to discourage the coefficients from reaching large values:
\begin_inset Formula 
\begin{equation}
\widetilde{E}(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\{y(x_{n},\mathbf{w})-t_{n}\}^{2}+\frac{\lambda}{2}\left\Vert \mathbf{w}\right\Vert ^{2}\label{eq:regularized E(w)}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Probability Theory
\end_layout

\begin_layout Standard
Equations for probability:
\end_layout

\begin_layout Itemize
Sum rule
\begin_inset Formula 
\[
p(X)=\sum_{Y}p(X,Y)
\]

\end_inset


\end_layout

\begin_layout Itemize
Product rule
\begin_inset Formula 
\[
p(X,Y)=p(Y|X)p(X)
\]

\end_inset


\end_layout

\begin_layout Itemize
Bayes' theorem
\begin_inset Formula 
\begin{equation}
p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}\label{eq:bayes}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The denominator in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:bayes"

\end_inset

 can be expressed in terms of the quantities appearing in the numerator
\begin_inset Formula 
\[
p(X)=\sum_{Y}p(X|Y)p(Y)
\]

\end_inset


\end_layout

\begin_layout Standard
We can view the denominator in Bayes' theorem as being the normalization
 constant required to ensure that the sum of the conditional probability
 on the left-hand side of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:bayes"

\end_inset

 over all values of 
\begin_inset Formula $Y$
\end_inset

 equals 
\begin_inset Formula $1$
\end_inset

.
\end_layout

\begin_layout Standard
Before any observation, we have a probability of a certain event 
\begin_inset Formula $Y$
\end_inset

, this is called 
\emph on
prior probability
\emph default
 
\begin_inset Formula $p(Y)$
\end_inset

, after some observation 
\begin_inset Formula $X$
\end_inset

, the probability of event 
\begin_inset Formula $Y$
\end_inset

 becomes the 
\emph on
posterior probability
\emph default
 
\begin_inset Formula $p(Y|X)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are said to be 
\emph on
independent
\emph default
 if 
\begin_inset Formula $p(X,Y)=p(X)p(Y)$
\end_inset

, which is equivalent to 
\begin_inset Formula $P(Y|X)=p(Y)$
\end_inset

.
\end_layout

\begin_layout Subsection
Probability densities
\end_layout

\begin_layout Standard
If the probability that 
\begin_inset Formula $x$
\end_inset

 will lie in 
\begin_inset Formula $(a,b)$
\end_inset

 is given by 
\begin_inset Formula 
\[
p(x\in(a,b))=\int_{a}^{b}p(x)\mathrm{d}x
\]

\end_inset

then 
\begin_inset Formula $p(x)$
\end_inset

 is called the 
\emph on
probability density
\emph default
 over 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
Apparently 
\begin_inset Formula $p(x)\geq0$
\end_inset

 and 
\begin_inset Formula $\int_{-\infty}^{\infty}p(x)\mathrm{d}x=1$
\end_inset

.
\end_layout

\begin_layout Standard
Under a nonlinear change of variable, a probability density transforms different
ly from a simple function, due to the Jacobian factor.
 If 
\begin_inset Formula $x=g(y)$
\end_inset

, since 
\begin_inset Formula $p_{x}(x)\mathrm{d}x=p_{y}(y)\mathrm{d}y$
\end_inset

, hence
\begin_inset Formula 
\begin{eqnarray}
p_{y}(y) & = & p_{x}(x)\left|\frac{\mathrm{d}x}{\mathrm{d}y}\right|\nonumber \\
 & = & p_{x}(g(y))\left|g'(y)\right|
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
cumulative distribution function
\emph default
 
\begin_inset Formula 
\[
P(z)=\int_{-\infty}^{z}p(x)\mathrm{d}x
\]

\end_inset


\end_layout

\begin_layout Standard
For several continuous variables 
\begin_inset Formula $x_{1},\ldots,x_{D}$
\end_inset

, denoted collectively by the vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

, then we can define a joint probability density 
\begin_inset Formula $p(\mathbf{x})$
\end_inset

 such that 
\begin_inset Formula $p\left(\mathbf{x}\in(\mathbf{x}_{0},\mathbf{x}_{0}+\delta\mathbf{x})\right)=p(\mathbf{x}_{0})\delta\mathbf{x}$
\end_inset

.
\end_layout

\begin_layout Subsection
Expectations and covariances
\end_layout

\begin_layout Standard
The average value of some function 
\begin_inset Formula $f(x)$
\end_inset

 under a probability distribution 
\begin_inset Formula $p(x)$
\end_inset

 is called the 
\emph on
expectation
\emph default
 of 
\begin_inset Formula $f(x)$
\end_inset

 and denoted by 
\begin_inset Formula $\mathbb{E}[f]$
\end_inset

.
 For a discrete distribution,
\begin_inset Formula 
\[
\mathbb{E}[f]=\sum_{x}p(x)f(x)
\]

\end_inset

For continuous variables,
\begin_inset Formula 
\[
\mathbb{E}[f]=\int p(x)f(x)\mathrm{d}x
\]

\end_inset

In either case, the expectation can be approximated given 
\begin_inset Formula $N$
\end_inset

 samples,
\begin_inset Formula 
\[
\mathbb{E}[f]\simeq\frac{1}{N}\sum_{n=1}^{N}f(x_{n})
\]

\end_inset


\end_layout

\begin_layout Standard
When considering expectations of functions of several variables, we use
 subscript to indicate which variable is being averaged over, e.g.
 
\begin_inset Formula $\mathbb{E}_{x}[f(x,y)]$
\end_inset

 is a function of 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Standard
We can also consider 
\emph on
conditional expectation
\begin_inset Formula 
\[
\mathbb{E}_{x}[f|y]=\sum_{x}p(x|y)f(x)
\]

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
variance
\emph default
 of 
\begin_inset Formula $f(x)$
\end_inset

 is defined by
\begin_inset Formula 
\begin{eqnarray}
\mathrm{var}[f] & = & \mathbb{E}\left[(f(x)-\mathbb{E}[f(x)]\right]{}^{2}\\
 & = & \mathbb{E}\left[f(x)^{2}\right]-\mathbb{E}\left[f(x)\right]{}^{2}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
covariance
\emph default
 of two random variable 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 is defined by
\begin_inset Formula 
\begin{eqnarray}
\mathrm{cov}[x,y] & = & \mathbb{E}_{x,y}\left[\{x-\mathbb{E}[x]\}\{y-\mathbb{E}[y]\}\right]\nonumber \\
 & = & \mathbb{E}_{x,y}[xy]-\mathbb{E}[x]\mathbb{E}[y]
\end{eqnarray}

\end_inset

which expresses the extent to which 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 vary together.
 If they are independent, then the covariance vanishes.
\end_layout

\begin_layout Standard
In the case of two vectors of random variables 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}$
\end_inset

, the covariance is a matrix
\begin_inset Formula 
\begin{eqnarray}
\mathrm{cov}[\mathbf{x},\mathbf{y}] & = & \mathbb{E}_{x,y}\left[\left\{ \mathbf{x}-\mathbb{E}[\mathbf{x]}\right\} \left\{ \mathbf{y}^{\mathrm{T}}-\mathbb{E}[\mathbf{y^{\mathrm{T}}]}\right\} \right]\nonumber \\
 & = & \mathbb{E}_{x,y}[\mathbf{x}\mathbf{y}^{\mathrm{T}}]-\mathbb{E}[\mathbf{x]}\mathbb{E}[\mathbf{y^{\mathrm{T}}]}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Subsection
Bayesian probabilities
\end_layout

\begin_layout Standard
In the 
\emph on
classical
\emph default
 or 
\emph on
frequentist
\emph default
 interpretation of probability, probabilities is viewed in terms of the
 frequencies of random, repeatable events.
 In the more general 
\emph on
Beyesian
\emph default
 view, probabilities provide a quantification of uncertainty, so we can
 say the probability of an uncertain event, like whether the Arctic ice
 cap will have disappeared by the end of the century, which is not events
 that can be repeated.
\end_layout

\begin_layout Standard
In the polynomial curve fitting example, we assume the parameters 
\begin_inset Formula $\mathbf{w}$
\end_inset

 have a prior probability distribution 
\begin_inset Formula $p(\mathbf{w})$
\end_inset

, then given the observed data 
\begin_inset Formula $\mathcal{D}$
\end_inset

, the posterior probability is
\begin_inset Formula 
\[
p(\mathbf{w}|\mathcal{D})=\frac{p(\mathcal{D}|\mathbf{w})p(\mathbf{w)}}{p(\mathcal{D})}
\]

\end_inset

where the quantity 
\begin_inset Formula $p(\mathcal{D}|\mathbf{w})$
\end_inset

 is called the 
\emph on
likelihood function
\emph default
, which expresses how probable the observed data set is for different settings
 of the parameter vector 
\begin_inset Formula $\mathbf{w}$
\end_inset

.
 The likelihood is not a probability distribution over 
\begin_inset Formula $\mathbf{w}$
\end_inset

, and its integral does not necessarily equal one.
\end_layout

\begin_layout Standard
Given the definition of likelihood, we can state Bayes' theorem in words
\begin_inset Formula 
\[
\text{posterior}\propto\text{likelihood}\times\text{prior}
\]

\end_inset

where all of these quantities are viewed as functions of 
\begin_inset Formula $\mathbf{w}$
\end_inset

.
\end_layout

\begin_layout Standard
In the likelihood function 
\begin_inset Formula $p(\mathcal{D}|\mathbf{w})$
\end_inset

, in the frequentist setting, 
\begin_inset Formula $\mathbf{w}$
\end_inset

 is considered to be a fixed parameter, whose value is determines by some
 form of `estimator', and error bars on this estimate are obtained by considerin
g the distribution of possible data sets 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
 By contrast, from Bayesian viewpoint there is only a single data set 
\begin_inset Formula $\mathcal{D}$
\end_inset

 (the one actually observed), and the uncertainty in the parameters is expressed
 through a probability distribution over 
\begin_inset Formula $\mathbf{w}$
\end_inset

.
\end_layout

\begin_layout Subsection
The Gaussian distribution
\end_layout

\begin_layout Standard
The Gaussian distribution on a single real-valued variable 
\begin_inset Formula $x$
\end_inset

 is defined by
\begin_inset Formula 
\[
\mathcal{N}(x|\mu,\sigma^{2})=\frac{1}{(2\pi\sigma^{2})^{1/2}}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} 
\]

\end_inset

which is governed by two parameters: 
\begin_inset Formula $\mu$
\end_inset

 the 
\emph on
mean
\emph default
 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

 the 
\emph on
variance
\emph default
.
 
\begin_inset Formula $\sigma$
\end_inset

 is called the 
\emph on
standard deviation
\emph default
, and 
\begin_inset Formula $\beta=1/\sigma^{2}$
\end_inset

 is called the 
\emph on
precision
\emph default
.
 The mean of 
\begin_inset Formula $x$
\end_inset

 is given by 
\begin_inset Formula $\mathbb{E}[x]=\mu$
\end_inset

 and the variance of 
\begin_inset Formula $x$
\end_inset

 is given by 
\begin_inset Formula $\mathrm{var}[x]=\mathbb{E}[x^{2}]-\mathbb{E}[x]^{2}=\sigma^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
The Gaussian distribution defined over a 
\begin_inset Formula $D$
\end_inset

-dimensional vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 of continuous variables is given by
\begin_inset Formula 
\[
\mathcal{N}(\mathbf{x}|\mathbf{\mu},\mathbf{\Sigma})=\frac{1}{(2\pi)^{D/2}}\frac{1}{\left|\bm{\Sigma}\right|^{1/2}}\exp\left\{ -\frac{1}{2}(\mathbf{x}-\bm{\mu})^{\mathrm{T}}\bm{\Sigma}^{-1}(\mathbf{x}-\bm{\mu})\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
Suppose we have a data set of observation 
\begin_inset Formula $\textbf{\textsf{x}}=(x_{1},\ldots,x_{N})^{\mathrm{T}}$
\end_inset

 which is 
\emph on
independent and identically distributed
\emph default
 (often abbreviated to i.i.d.) from a Gaussian distribution.
 The likelihood of the data set, which is a function of 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

, is in the form
\begin_inset Formula 
\[
p\left(\textbf{\textsf{x}}|\mu,\sigma^{2}\right)=\prod_{n=1}^{N}\mathcal{N}(x_{n}|\mu,\sigma^{2})
\]

\end_inset


\end_layout

\begin_layout Standard
One common criterion for determining the parameters in a probability distributio
n using an observed data set is to find the parameter values that maximize
 the likelihood function.
\end_layout

\begin_layout Standard
In practice, for mathematical and numerical reasons, it's more convenient
 to maximize the log of the likelihood functions
\begin_inset Formula 
\begin{equation}
\ln p\left(\textbf{\textsf{x}}|\mu,\sigma^{2}\right)=-\frac{1}{2\sigma^{2}}\sum_{n=1}^{N}(x_{n}-\mu)^{2}-\frac{N}{2}\ln\sigma^{2}-\frac{N}{2}\ln(2\pi)\label{eq:ln p(x|mu,sigma^2)}
\end{equation}

\end_inset

 
\end_layout

\begin_layout Standard
Maximizing 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(x|mu,sigma^2)"

\end_inset

 with respect to 
\begin_inset Formula $\mu$
\end_inset

 gives the maximum likelihood solution
\begin_inset Formula 
\[
\mu_{\mathrm{ML}}=\frac{1}{N}\sum_{n=1}^{N}x_{n}
\]

\end_inset

which is the 
\emph on
sample mean
\emph default
.
 Similarly, Maximize 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(x|mu,sigma^2)"

\end_inset

 with respect to 
\begin_inset Formula $\sigma^{2}$
\end_inset

gives
\begin_inset Formula 
\[
\sigma_{\mathrm{ML}}^{2}=\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu_{\mathrm{ML}})^{2}
\]

\end_inset

which is the 
\emph on
sample variance.
\end_layout

\begin_layout Standard
The maximum likelihood approach systematically underestimates the variance
 of the distribution.
 This is an example of a phenomenon called 
\emph on
bias
\emph default
 and is related to the problem of over-fitting encountered in the context
 of polynomial curve fitting.
 First, we note that 
\begin_inset Formula $\mu_{\mathrm{ML}}$
\end_inset

 and 
\begin_inset Formula $\sigma_{\mathrm{ML}}^{2}$
\end_inset

 are functions of the data set values 
\begin_inset Formula $x_{1},\ldots,x_{N}$
\end_inset

.
 Consider the expectations of these quantities with respect to the data
 set values, which themselves come from a Gaussian distribution with parameters
 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset


\begin_inset Formula 
\begin{eqnarray}
\mathbb{E}[\mu_{\mathrm{ML}}] & = & \mu\\
\mathbb{E}[\sigma_{\mathrm{ML}}^{2}] & = & \left(\frac{N-1}{N}\right)\sigma^{2}\label{eq:E[sigma_ML^2]}
\end{eqnarray}

\end_inset

so on average the maximum likelihood approach will underestimate the true
 variance by a factor 
\begin_inset Formula $(N-1)/N$
\end_inset

.
\end_layout

\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:E[sigma_ML^2]"

\end_inset

 we see the following estimate for the variance parameter is unbiased
\begin_inset Formula 
\[
\widetilde{\sigma}^{2}=\frac{N}{N-1}\sigma_{\mathrm{ML}}^{2}=\frac{1}{N-1}\sum_{n=1}^{N}(x_{n}-\mu_{\mathrm{ML}})^{2}
\]

\end_inset

 this result arises automatically when we adopt a Bayesian approach (Section
 10.1.3).
\end_layout

\begin_layout Subsection
Curve fitting re-visited
\end_layout

\begin_layout Standard
The goal of curve fitting problem is to make predictions for the target
 variable 
\begin_inset Formula $t$
\end_inset

 given some new value of the input variable 
\begin_inset Formula $x$
\end_inset

 on the basis of a set of training data 
\begin_inset Formula $\textbf{\textsf{x}}=(x_{1},\ldots,x_{N})^{\mathrm{T}}$
\end_inset

 and 
\begin_inset Formula $\textbf{\textsf{t}}=(t_{1},\ldots,t_{N})^{\mathrm{T}}$
\end_inset

.
 We can express out uncertainty over the value of the target variable using
 a probability distribution.
 Assume that, given the value of 
\begin_inset Formula $x$
\end_inset

, the corresponding value of 
\begin_inset Formula $t$
\end_inset

 has a Gaussian distribution with a mean equal to the value 
\begin_inset Formula $y(x,\mathbf{w})$
\end_inset

 given by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:y(x,w)"

\end_inset

.
 Thus we have
\begin_inset Formula 
\begin{equation}
p(t|x,\mathbf{w},\beta)=\mathcal{N}\left(t|y(x,\mathbf{w}),\beta^{-1}\right)\label{eq:p(t|x,w,beta)}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\beta$
\end_inset

 is the precision parameter.
\end_layout

\begin_layout Standard
Use the training data 
\begin_inset Formula $\{\textbf{\textsf{x}},\textbf{\textsf{t}}\}$
\end_inset

 to determine the values of the unknown parameters 
\begin_inset Formula $\mathbf{w}$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 by maximum likelihood, the likelihood function is
\begin_inset Formula 
\[
p(\textbf{\textsf{t}}|\textbf{\textsf{x}},\mathbf{w},\beta)=\prod_{n=1}^{N}\mathcal{N}\left(t_{n}|y(x_{n},\mathbf{w}),\beta^{-1}\right)
\]

\end_inset

and its logarithm is
\begin_inset Formula 
\begin{equation}
\ln p(\textbf{\textsf{t}}|\textbf{\textsf{x}},\mathbf{w},\beta)=-\frac{\beta}{2}\sum_{n=1}^{N}\left\{ y(x_{n},\mathbf{w})-t_{n}\right\} ^{2}+\frac{N}{2}\ln\beta-\frac{N}{2}\ln(2\pi)\label{eq:ln p(t|x,w,beta)}
\end{equation}

\end_inset

Maximizing 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(t|x,w,beta)"

\end_inset

 with respect to 
\begin_inset Formula $\mathbf{w}$
\end_inset

 gives us 
\begin_inset Formula $\mathbf{w}_{\mathrm{ML}}$
\end_inset

, which is the same as minimize the 
\emph on
sum-of-squares error function
\emph default
 defined by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:E(w)"

\end_inset

.
\end_layout

\begin_layout Standard
Maximizing 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(t|x,w,beta)"

\end_inset

 with respect to 
\begin_inset Formula $\beta$
\end_inset

 gives
\begin_inset Formula 
\[
\frac{1}{\beta_{\mathrm{ML}}}=\frac{1}{N}\sum_{n=1}^{N}\left\{ y(x_{n},\mathrm{w}_{\mathrm{ML}})-t_{n}\right\} ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Having determined the parameters 
\begin_inset Formula $\mathbf{w}$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

, we can now make predictions for new values of 
\begin_inset Formula $x$
\end_inset

, and in probabilistic model, these are expressed in terms of the 
\emph on
predictive distribution
\emph default
 that gives the probability distribution over 
\emph on
t
\begin_inset Formula 
\[
p(t|x,\mathrm{w}_{\mathrm{ML}},\beta_{\mathrm{ML}})=\mathcal{N}\left(t|y(x,\mathbf{w}_{ML}),\beta_{\mathrm{ML}}^{-1}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
In a more Bayesian approach, we introduce a Gaussian prior distribution
 over the polynomial coefficients 
\begin_inset Formula $\mathbf{w}$
\end_inset


\begin_inset Formula 
\begin{equation}
p(\mathbf{w}|\alpha)=\mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I})=\left(\frac{\alpha}{2\pi}\right)^{(M+1)/2}\exp\left\{ -\frac{\alpha}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}\right\} \label{eq:p(w|alpha)}
\end{equation}

\end_inset

where 
\begin_inset Formula $\alpha$
\end_inset

 is the precision of the distribution and 
\begin_inset Formula $M+1$
\end_inset

 is the number of elements in 
\begin_inset Formula $\mathbf{w}$
\end_inset

.
 Values such as 
\begin_inset Formula $\alpha$
\end_inset

, which controls the distribution of model parameters, are called 
\emph on
hyperparameters
\emph default
.
\end_layout

\begin_layout Standard
Using Bayes' theorem
\begin_inset Formula 
\begin{equation}
p(\mathbf{w}|\textbf{\textsf{x}},\textbf{\textsf{t }},\alpha,\beta)\propto p(\textsf{t }|\textsf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha)\label{eq:p(w|x,t,alpha,beta)}
\end{equation}

\end_inset

We can now determine 
\begin_inset Formula $\mathbf{w}$
\end_inset

 by finding the most probable value of 
\begin_inset Formula $\mathbf{w}$
\end_inset

 given the data, in other words by maximizing the posterior distribution.
 This technique is called 
\emph on
maximum posterior
\emph default
, or simply 
\emph on
MAP
\emph default
.
\end_layout

\begin_layout Standard
Taking the negative logarithm of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(w|x,t,alpha,beta)"

\end_inset

 and combining with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(t|x,w,beta)"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(w|alpha)"

\end_inset

, we find that the maximum of the posterior is given by the minimum of
\begin_inset Formula 
\[
\frac{\beta}{2}\sum_{n=1}^{N}\left\{ y(x_{n},\mathbf{w})-t_{n}\right\} ^{2}+\frac{\alpha}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}
\]

\end_inset

Thus we see that maximizing the posterior distribution is equivalent to
 minimizing the regularized sum-of-squares error function 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:regularized E(w)"

\end_inset

.
\end_layout

\begin_layout Subsection
Bayesian curve fitting
\end_layout

\begin_layout Standard
Although we have included a prior distribution 
\begin_inset Formula $p(\mathbf{w}|\alpha)$
\end_inset

, we are still making a point estimate of 
\begin_inset Formula $\mathbf{w}$
\end_inset

 and so this does not yet amount to a Bayesian treatment.
 In a fully Bayesian approach, we should consistently apply the sum and
 product rules of probability, which requires, as we shall see shortly,
 that we integrate over all values of 
\begin_inset Formula $\mathbf{w}$
\end_inset

.
 Such marginalizations lie at the heart of Bayesian methods for pattern
 recognition.
\end_layout

\begin_layout Standard
In curve fitting, we are given the training data 
\begin_inset Formula $\{\textbf{\textsf{x}},\textbf{\textsf{t}}\}$
\end_inset

, along with a new test point 
\begin_inset Formula $x$
\end_inset

, and our goal is to predict the value of 
\begin_inset Formula $t$
\end_inset

.
 Assuming the parameters 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 are fixed and known in advance by now, we wish the evaluate the predictive
 distribution 
\begin_inset Formula $p(t|\textbf{\textsf{x}},\textbf{\textsf{t}})$
\end_inset

.
 Using the product rules of probability
\begin_inset Formula 
\begin{equation}
p(t|x,\textbf{\textsf{x}},\textbf{\textsf{t}})=\int p(t|x,\mathbf{w})p(\mathbf{w}|\textbf{\textsf{x}},\textbf{\textsf{t}})\mathrm{d}\mathbf{w}\label{eq:p(t|x,x,t)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Here 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $p(t|x,\mathbf{w})$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 and 
\begin_inset Formula $p(\mathbf{w}|\textbf{\textsf{x}},\textbf{\textsf{t}})$
\end_inset

 are given by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(t|x,w,beta)"

\end_inset

 and normalizing the right-hand side of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(w|x,t,alpha,beta)"

\end_inset

.
\end_layout

\begin_layout Standard
The calculation and integration in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(t|x,x,t)"

\end_inset

 can be performed analytically with the result in a Gaussian distribution
\begin_inset Formula 
\begin{equation}
p(t|x,\textbf{\textsf{x}},\textbf{\textsf{t}})=\mathcal{N}\left(t|m(x),s^{2}(x)\right)\label{eq:p(t|x,x,t)=}
\end{equation}

\end_inset

where the mean and variance are given by
\begin_inset Formula 
\begin{eqnarray}
m(x) & = & \beta\bm{\phi}(x)^{\mathrm{T}}\mathbf{S}\sum_{n=1}^{N}\bm{\phi}(x_{n})t_{n}\\
s^{2}(x) & = & \beta^{-1}+\bm{\phi}(x)^{\mathrm{T}}\mathbf{S}\bm{\phi}(x)\label{eq:s^2(x)}
\end{eqnarray}

\end_inset

Here the matrix 
\begin_inset Formula $\mathbf{S}$
\end_inset

 is given by
\begin_inset Formula 
\[
\mathbf{S}^{-1}=\alpha\mathbf{I}+\beta\sum_{n=1}^{N}\bm{\phi}(x_{n})\bm{\phi}(x)^{\mathrm{T}}
\]

\end_inset

and we have defined the vector 
\begin_inset Formula $\bm{\phi}(x)$
\end_inset

 with elements 
\begin_inset Formula $\phi_{i}(x)=x^{i}$
\end_inset

 for 
\begin_inset Formula $i=0,\ldots,M$
\end_inset

.
\end_layout

\begin_layout Standard
The matrix and the mean of the predictive distribution in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(t|x,x,t)="

\end_inset

 is dependent on 
\begin_inset Formula $x$
\end_inset

.
 The first term in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:s^2(x)"

\end_inset

 represents the uncertainty due to the noise on the target variables, and
 the second term arises from the uncertainty in the parameters 
\begin_inset Formula $\mathbf{w}$
\end_inset

 and is a consequence of the Bayesian treatment.
\end_layout

\begin_layout Section
Model Selection
\end_layout

\begin_layout Standard
Model selection is to find the appropriate values of complexity parameters
 within a given model and to find the best model for a particular application.
\end_layout

\begin_layout Standard
Due to the problem of over-fitting, performance on the training set is not
 a good indicator of predictive performance.
 If data is plentiful, we can set aside a 
\emph on
validation set
\emph default
 for comparing models.
 If the model design is iterated many times using a limited size data set,
 some over-fitting to the validation data can occur so it may be necessary
 to keep aside a third 
\emph on
test set
\emph default
 on which the performance of the selected model is finally evaluated.
\end_layout

\begin_layout Standard
But the supply of data for training and testing will be limited.
 To use as much of the available data as possible for training, one solution
 is to use 
\emph on
cross-validation
\emph default
, which is, to divide the data into 
\begin_inset Formula $S$
\end_inset

 sets, and use 
\begin_inset Formula $S-1$
\end_inset

 sets for training and 
\begin_inset Formula $1$
\end_inset

 set for validation, in total 
\begin_inset Formula $S$
\end_inset

 runs.
 When 
\begin_inset Formula $S=N$
\end_inset

, it's called the 
\emph on
leave-one-out
\emph default
 technique.
\end_layout

\begin_layout Standard
One major drawback of cross-validation is that the number of training runs
 is increased by a factor of 
\begin_inset Formula $S$
\end_inset

, and this can be problematic when training is computationally expensive.
 And when there are multiple parameters to explore, required number of training
 runs is exponential in the number of parameters.
 We therefore need a measure of performance which depends only on the training
 data (i.e.
 not validation-based) and which does not suffer from bias due to over-fitting.
\end_layout

\begin_layout Standard
Historically various `information criteria' have been proposed that attempt
 to correct for the bias of maximum likelihood by the addition of a penalty
 term to compensate for the over-fitting of more complex models.
 For example, the 
\emph on
Akaike information criterion
\emph default
, or AIC, chooses the model for which the quantity
\begin_inset Formula 
\[
\ln p(\mathcal{D}|\mathbf{w}_{\mathrm{ML}})-M
\]

\end_inset

is largest.
 Later we'll see how complexity penalties arise in a natural and principled
 way in a fully Bayesian approach.
\end_layout

\begin_layout Section
The Curse of Dimensionality
\end_layout

\begin_layout Standard
In the polynomial curve fitting example we had just one input variable 
\begin_inset Formula $x$
\end_inset

, but in practice we will deal with spaces of high dimensionality comprising
 many input variables.
 This poses some serious challenges and is an important factor influencing
 the design of pattern recognition techniques.
\end_layout

\begin_layout Standard
For example, a simple approach for classification is to divide the input
 space into regular cells and classify each cell independently.
 But the number of cells grows exponentially with the dimensionality of
 the space, so we need exponentially large quantity of training data in
 order to ensure that the cells are not empty, which is not practical in
 a space of more than a few variables.
 High-dimensional general polynomial curve fitting have similar problems,
 as 
\begin_inset Formula $D$
\end_inset

 the number of input variables increases, the number of independent coefficients
 grows proportionally to 
\begin_inset Formula $D^{M}$
\end_inset

 for a polynomial of order 
\begin_inset Formula $M$
\end_inset

.
\end_layout

\begin_layout Standard
Our geometrical intuitions formed from life can fail badly when we consider
 spaces of higher dimensionality.
 For example, consider a sphere of radius 
\begin_inset Formula $r=1$
\end_inset

 in a space of 
\begin_inset Formula $D$
\end_inset

 dimensions, the fraction of the volume of the sphere that lies between
 radius 
\begin_inset Formula $r=1-\epsilon$
\end_inset

 and 
\begin_inset Formula $r=1$
\end_inset

 is given by 
\begin_inset Formula $1-(1-\epsilon)^{D}$
\end_inset

.
 For large 
\begin_inset Formula $D$
\end_inset

, this fraction tends to 
\begin_inset Formula $1$
\end_inset

 even for small values of 
\begin_inset Formula $\epsilon$
\end_inset

.
 Thus, in spaces of high dimensionality, most of the volume of a sphere
 is concentrated in a thin shell near the surface! 
\end_layout

\begin_layout Standard
Similarly, consider Gaussian distribution in high-dimensional space.
 If we transform from Cartesian to polar coordinates, and then integrate
 out the directional variables, we obtain an expression for the density
 
\begin_inset Formula $p(r)$
\end_inset

 as a function of radius 
\begin_inset Formula $r$
\end_inset

 from the origin.
 We can see that for large 
\begin_inset Formula $D$
\end_inset

 the probability mass of the Gaussian is concentrated in a thin shell.
\end_layout

\begin_layout Standard
The severe difficulty that can arise in spaces of many dimensions is sometimes
 called the 
\emph on
curse of dimensionality
\emph default
.
 But it does not prevent us from finding effective techniques applicable
 to high-dimensional spaces.
 First, real data will often be confined to a region of the space having
 lower effective dimensionality, and in particular the directions over which
 important variations in the target variables occur may be so confined.
 Second, real data will typically exhibit some smoothness properties (at
 least locally) so that for the most part small changes in the input variables
 will produce small changes in the target variables, and so we can exploit
 local interpolation-like techniques to allow us to make predictions of
 the target variables for new values of the input variables.
 Successful pattern recognition techniques exploit one or both of these
 properties.
 For example, an application in manufacturing in which images are captured
 of identical planar objects on a conveyor belt, in which the goal is to
 determine their orientation.
 Each image is a point in a space whose dimensionality is determined by
 the number of pixels.
 But since there are three degrees of freedom of variability between images,
 actually a set of images will live on a three dimensional 
\emph on
manifold
\emph default
 embedded within the high-dimensional space.
\end_layout

\begin_layout Section
Decision Theory
\end_layout

\begin_layout Standard
Decision theory, when combined with probability theory, allows us to make
 optimal decisions in situations involving uncertainty.
\end_layout

\begin_layout Standard
Suppose we have an input vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 together with a corresponding vector 
\begin_inset Formula $\mathbf{t}$
\end_inset

 of target variables, and our goal is to predict 
\begin_inset Formula $\mathbf{t}$
\end_inset

 given a new value for 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 
\begin_inset Formula $\mathbf{t}$
\end_inset

 are continuous variables or class labels for regression and classification
 problems.
 The joint probability distribution 
\begin_inset Formula $p(\mathbf{x},\mathbf{t})$
\end_inset

 provides a complete summary of the uncertainty associated with these variables.
 Determination of 
\begin_inset Formula $p(\mathbf{x},\mathbf{t})$
\end_inset

 from a set of training data is an example of 
\emph on
inference
\emph default
 and is typically very difficult.
 In practice, what we need is the prediction of 
\begin_inset Formula $\mathbf{t}$
\end_inset

, or more generally take a specific action based on our understudying of
 values 
\begin_inset Formula $\mathbf{t}$
\end_inset

 is likely to take, and this aspect is the subject of decision theory.
\end_layout

\begin_layout Standard
Consider, for example, a medical diagnosis problem, we have a X-ray image
 input vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

, and output value 
\begin_inset Formula $t$
\end_inset

 to be a binary variable such that 
\begin_inset Formula $t=0$
\end_inset

 corresponds to class 
\begin_inset Formula $\mathcal{C}_{1}$
\end_inset

, the presence of cancer, and 
\begin_inset Formula $t=1$
\end_inset

 corresponds to 
\begin_inset Formula $\mathcal{C}_{2}$
\end_inset

.
 The general inference problem involves determining the joint distribution
 
\begin_inset Formula $p(\mathbf{x},\mathcal{C}_{k})$
\end_inset

, or equivalently 
\begin_inset Formula $p(\mathbf{x},t)$
\end_inset

.
 Although this can be very useful and informative, in the end we must decide
 whether to give treatment, and we would like this choice to be optimal
 in some appropriate sense.
 This is the 
\emph on
decision
\emph default
 step.
\end_layout

\begin_layout Standard
When we obtained 
\begin_inset Formula $\mathbf{x}$
\end_inset

, we're interested in the probabilities of the two classes given the image,
 which are given by 
\begin_inset Formula $p(\mathcal{C}_{k}|\mathbf{x})$
\end_inset

, using Bayes' theorem, it can be expressed in the form
\begin_inset Formula 
\[
p(\mathcal{C}_{k}|\mathbf{x})=\frac{p(\mathbf{x}|\mathcal{C}_{k})p(\mathcal{C}_{k})}{p(\mathbf{x})}
\]

\end_inset

If our aim is to minimize the chance of assigning 
\begin_inset Formula $\mathbf{x}$
\end_inset

 to the wrong class, then intuitively we would choose the class having the
 higher posterior probability.
\end_layout

\begin_layout Subsection
Minimizing the misclassification rate
\end_layout

\begin_layout Standard
We need a rule to assign each value of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 to one of the available classes.
 Such a rule will divide the input space into regions 
\begin_inset Formula $\mathcal{R}_{k}$
\end_inset

 called 
\emph on
decision regions
\emph default
, one for each class.
 The boundaries between decision regions are called 
\emph on
decision boundaries
\emph default
 or 
\emph on
decision surfaces
\emph default
.
\end_layout

\begin_layout Standard
In the case of two classes, the probability of misclassification is
\begin_inset Formula 
\begin{eqnarray}
p(\text{mistake}) & = & p(\mathbf{x}\in\mathcal{R}_{1},\mathcal{C}_{2})+p(\mathbf{x}\in\mathcal{R}_{2},\mathcal{C}_{1})\nonumber \\
 & = & \int_{\mathcal{R}_{1}}p(\mathbf{x},\mathcal{C}_{2})\mathrm{d}\mathbf{x}+\int_{\mathcal{R}_{2}}p(\mathbf{x},\mathcal{C}_{1})\mathrm{d}\mathbf{x}\label{eq:p(mistake)}
\end{eqnarray}

\end_inset

Clearly to minimize 
\begin_inset Formula $p(\text{mistake})$
\end_inset

 we should arrange that each 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is assigned to whichever class has the smaller value of the integrand in
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(mistake)"

\end_inset

.
 Since 
\begin_inset Formula $p(\mathbf{x},\mathcal{C}_{k})=p(\mathcal{C}_{k}|\mathbf{x})p(\mathbf{x})$
\end_inset

, it's equivalent to assign 
\begin_inset Formula $\mathbf{x}$
\end_inset

 to the class for which the posterior probability 
\begin_inset Formula $p(\mathcal{C}_{k}|\mathbf{x})$
\end_inset

 is largest.
\end_layout

\begin_layout Standard
For the more general case of 
\begin_inset Formula $K$
\end_inset

 classes, it's slightly easier to maximize the probability of being correct,
 which is given by
\begin_inset Formula 
\begin{eqnarray}
p(\text{correct}) & = & \sum_{k=1}^{K}p(\mathbf{x}\in\mathcal{R}_{k},\mathcal{C}_{k})\nonumber \\
 & = & \sum_{k=1}^{K}\int_{\mathcal{R}_{k}}p(\mathbf{x},\mathcal{C}_{k})\mathrm{d}\mathbf{x}
\end{eqnarray}

\end_inset

which is maximized when the regions 
\begin_inset Formula $\mathcal{R}_{k}$
\end_inset

 are chosen such that each 
\begin_inset Formula $\mathrm{x}$
\end_inset

 is assigned to the class for which 
\begin_inset Formula $p(\mathbf{x},\mathcal{C}_{k})$
\end_inset

 or 
\begin_inset Formula $p(\mathcal{C}_{k}|\mathbf{x})$
\end_inset

 is the largest.
\end_layout

\begin_layout Subsection
Minimizing the expected loss
\end_layout

\begin_layout Standard
For many applications, different kinds of misclassifications lead to different
 penalty, which can be formalized through a 
\emph on
loss function
\emph default
, also called a 
\emph on
cost function
\emph default
, which is a single, overall measure of loss incurred in taking any of the
 available decisions or actions.
 Our goal is then to minimize the total loss incurred.
 Suppose 
\begin_inset Formula $L_{kj}$
\end_inset

 represents the loss when the true class is 
\begin_inset Formula $\mathcal{C}_{k}$
\end_inset

 and we assign the input to class 
\begin_inset Formula $\mathcal{C}_{j}$
\end_inset

, 
\begin_inset Formula $L$
\end_inset

 is called a 
\emph on
loss matrix
\emph default
.
\end_layout

\begin_layout Standard
The optimal solution is the one which minimizes the loss function.
 However, the loss function depends on the true class, which is unknown.
 So we seek instead of minimize the average loss respect to the distribution
 
\begin_inset Formula $p(\mathbf{x},\mathcal{C}_{k})$
\end_inset

, which is given by
\begin_inset Formula 
\[
\mathbb{E}[L]=\sum_{k}\sum_{j}\int_{\mathcal{R}_{j}}L_{kj}p(\mathbf{x},\mathcal{C}_{k})\mathrm{d}\mathbf{x}
\]

\end_inset

Each 
\begin_inset Formula $\mathbf{x}$
\end_inset

 can be assigned to one of 
\begin_inset Formula $\mathcal{R}_{j}$
\end_inset

, which implies that for each 
\begin_inset Formula $\mathbf{x}$
\end_inset

 we should minimize 
\begin_inset Formula $\sum_{k}L_{kj}p(\mathbf{x},\mathcal{C}_{k})$
\end_inset

.
 As before we can use the product rule 
\begin_inset Formula $p(\mathbf{x},\mathcal{C}_{k})=p(\mathcal{C}_{k}|\mathbf{x})p(\mathbf{x})$
\end_inset

 to eliminate the common factor of 
\begin_inset Formula $p(\mathbf{x})$
\end_inset

.
 Thus the decision rule that minimizes the expected loss is the one that
 assigns each new 
\begin_inset Formula $\mathbf{x}$
\end_inset

 to the class 
\begin_inset Formula $j$
\end_inset

 for which the quantity
\begin_inset Formula 
\[
\sum_{k}L_{kj}p(\mathbf{x},\mathcal{C}_{k})
\]

\end_inset

is a minimum.
\end_layout

\begin_layout Subsection
The reject option
\end_layout

\begin_layout Standard
The classification errors arise from the regions of input space where the
 largest of the posterior probabilities 
\begin_inset Formula $p(\mathcal{C}_{k}|\mathbf{x})$
\end_inset

 is significantly less than unity, or equivalently where the joint distributions
 
\begin_inset Formula $p(\mathbf{x},\mathcal{C}_{k})$
\end_inset

 have comparable values.
 These are the regions where we are relatively uncertain about class membership.
 In some applications, it will be appropriate to avoid making decisions
 on the difficult cases in anticipation of a lower error rate on those examples
 for which a classification decision is made.
 This is known as the reject option.
 We can achieve this by introducing a threshold 
\begin_inset Formula $\theta$
\end_inset

 and rejecting those inputs 
\begin_inset Formula $\mathbf{x}$
\end_inset

 for which the largest of the posterior probabilities 
\begin_inset Formula $p(\mathcal{C}_{k}|\mathbf{x})$
\end_inset

 is less than or equal to 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\begin_layout Standard
We can easily extend the reject criterion to minimize the expected loss,
 when a loss matrix include the loss incurred when a reject decision is
 made.
\end_layout

\begin_layout Subsection
Inference and decision
\end_layout

\begin_layout Standard
We have broken the classification problem down into two separate stages,
 the 
\emph on
inference stage
\emph default
 in which we use training data to learn a model for 
\begin_inset Formula $p(\mathcal{C}_{k}|\mathbf{x})$
\end_inset

, and the subsequent 
\emph on
decision stage
\emph default
 in which we use these posterior probabilities to make optimal class assignments.
 In fact, we can identity three distinct approaches to solving decision
 problems.
\end_layout

\begin_layout Enumerate
\begin_inset Argument
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

label=(
\backslash
alph*)
\end_layout

\end_inset


\end_layout

\end_inset

First solve the inference problem of determining the class-conditional densities
 
\begin_inset Formula $p(\mathbf{x}|\mathcal{C}_{k})$
\end_inset

.
 Also separately infer the prior class probabilities 
\begin_inset Formula $p(\mathcal{C}_{k})$
\end_inset

.
 Then use Bayes' theorem to find the posterior class probabilities 
\begin_inset Formula $p(\mathcal{C}_{k}|\mathbf{x})$
\end_inset

.
 Equivalently, we can model the joint distribution 
\begin_inset Formula $p(\mathbf{x},\mathcal{C}_{k})$
\end_inset

 directly and then normalize to obtain the posterior probabilities.
 Then we use decision theory to determine class membership.
 Approaches that explicitly or implicitly model the distribution of inputs
 as well as outputs are known as 
\emph on
generative models
\emph default
, because by sampling from them it is possible to generate synthetic data
 points in the data space.
\begin_inset CommandInset label
LatexCommand label
name "enu:generative models"

\end_inset


\end_layout

\begin_layout Enumerate
First solve the inference problem of determining the posterior class probabiliti
es 
\begin_inset Formula $p(\mathcal{C}_{k}|\mathbf{x})$
\end_inset

, and then use decision theory to assign each new 
\begin_inset Formula $\mathbf{x}$
\end_inset

 to one of the classes.
 Approaches that model the posterior probabilities directly are called 
\emph on
discriminative models
\emph default
.
\begin_inset CommandInset label
LatexCommand label
name "enu:discriminative models"

\end_inset


\end_layout

\begin_layout Enumerate
Find a function 
\begin_inset Formula $f(\mathbf{x})$
\end_inset

, called a 
\emph on
discriminant function
\emph default
, which maps each input 
\begin_inset Formula $\mathbf{x}$
\end_inset

 directly onto a class label.
 In this case, probabilities play no role.
\begin_inset CommandInset label
LatexCommand label
name "enu:discriminant function"

\end_inset


\end_layout

\begin_layout Standard
Approach 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:generative models"

\end_inset

 is the most demanding, because for many applications 
\begin_inset Formula $\mathbf{x}$
\end_inset

 will have high dimensionality, and consequently we need a large training
 set in order to determine the class-conditional densities 
\begin_inset Formula $p(\mathbf{x}|\mathcal{C}_{k})$
\end_inset

 or the joint distribution 
\begin_inset Formula $p(\mathbf{x},\mathcal{C}_{k})$
\end_inset

 to reasonable accuracy.
 However, one advantage is it can also determine 
\begin_inset Formula $p(\mathbf{x})$
\end_inset

.
 This can be useful for detecting new data points that have low probability
 under the model and for which the predictions may be of low accuracy, which
 is known as 
\emph on
outlier detection
\emph default
 or 
\emph on
novelty detection
\emph default
.
\end_layout

\begin_layout Standard
The class-conditional densities may contain a lot of structure that has
 little effect on the posterior probabilities, so in approach 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:discriminative models"

\end_inset

 we find the posterior probabilities 
\begin_inset Formula $p(\mathcal{C}_{k}|\mathbf{x})$
\end_inset

 directly.
\end_layout

\begin_layout Standard
Approach 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:discriminant function"

\end_inset

 is even simpler, in which we combine the inference and decision stages
 into a simple learning problem.
\end_layout

\begin_layout Standard
There are many powerful reasons for wanting to compute the posterior probabiliti
es 
\begin_inset Formula $p(\mathcal{C}_{k}|\mathbf{x})$
\end_inset

 before making decisions:
\end_layout

\begin_layout Itemize
The loss matrix may be subjected to revision.
\end_layout

\begin_layout Itemize
The possibility of reject option.
\end_layout

\begin_layout Itemize
Compensating for class priors.
 Consider the medical X-ray problem, since cancer is rare, only 0.1% of our
 samples are in the cancer class.
 A classifier that assigned every point to the normal class would already
 achieve 99.9% accuracy and it would be difficult to avoid this trivial solution.
 Also, the learning algorithm will not be exposed to a broad range of examples
 in the cancer class and hence is not likely to generalize well.
 A balanced data set in which we have selected equal numbers of examples
 from each of the classes would allow us to find a more accurate model.
 However, we must compensate for the effects of our modifications to the
 training data.
 We can simply take the posterior probabilities obtained from our artificially
 balanced data set and first divide by the class fractions in that data
 set and then multiply by the class fractions in the population to which
 we wish to apply the model.
 Finally, we need to normalize to ensure that the new posterior probabilities
 sum to one.
 Note that this procedure cannot be applied if we have learned a discriminant
 function directly instead of determining posterior probabilities.
\end_layout

\begin_layout Itemize
Combining models.
 For complex applications, we can break the problem into a number of smaller
 subproblems each of which can be tackled by a separate model.
 For example in the medical X-ray problem, we may assume that the distribution
 of inputs for X-ray images 
\begin_inset Formula $\mathbf{x}_{\mathrm{I}}$
\end_inset

 and the blood data 
\begin_inset Formula $\mathbf{x}_{\mathrm{B}}$
\end_inset

 are independently, so that
\begin_inset Formula 
\[
p(\mathbf{x}_{\mathrm{I}},\mathbf{x}_{\mathrm{B}}|\mathcal{C}_{k})=p(\mathbf{x}_{\mathrm{I}}|\mathcal{C}_{k})p(\mathbf{x}_{\mathrm{B}}|\mathcal{C}_{k})
\]

\end_inset

This is an example of 
\emph on
conditional independence
\emph default
 property.
 Then the posterior probability given both the data is given by
\begin_inset Formula 
\begin{eqnarray}
p(\mathcal{C}_{k}|\mathbf{x}_{\mathrm{I}},\mathbf{x}_{\mathrm{B}}) & \propto & p(\mathbf{x}_{\mathrm{I}},\mathbf{x}_{\mathrm{B}}|\mathcal{C}_{k})p(\mathcal{C}_{k})\nonumber \\
 & \propto & p(\mathbf{x}_{\mathrm{I}}|\mathcal{C}_{k})p(\mathbf{x}_{\mathrm{B}}|\mathcal{C}_{k})p(\mathcal{C}_{k})\nonumber \\
 & \propto & p(\mathcal{C}_{k}|\mathbf{x}_{\mathrm{I}})p(\mathcal{C}_{k}|\mathbf{x}_{\mathrm{B}})
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Subsection
Loss functions for regression
\end_layout

\begin_layout Standard
In regression problems, the decision stage consists of choosing a specific
 estimate 
\begin_inset Formula $y(\mathbf{x})$
\end_inset

 of the value of 
\begin_inset Formula $t$
\end_inset

 for each input 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Suppose that in doing so, we incur a loss 
\begin_inset Formula $L(t,y(\mathbf{x}))$
\end_inset

.
 The expected loss is given by
\begin_inset Formula 
\[
\mathbb{E}[L]=\iint L(t,y(\mathbf{x}))p(\mathbf{x},t)\mathrm{d}\mathbf{x}\mathrm{d}t
\]

\end_inset

A common choice of the loss function is the squared loss 
\begin_inset Formula $L(t,y(\mathbf{x}))=\left\{ y(\mathbf{x})-t\right\} ^{2}$
\end_inset

.
 In this case
\begin_inset Formula 
\[
\mathbb{E}[L]=\iint\left\{ y(\mathbf{x})-t\right\} ^{2}p(\mathbf{x},t)\mathrm{d}\mathbf{x}\mathrm{d}t
\]

\end_inset

Our goal is to choose 
\begin_inset Formula $y(\mathbf{x})$
\end_inset

 so as to minimize 
\begin_inset Formula $\mathbb{E}[L]$
\end_inset

.
 If we assume a completely flexible function 
\begin_inset Formula $y(\mathbf{x})$
\end_inset

, we can do this formally using the calculus of variations to give
\begin_inset Formula 
\[
\frac{\delta\mathbb{E}[L]}{\delta y(\mathbf{x})}=2\int\left\{ y(\mathbf{x})-t\right\} p(\mathbf{x},t)\mathrm{d}t=0
\]

\end_inset

Solving for 
\begin_inset Formula $y(\mathbf{x})$
\end_inset

, and using the sum and product rules of probability, we obtain
\begin_inset Formula 
\[
y(\mathbf{x})=\frac{\int tp(\mathbf{x},t)\mathrm{d}t}{p(\mathbf{x})}=\int tp(t|\mathbf{x})\mathrm{d}t=\mathbb{E}_{t}[t|\mathbf{x}]
\]

\end_inset

which is the conditional average of 
\begin_inset Formula $t$
\end_inset

 conditioned on 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and is known as the 
\emph on
regression function
\emph default
.
 It can readily be extended to multiple variables represented by the vector
 
\begin_inset Formula $\mathbf{t}$
\end_inset

, in which case the optimal solution is the conditional average 
\begin_inset Formula $\mathbf{y}(\mathbf{x})=\mathbb{E}_{\mathbf{t}}[\mathbf{t}|\mathbf{x}]$
\end_inset

.
\end_layout

\begin_layout Standard
The squared loss is not the only possible choice of loss function for regression.
 Indeed, there are situations in which squared loss can lead to very poor
 results and where we need to develop more sophisticated approaches.
 An important example concerns situations in which the conditional distribution
 
\begin_inset Formula $p(t|\mathbf{x})$
\end_inset

 is multimodal, as often arises in the solution of inverse problems.
 One simple generalization of the squared loss is the 
\begin_inset Formula $Minkowski$
\end_inset

 loss, whose expectation is given by
\begin_inset Formula 
\[
\mathbb{E}[L_{q}]=\iint\left|y(\mathbf{x})-t\right|^{q}p(\mathbf{x},t)\mathrm{d}\mathbf{x}\mathrm{d}t
\]

\end_inset

The minimum of 
\begin_inset Formula $\mathbb{E}[L_{q}]$
\end_inset

 is given by the conditional mean for 
\begin_inset Formula $q=2$
\end_inset

, the conditional media for 
\begin_inset Formula $q=1$
\end_inset

, and the conditional mode for 
\begin_inset Formula $q\to0$
\end_inset

.
\end_layout

\end_body
\end_document
