#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass scrbook
\begin_preamble
\usepackage{indentfirst}
\usepackage{helvet}

\renewcommand\[{\begin{equation}}
\renewcommand\]{\end{equation}}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format pdf4
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Reading Notes of
\begin_inset Newline newline
\end_inset

Pattern Classification and Machine Learning
\end_layout

\begin_layout Author
Tianyi Cui
\end_layout

\begin_layout Date
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Standard
Different kinds of tasks of machine learning:
\end_layout

\begin_layout Itemize
supervised learning: known input and target vectors
\end_layout

\begin_layout Itemize
classification: output is one of a finite number of discrete categories
\end_layout

\begin_deeper
\begin_layout Itemize
regression: output is one or more continuous variables
\end_layout

\end_deeper
\begin_layout Itemize
unsupervised learning: no corresponding target values
\end_layout

\begin_deeper
\begin_layout Itemize
clustering: discover groups of similar examples within the data
\end_layout

\begin_layout Itemize
density estimation: determine the distribution of data within the input
 space
\end_layout

\begin_layout Itemize
dimension reduction
\end_layout

\end_deeper
\begin_layout Itemize
reinforcement learning: finding suitable actions to take in a given situation
 in order to maximize a reward
\end_layout

\begin_layout Section
Example: Polynomial Curve Fitting
\end_layout

\begin_layout Standard
In regression problems, we can use a polynomial function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
y(x,\mathbf{w})=w_{0}+w_{1}+w_{2}x^{2}+\ldots+w_{M}x^{M}=\sum_{j=0}^{M}w_{j}x^{j}\label{eq:y(x,w)}
\end{equation}

\end_inset

 to fit the underlying function.
\end_layout

\begin_layout Standard
We need to minimize the 
\emph on
error function
\emph default
 
\begin_inset Formula 
\begin{equation}
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\{y(x_{n},\mathbf{w})-t_{n}\}^{2}\label{eq:E(w)}
\end{equation}

\end_inset

in which unique solution 
\begin_inset Formula $\mathbf{w^{*}}$
\end_inset

 can be found in closed form.
\end_layout

\begin_layout Standard
The root-mean-square (RMS) error is defined by 
\begin_inset Formula 
\[
E_{RMS}=\sqrt{2E(\mathbf{w^{*})/N}}
\]

\end_inset


\end_layout

\begin_layout Standard
When 
\begin_inset Formula $M$
\end_inset

 is large, 
\emph on
over-fitting
\emph default
 occurs, i.e.
 
\begin_inset Formula $E_{RMS}$
\end_inset

 against test data becomes large.
 One technique to control over-fitting is 
\emph on
regularization
\emph default
, by adding a penalty term to the error function 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:E(w)"

\end_inset

 in order to discourage the coefficients from reaching large values:
\begin_inset Formula 
\begin{equation}
\widetilde{E}(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\{y(x_{n},\mathbf{w})-t_{n}\}^{2}+\frac{\lambda}{2}\left\Vert \mathbf{w}\right\Vert ^{2}\label{eq:regularized E(w)}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Probability Theory
\end_layout

\begin_layout Standard
Equations for probability:
\end_layout

\begin_layout Itemize
Sum rule
\begin_inset Formula 
\[
p(X)=\sum_{Y}p(X,Y)
\]

\end_inset


\end_layout

\begin_layout Itemize
Product rule
\begin_inset Formula 
\[
p(X,Y)=p(Y|X)p(X)
\]

\end_inset


\end_layout

\begin_layout Itemize
Bayes' theorem
\begin_inset Formula 
\begin{equation}
p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}\label{eq:bayes}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The denominator in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:bayes"

\end_inset

 can be expressed in terms of the quantities appearing in the numerator
\begin_inset Formula 
\[
p(X)=\sum_{Y}p(X|Y)p(Y)
\]

\end_inset


\end_layout

\begin_layout Standard
We can view the denominator in Bayes' theorem as being the normalization
 constant required to ensure that the sum of the conditional probability
 on the left-hand side of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:bayes"

\end_inset

 over all values of 
\begin_inset Formula $Y$
\end_inset

 equals 
\begin_inset Formula $1$
\end_inset

.
\end_layout

\begin_layout Standard
Before any observation, we have a probability of a certain event 
\begin_inset Formula $Y$
\end_inset

, this is called 
\emph on
prior probability
\emph default
 
\begin_inset Formula $p(Y)$
\end_inset

, after some observation 
\begin_inset Formula $X$
\end_inset

, the probability of event 
\begin_inset Formula $Y$
\end_inset

 becomes the 
\emph on
posterior probability
\emph default
 
\begin_inset Formula $p(Y|X)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are said to be 
\emph on
independent
\emph default
 if 
\begin_inset Formula $p(X,Y)=p(X)p(Y)$
\end_inset

, which is equivaent to 
\begin_inset Formula $P(Y|X)=p(Y)$
\end_inset

.
\end_layout

\begin_layout Subsection
Probability densities
\end_layout

\begin_layout Standard
If the probability that 
\begin_inset Formula $x$
\end_inset

 will lie in 
\begin_inset Formula $(a,b)$
\end_inset

 is given by 
\begin_inset Formula 
\[
p(x\in(a,b))=\int_{a}^{b}p(x)\mathrm{d}x
\]

\end_inset

then 
\begin_inset Formula $p(x)$
\end_inset

 is called the 
\emph on
probability density
\emph default
 over 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
Apparently 
\begin_inset Formula $p(x)\geq0$
\end_inset

 and 
\begin_inset Formula $\int_{-\infty}^{\infty}p(x)\mathrm{d}x=1$
\end_inset

.
\end_layout

\begin_layout Standard
Under a nonlinear change of variable, a probability density transforms different
ly from a simple function, due to the Jacobian factor.
 If 
\begin_inset Formula $x=g(y)$
\end_inset

, since 
\begin_inset Formula $p_{x}(x)\mathrm{d}x=p_{y}(y)\mathrm{d}y$
\end_inset

, hence
\begin_inset Formula 
\begin{eqnarray}
p_{y}(y) & = & p_{x}(x)\left|\frac{\mathrm{d}x}{\mathrm{d}y}\right|\nonumber \\
 & = & p_{x}(g(y))\left|g'(y)\right|
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
cumulative distribution function
\emph default
 
\begin_inset Formula 
\[
P(z)=\int_{-\infty}^{z}p(x)\mathrm{d}x
\]

\end_inset


\end_layout

\begin_layout Standard
For several continuous variables 
\begin_inset Formula $x_{1},\ldots,x_{D}$
\end_inset

, denoted collectively by the vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

, then we can define a joint probability density 
\begin_inset Formula $p(\mathbf{x})$
\end_inset

 such that 
\begin_inset Formula $p\left(\mathbf{x}\in(\mathbf{x}_{0},\mathbf{x}_{0}+\delta\mathbf{x})\right)=p(\mathbf{x}_{0})\delta\mathbf{x}$
\end_inset

.
\end_layout

\begin_layout Subsection
Expectations and covariances
\end_layout

\begin_layout Standard
The average value of some function 
\begin_inset Formula $f(x)$
\end_inset

 under a probability distribution 
\begin_inset Formula $p(x)$
\end_inset

 is called the 
\emph on
expectaion
\emph default
 of 
\begin_inset Formula $f(x)$
\end_inset

 and denoted by 
\begin_inset Formula $\mathbb{E}[f]$
\end_inset

.
 For a descrite distribution,
\begin_inset Formula 
\[
\mathbb{E}[f]=\sum_{x}p(x)f(x)
\]

\end_inset

For continuous variables,
\begin_inset Formula 
\[
\mathbb{E}[f]=\int p(x)f(x)\mathrm{d}x
\]

\end_inset

In either case, the expectation can be approximated given 
\begin_inset Formula $N$
\end_inset

 samples,
\begin_inset Formula 
\[
\mathbb{E}[f]\simeq\frac{1}{N}\sum_{n=1}^{N}f(x_{n})
\]

\end_inset


\end_layout

\begin_layout Standard
When considering expectations of functions of several variables, we use
 subscript to indicate which variable is being averaged over, e.g.
 
\begin_inset Formula $\mathbb{E}_{x}[f(x,y)]$
\end_inset

 is a function of 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Standard
We can also consider 
\emph on
conditional expectation
\begin_inset Formula 
\[
\mathbb{E}_{x}[f|y]=\sum_{x}p(x|y)f(x)
\]

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
variance
\emph default
 of 
\begin_inset Formula $f(x)$
\end_inset

 is defined by
\begin_inset Formula 
\begin{eqnarray}
\mathrm{var}[f] & = & \mathbb{E}\left[(f(x)-\mathbb{E}[f(x)]\right]{}^{2}\\
 & = & \mathbb{E}\left[f(x)^{2}\right]-\mathbb{E}\left[f(x)\right]{}^{2}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
covariance
\emph default
 of two random variable 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 is defined by
\begin_inset Formula 
\begin{eqnarray}
\mathrm{cov}[x,y] & = & \mathbb{E}_{x,y}\left[\{x-\mathbb{E}[x]\}\{y-\mathbb{E}[y]\}\right]\nonumber \\
 & = & \mathbb{E}_{x,y}[xy]-\mathbb{E}[x]\mathbb{E}[y]
\end{eqnarray}

\end_inset

which expresses the extent to which 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 vary together.
 If they are independent, then thei covariance vanishes.
\end_layout

\begin_layout Standard
In the case of two vectors of random variables 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}$
\end_inset

, the covariance is a matrix
\begin_inset Formula 
\begin{eqnarray}
\mathrm{cov}[\mathbf{x},\mathbf{y}] & = & \mathbb{E}_{x,y}\left[\left\{ \mathbf{x}-\mathbb{E}[\mathbf{x]}\right\} \left\{ \mathbf{y}^{\mathrm{T}}-\mathbb{E}[\mathbf{y^{\mathrm{T}}]}\right\} \right]\nonumber \\
 & = & \mathbb{E}_{x,y}[\mathbf{x}\mathbf{y}^{\mathrm{T}}]-\mathbb{E}[\mathbf{x]}\mathbb{E}[\mathbf{y^{\mathrm{T}}]}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Subsection
Bayesian probabilities
\end_layout

\begin_layout Standard
In the 
\emph on
classical
\emph default
 or 
\emph on
frequentist
\emph default
 interpretation of probability, probabilities is viewed in terms of the
 frequencies of random, repeatable events.
 In the more general 
\emph on
Beyesian
\emph default
 view, probabilities provide a quantification of uncertainty, so we can
 say the probability of an uncertain event, like whether the Arctic ice
 cap will have disappeared by the end of the century, which is not events
 that can be repeated.
\end_layout

\begin_layout Standard
In the polynomial curve fitting example, we assume the parameters 
\begin_inset Formula $\mathbf{w}$
\end_inset

 have a prior probability distribution 
\begin_inset Formula $p(\mathbf{w})$
\end_inset

, then given the observed data 
\begin_inset Formula $\mathcal{D}$
\end_inset

, the posterior probability is
\begin_inset Formula 
\[
p(\mathbf{w}|\mathcal{D})=\frac{p(\mathcal{D}|\mathbf{w})p(\mathbf{w)}}{p(\mathcal{D})}
\]

\end_inset

where the quantity 
\begin_inset Formula $p(\mathcal{D}|\mathbf{w})$
\end_inset

 is called the 
\emph on
likelihood function
\emph default
, which expresses how probable the observed data set is for different settings
 of the parameter vector 
\begin_inset Formula $\mathbf{w}$
\end_inset

.
 The likelihood is not a probability distribution over 
\begin_inset Formula $\mathbf{w}$
\end_inset

, and its integral does not necessarily equal one.
\end_layout

\begin_layout Standard
Given the definition of liklihood, we can state Bayes' theorem in words
\begin_inset Formula 
\[
\text{posterior}\propto\text{likelihood}\times\text{prior}
\]

\end_inset

where all of these quantities are viewed as functions of 
\begin_inset Formula $\mathbf{w}$
\end_inset

.
\end_layout

\begin_layout Standard
In the likelihood function 
\begin_inset Formula $p(\mathcal{D}|\mathbf{w})$
\end_inset

, in the frequentist setting, 
\begin_inset Formula $\mathbf{w}$
\end_inset

 is considered to be a fixed parameter, whose value is determines by some
 form of `estimator', and error bars on this estimate are obtained by considerin
g the distribution of possible data sets 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
 By contrast, from Bayesian viewpoint there is only a single data set 
\begin_inset Formula $\mathcal{D}$
\end_inset

 (the one actually observed), and the uncertainty in the parameters is expressed
 through a probability distribution over 
\begin_inset Formula $\mathbf{w}$
\end_inset

.
\end_layout

\begin_layout Subsection
The Gaussian distribution
\end_layout

\begin_layout Standard
The Gaussian distribution on a single real-valued variable 
\begin_inset Formula $x$
\end_inset

 is defined by
\begin_inset Formula 
\[
\mathcal{N}(x|\mu,\sigma^{2})=\frac{1}{(2\pi\sigma^{2})^{1/2}}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} 
\]

\end_inset

which is governed by two parameters: 
\begin_inset Formula $\mu$
\end_inset

 the 
\emph on
mean
\emph default
 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

 the 
\emph on
variance
\emph default
.
 
\begin_inset Formula $\sigma$
\end_inset

 is called the 
\emph on
standard deviation
\emph default
, and 
\begin_inset Formula $\beta=1/\sigma^{2}$
\end_inset

 is called the 
\emph on
precision
\emph default
.
 The mean of 
\begin_inset Formula $x$
\end_inset

 is given by 
\begin_inset Formula $\mathbb{E}[x]=\mu$
\end_inset

 and the variance of 
\begin_inset Formula $x$
\end_inset

 is given by 
\begin_inset Formula $\mathrm{var}[x]=\mathbb{E}[x^{2}]-\mathbb{E}[x]^{2}=\sigma^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
The Gaussian distribution defined over a 
\begin_inset Formula $D$
\end_inset

-dimensional vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 of continuous variables is given by
\begin_inset Formula 
\[
\mathcal{N}(\mathbf{x}|\mathbf{\mu},\mathbf{\Sigma})=\frac{1}{(2\pi)^{D/2}}\frac{1}{\left|\bm{\Sigma}\right|^{1/2}}\exp\left\{ -\frac{1}{2}(\mathbf{x}-\bm{\mu})^{\mathrm{T}}\bm{\Sigma}^{-1}(\mathbf{x}-\bm{\mu})\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
Suppose we have a data set of observation 
\begin_inset Formula $\textbf{\textsf{x}}=(x_{1},\ldots,x_{N})^{\mathrm{T}}$
\end_inset

 which is 
\emph on
independent and identically distributed
\emph default
 (often abbreviated to i.i.d.) from a Gaussian distribution.
 The likelihood of the data set, which is a function of 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset

, is in the form
\begin_inset Formula 
\[
p\left(\textbf{\textsf{x}}|\mu,\sigma^{2}\right)=\prod_{n=1}^{N}\mathcal{N}(x_{n}|\mu,\sigma^{2})
\]

\end_inset


\end_layout

\begin_layout Standard
One common criterion for determining the parameters in a probability distributio
n using an observed data set is to find the parameter values that maximize
 the likelihood function.
\end_layout

\begin_layout Standard
In practice, for mathematical and numerical reasons, it's more convenient
 to maximize the log of the likelihood functions
\begin_inset Formula 
\begin{equation}
\ln p\left(\textbf{\textsf{x}}|\mu,\sigma^{2}\right)=-\frac{1}{2\sigma^{2}}\sum_{n=1}^{N}(x_{n}-\mu)^{2}-\frac{N}{2}\ln\sigma^{2}-\frac{N}{2}\ln(2\pi)\label{eq:ln p(x|mu,sigma^2)}
\end{equation}

\end_inset

 
\end_layout

\begin_layout Standard
Maximizing 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(x|mu,sigma^2)"

\end_inset

 with respect to 
\begin_inset Formula $\mu$
\end_inset

 gives the maximum likelihood solution
\begin_inset Formula 
\[
\mu_{\mathrm{ML}}=\frac{1}{N}\sum_{n=1}^{N}x_{n}
\]

\end_inset

which is the 
\emph on
sample mean
\emph default
.
 Similarly, Maximize 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(x|mu,sigma^2)"

\end_inset

 with respect to 
\begin_inset Formula $\sigma^{2}$
\end_inset

gives
\begin_inset Formula 
\[
\sigma_{\mathrm{ML}}^{2}=\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu_{\mathrm{ML}})^{2}
\]

\end_inset

which is the 
\emph on
sample variance.
\end_layout

\begin_layout Standard
The maximum likelihood approach systematically underestimates the variance
 of the distribution.
 This is an example of a phenomenon called 
\emph on
bias
\emph default
 and is related to the problem of over-fitting encountered in the context
 of polynomial curve fitting.
 First, we note that 
\begin_inset Formula $\mu_{\mathrm{ML}}$
\end_inset

 and 
\begin_inset Formula $\sigma_{\mathrm{ML}}^{2}$
\end_inset

 are functions of the data set values 
\begin_inset Formula $x_{1},\ldots,x_{N}$
\end_inset

.
 Consider the expectations of these quantities with respect to the data
 set values, which themselves come from a Gaussian distribution with parameters
 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\sigma^{2}$
\end_inset


\begin_inset Formula 
\begin{eqnarray}
\mathbb{E}[\mu_{\mathrm{ML}}] & = & \mu\\
\mathbb{E}[\sigma_{\mathrm{ML}}^{2}] & = & \left(\frac{N-1}{N}\right)\sigma^{2}\label{eq:E[sigma_ML^2]}
\end{eqnarray}

\end_inset

so on average the maximum likelihood approach will underestimate the true
 variance by a factor 
\begin_inset Formula $(N-1)/N$
\end_inset

.
\end_layout

\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:E[sigma_ML^2]"

\end_inset

 we see the following estimate for the variance parameter is unbiased
\begin_inset Formula 
\[
\widetilde{\sigma}^{2}=\frac{N}{N-1}\sigma_{\mathrm{ML}}^{2}=\frac{1}{N-1}\sum_{n=1}^{N}(x_{n}-\mu_{\mathrm{ML}})^{2}
\]

\end_inset

 this result arises automatically when we adopt a Bayesian approach (Section
 10.1.3).
\end_layout

\begin_layout Subsection
Curve fitting re-visited
\end_layout

\begin_layout Standard
The goal of curve fitting problem is to make predictions for the target
 variable 
\begin_inset Formula $t$
\end_inset

 given some new value of the input variable 
\begin_inset Formula $x$
\end_inset

 on the basis of a set of training data 
\begin_inset Formula $\textbf{\textsf{x}}=(x_{1},\ldots,x_{N})^{\mathrm{T}}$
\end_inset

 and 
\begin_inset Formula $\textbf{\textsf{t}}=(t_{1},\ldots,t_{N})^{\mathrm{T}}$
\end_inset

.
 We can express out uncertainty over the value of the target variable using
 a probability distribution.
 Assume that, given the value of 
\begin_inset Formula $x$
\end_inset

, the corresponding value of 
\begin_inset Formula $t$
\end_inset

 has a Gaussian distribution with a mean equal to the value 
\begin_inset Formula $y(x,\mathbf{w})$
\end_inset

 given by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:y(x,w)"

\end_inset

.
 Thus we have
\begin_inset Formula 
\begin{equation}
p(t|x,\mathbf{w},\beta)=\mathcal{N}\left(t|y(x,\mathbf{w}),\beta^{-1}\right)\label{eq:p(t|x,w,beta)}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\beta$
\end_inset

 is the precision parameter.
\end_layout

\begin_layout Standard
Use the training data 
\begin_inset Formula $\{\textbf{\textsf{x}},\textbf{\textsf{t}}\}$
\end_inset

 to determine the values of the unknown parameters 
\begin_inset Formula $\mathbf{w}$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 by maximum likelihood, the likelihood function is
\begin_inset Formula 
\[
p(\textbf{\textsf{t}}|\textbf{\textsf{x}},\mathbf{w},\beta)=\prod_{n=1}^{N}\mathcal{N}\left(t_{n}|y(x_{n},\mathbf{w}),\beta^{-1}\right)
\]

\end_inset

and its logarithm is
\begin_inset Formula 
\begin{equation}
\ln p(\textbf{\textsf{t}}|\textbf{\textsf{x}},\mathbf{w},\beta)=-\frac{\beta}{2}\sum_{n=1}^{N}\left\{ y(x_{n},\mathbf{w})-t_{n}\right\} ^{2}+\frac{N}{2}\ln\beta-\frac{N}{2}\ln(2\pi)\label{eq:ln p(t|x,w,beta)}
\end{equation}

\end_inset

Maximizing 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(t|x,w,beta)"

\end_inset

 with respect to 
\begin_inset Formula $\mathbf{w}$
\end_inset

 gives us 
\begin_inset Formula $\mathbf{w}_{\mathrm{ML}}$
\end_inset

, which is the same as minimize the 
\emph on
sum-of-squares error function
\emph default
 defined by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:E(w)"

\end_inset

.
\end_layout

\begin_layout Standard
Maximizing 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(t|x,w,beta)"

\end_inset

 with respect to 
\begin_inset Formula $\beta$
\end_inset

 gives
\begin_inset Formula 
\[
\frac{1}{\beta_{\mathrm{ML}}}=\frac{1}{N}\sum_{n=1}^{N}\left\{ y(x_{n},\mathrm{w}_{\mathrm{ML}})-t_{n}\right\} ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Having determined the parameters 
\begin_inset Formula $\mathbf{w}$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

, we can now make predictions for new values of 
\begin_inset Formula $x$
\end_inset

, and in probabilistic model, these are expressed in terms of the 
\emph on
predictive distribution
\emph default
 that gives the probability distribution over 
\emph on
t
\begin_inset Formula 
\[
p(t|x,\mathrm{w}_{\mathrm{ML}},\beta_{\mathrm{ML}})=\mathcal{N}\left(t|y(x,\mathbf{w}_{ML}),\beta_{\mathrm{ML}}^{-1}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
In a more Bayesian approach, we introduce a Gaussian prior distribution
 over the polynomial coefficients 
\begin_inset Formula $\mathbf{w}$
\end_inset


\begin_inset Formula 
\begin{equation}
p(\mathbf{w}|\alpha)=\mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I})=\left(\frac{\alpha}{2\pi}\right)^{(M+1)/2}\exp\left\{ -\frac{\alpha}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}\right\} \label{eq:p(w|alpha)}
\end{equation}

\end_inset

where 
\begin_inset Formula $\alpha$
\end_inset

 is the precision of the distribution and 
\begin_inset Formula $M+1$
\end_inset

 is the number of elements in 
\begin_inset Formula $\mathbf{w}$
\end_inset

.
 Values such as 
\begin_inset Formula $\alpha$
\end_inset

, which controls the distribution of model parameters, are called 
\emph on
hyperparameters
\emph default
.
\end_layout

\begin_layout Standard
Using Bayes' theorem
\begin_inset Formula 
\begin{equation}
p(\mathbf{w}|\textbf{\textsf{x}},\textbf{\textsf{t }},\alpha,\beta)\propto p(\textsf{t }|\textsf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha)\label{eq:p(w|x,t,alpha,beta)}
\end{equation}

\end_inset

We can now determine 
\begin_inset Formula $\mathbf{w}$
\end_inset

 by finding the most probable value of 
\begin_inset Formula $\mathbf{w}$
\end_inset

 given the data, in other words by maximizing the posterior distribution.
 This technique is called 
\emph on
maximum posterior
\emph default
, or simply 
\emph on
MAP
\emph default
.
\end_layout

\begin_layout Standard
Taking the negative logarithm of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(w|x,t,alpha,beta)"

\end_inset

 and combining with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(t|x,w,beta)"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(w|alpha)"

\end_inset

, we find that the maximum of the posterior is given by the minimum of
\begin_inset Formula 
\[
\frac{\beta}{2}\sum_{n=1}^{N}\left\{ y(x_{n},\mathbf{w})-t_{n}\right\} ^{2}+\frac{\alpha}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}
\]

\end_inset

Thus we see that maximizing the posterior distribution is equivalent to
 minimizing the regularized sum-of-squares error function 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:regularized E(w)"

\end_inset

.
\end_layout

\begin_layout Subsection
Bayesian curve fitting
\end_layout

\begin_layout Standard
Although we have included a prior distribution 
\begin_inset Formula $p(\mathbf{w}|\alpha)$
\end_inset

, we are still making a point estimate of 
\begin_inset Formula $\mathbf{w}$
\end_inset

 and so this does not yet amount to a Bayesian treatment.
 In a fully Bayesian approach, we should consistently apply the sum and
 product rules of probability, which requires, as we shall see shortly,
 that we integrate over all values of 
\begin_inset Formula $\mathbf{w}$
\end_inset

.
 Such marginalizations lie at the heart of Bayesian methods for pattern
 recognition.
\end_layout

\begin_layout Standard
In curve fitting, we are given the training data 
\begin_inset Formula $\{\textbf{\textsf{x}},\textbf{\textsf{t}}\}$
\end_inset

, along with a new test point 
\begin_inset Formula $x$
\end_inset

, and our goal is to predict the value of 
\begin_inset Formula $t$
\end_inset

.
 Assuming the parameters 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $\beta$
\end_inset

 are fixed and known in advance by now, we wish the evaluate the predictive
 distribution 
\begin_inset Formula $p(t|\textbf{\textsf{x}},\textbf{\textsf{t}})$
\end_inset

.
 Using the product rules of probability
\begin_inset Formula 
\begin{equation}
p(t|x,\textbf{\textsf{x}},\textbf{\textsf{t}})=\int p(t|x,\mathbf{w})p(\mathbf{w}|\textbf{\textsf{x}},\textbf{\textsf{t}})\mathrm{d}\mathbf{w}\label{eq:p(t|x,x,t)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Here 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $p(t|x,\mathbf{w})$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 and 
\begin_inset Formula $p(\mathbf{w}|\textbf{\textsf{x}},\textbf{\textsf{t}})$
\end_inset

 are given by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(t|x,w,beta)"

\end_inset

 and normalizing the right-hand side of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(w|x,t,alpha,beta)"

\end_inset

.
\end_layout

\begin_layout Standard
The calculation and integration in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(t|x,x,t)"

\end_inset

 can be performed analytically with the result in a Gaussian distribution
\begin_inset Formula 
\begin{equation}
p(t|x,\textbf{\textsf{x}},\textbf{\textsf{t}})=\mathcal{N}\left(t|m(x),s^{2}(x)\right)\label{eq:p(t|x,x,t)=}
\end{equation}

\end_inset

where the mean and variance are given by
\begin_inset Formula 
\begin{eqnarray}
m(x) & = & \beta\bm{\phi}(x)^{\mathrm{T}}\mathbf{S}\sum_{n=1}^{N}\bm{\phi}(x_{n})t_{n}\\
s^{2}(x) & = & \beta^{-1}+\bm{\phi}(x)^{\mathrm{T}}\mathbf{S}\bm{\phi}(x)\label{eq:s^2(x)}
\end{eqnarray}

\end_inset

Here the matrix 
\begin_inset Formula $\mathbf{S}$
\end_inset

 is given by
\begin_inset Formula 
\[
\mathbf{S}^{-1}=\alpha\mathbf{I}+\beta\sum_{n=1}^{N}\bm{\phi}(x_{n})\bm{\phi}(x)^{\mathrm{T}}
\]

\end_inset

and we have defined the vector 
\begin_inset Formula $\bm{\phi}(x)$
\end_inset

 with elements 
\begin_inset Formula $\phi_{i}(x)=x^{i}$
\end_inset

 for 
\begin_inset Formula $i=0,\ldots,M$
\end_inset

.
\end_layout

\begin_layout Standard
The matrix and the mean of the predictive distribution in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(t|x,x,t)="

\end_inset

 is dependent on 
\begin_inset Formula $x$
\end_inset

.
 The first term in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:s^2(x)"

\end_inset

 represents the uncertainty due to the noise on the target variables, and
 the second term arises from the uncertainty in the parameters 
\begin_inset Formula $\mathbf{w}$
\end_inset

 and is a consequence of the Bayesian treatment.
\end_layout

\begin_layout Section
Model Selection
\end_layout

\begin_layout Standard
Model selection is to find the appropriate values of complexity parameters
 within a given model and to find the best model for a particular application.
\end_layout

\begin_layout Standard
Due to the problem of over-fitting, performance on the training set is not
 a good indicator of predictive performance.
 If data is plentiful, we can set aside a 
\emph on
validation set
\emph default
 for comparing models.
 If the model design is iterated many times using a limited size data set,
 some over-fitting to the validation data can occur so it may be necessary
 to keep aside a third 
\emph on
test set
\emph default
 on which the performance of the selected model is finally evaluated.
\end_layout

\begin_layout Standard
But the supply of data for training and testing will be limited.
 To use as much of the available data as possible for training, one solution
 is to use 
\emph on
cross-validation
\emph default
, which is, to divide the data into 
\begin_inset Formula $S$
\end_inset

 sets, and use 
\begin_inset Formula $S-1$
\end_inset

 sets for training and 
\begin_inset Formula $1$
\end_inset

 set for validation, in total 
\begin_inset Formula $S$
\end_inset

 runs.
 When 
\begin_inset Formula $S=N$
\end_inset

, it's called the 
\emph on
leave-one-out
\emph default
 technique.
\end_layout

\begin_layout Standard
One major drawback of cross-validation is that the number of training runs
 is increased by a factor of 
\begin_inset Formula $S$
\end_inset

, and this can be problematic when training is computationally expensive.
 And when there are multiple parameters to explore, required number of training
 runs is exponential in the number of parameters.
 We therefore need a measure of performence which depends only on the training
 data (i.e.
 not validation-based) and which does not suffer from bias due to over-fitting.
\end_layout

\begin_layout Standard
Historically various `information criteria' have been proposed that attempt
 to correct for the bias of maximum likelihood by the addition of a penalty
 term to compensate for the over-fitting of more complex models.
 For example, the 
\emph on
Akaike information criterion
\emph default
, or AIC, chooses the model for which the quantity
\begin_inset Formula 
\[
\ln p(\mathcal{D}|\mathbf{w}_{\mathrm{ML}})-M
\]

\end_inset

is largest.
 Later we'll see how complexity penalties arise in a natural and principled
 way in a fully Bayesian approach.
\end_layout

\end_body
\end_document
