#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass scrbook
\use_default_options true
\master note.lyx
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand input
filename "macros.lyx"

\end_inset


\end_layout

\begin_layout Chapter
Probability Distributions
\end_layout

\begin_layout Standard
In this chapter, we'll discuss different probability distributions.
 They're used as building blocks for more complex models, and to provide
 the opportunity to discuss some key statistical concepts.
\end_layout

\begin_layout Standard
The problem known as 
\emph on
density estimation
\emph default
 is to model the probability distribution 
\begin_inset Formula $p({\bf x})$
\end_inset

 of a random variable 
\begin_inset Formula ${\bf x}$
\end_inset

, given a finite set 
\begin_inset Formula ${\bf x}_{1},\ldots,{\bf x}_{N}$
\end_inset

 of observations.
 For the purpose of this chapter, we shall assume that the data points are
 i.i.d.
 It should be emphasized that the problem of density estimation is fundamentally
 ill-posed, because any distribution 
\begin_inset Formula $p({\bf x})$
\end_inset

 that is nonzero at each of the data points is a potential candidate.
 The issue of choosing an appropriate distribution relates to the problem
 of model selection and is a central issue in pattern recognition.
\end_layout

\begin_layout Standard
To apply 
\emph on
parametric
\emph default
 distributions to density estimation, we need a procedure for determining
 suitable values for the parameters, given an observed data set.
 In a frequentist treatment, we choose specific values for the parameters
 by optimizing some criterion, such as the likelihood function.
 In a Bayesian treatment we introduce prior distributions over the parameters
 and then use Bayesâ€™ theorem to compute the corresponding posterior distribution
 given the observed data.
\end_layout

\begin_layout Standard

\emph on
Conjugate
\emph default
 priors lead to posterior distributions having the same functional form
 as the priors, and therefore lead to a greatly simplified Bayesian analysis.
\end_layout

\begin_layout Standard
One limitation of the parametric approach is that it assumes a specific
 functional form for the distribution, which may turn out to be inappropriate
 for a particular application.
 An alternative approach is given by 
\emph on
nonparametric
\emph default
 density estimation methods in which the form of the distribution typically
 depends on the size of the data set.
 Such models still contain parameters, but these control the model complexity
 rather than the form of the distribution.
\end_layout

\begin_layout Section
Binary Variables
\end_layout

\begin_layout Standard
The distribution of single binary variable 
\begin_inset Formula $x\in\{0,1\}$
\end_inset

, with a single parameter 
\begin_inset Formula $\mu$
\end_inset

 given by 
\begin_inset Formula $p(x=1)=\mu$
\end_inset

, can be written in the form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Bern(x|\mu)=\mu^{x}(1-\mu)^{1-x}
\]

\end_inset

where 
\begin_inset Formula $0\leq\mu\leq1$
\end_inset

, It is known as the 
\emph on
Bernoulli
\emph default
 distribution.
 Its mean and variance are given by
\begin_inset Formula 
\begin{eqnarray}
\EE[x] & = & \mu\\
\var[x] & = & \mu(1-\mu)
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Suppose we have a data set 
\begin_inset Formula ${\cal D}=\{x_{1},\ldots,x_{N}\}$
\end_inset

 of observed values of 
\begin_inset Formula $x$
\end_inset

.
 By maximizing the likelihood function over 
\begin_inset Formula $\mu$
\end_inset

, we obtain the maximum likelihood estimator
\begin_inset Formula 
\begin{equation}
\mu_{{\rm ML}}=\frac{1}{N}\sum_{n=1}^{N}x_{n}\label{eq:mu_ML for Bern}
\end{equation}

\end_inset

 which is also known as the 
\emph on
sample mean
\emph default
.
 If we denote the number of observations of 
\begin_inset Formula $x=1$
\end_inset

 within the data set by 
\begin_inset Formula $m$
\end_inset

, then we can write 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mu_ML for Bern"

\end_inset

 in the form
\begin_inset Formula 
\[
\mu_{{\rm ML}}=\frac{m}{N}
\]

\end_inset

so that the probability of landing heads is given, in this maximum likelihood
 framework, by the fraction of observations of heads in the data set.
\end_layout

\begin_layout Standard
If we flip a coin 
\begin_inset Formula $3$
\end_inset

 times and happen to observe 
\begin_inset Formula $3$
\end_inset

 heads.
 Then 
\begin_inset Formula $N=m=3$
\end_inset

 and 
\begin_inset Formula $\mu_{{\rm ML}}=1$
\end_inset

.
 In this case, the maximum likelihood result would predict that all future
 observations should give heads.
 This is an extreme example of the over-fitting associated with maximum
 likelihood.
 We shall see shortly how to arrive at more sensible conclusions through
 the introduction of a prior distribution over 
\begin_inset Formula $\mu$
\end_inset

.
\end_layout

\begin_layout Standard
We can also work out the distribution of the number 
\begin_inset Formula $m$
\end_inset

 of observations of 
\begin_inset Formula $x=1$
\end_inset

, in a data set which has size 
\begin_inset Formula $N$
\end_inset

.
 This is called the 
\emph on
binomial
\emph default
 distribution
\begin_inset Formula 
\begin{equation}
\Bin(m|N,\mu)=\binom{N}{m}\mu^{m}(1-\mu)^{N-m}\label{eq:Bin(m|N,mu)}
\end{equation}

\end_inset

which has mean and variance
\begin_inset Formula 
\begin{eqnarray}
\EE[m] & = & N_{\mu}\label{eq:E[m] for binomial}\\
\var[m] & = & N\mu(1-\mu)\label{eq:var[m] for binomial}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Subsection
The beta distribution
\end_layout

\begin_layout Standard
Here we consider a form of prior distribution of the parameter 
\begin_inset Formula $\mu$
\end_inset

 in the Bernoulli distribution, which has a simple interpretation as well
 as some useful analytical properties.
 To motivate this prior, we note that the likelihood function takes the
 form of the product of factors of the form 
\begin_inset Formula $\mu^{x}(1-\mu)^{1-x}$
\end_inset

.
 If we choose a prior to be proportional to powers of 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $(1-\mu)$
\end_inset

, then the posterior distribution will have the same functional form as
 the prior.
 This property is called 
\emph on
conjugacy
\emph default
.
 We therefore choose a prior, called the 
\emph on
beta
\emph default
 distribution, given by
\begin_inset Formula 
\begin{equation}
\Beta[\mu|a,b]=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\label{eq:Beta[mu|a,b]}
\end{equation}

\end_inset

where 
\begin_inset Formula $\Gamma(x)$
\end_inset

 is the gamma function defined by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Gamma(x)\equiv\int_{0}^{\infty}u^{x-1}e^{-u}\d u
\]

\end_inset

and the coefficients in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Beta[mu|a,b]"

\end_inset

 ensures that the beta distribution is normalized.
\end_layout

\begin_layout Standard
The mean and variance of the beta distribution are given by
\begin_inset Formula 
\begin{eqnarray}
\EE[\mu] & = & \frac{a}{a+b}\label{eq:E[mu] for beta}\\
\var[\mu] & = & \frac{ab}{(a+b)^{2}(a+b+1)}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
The parameters 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 are often called 
\emph on
hyperparameters
\emph default
 because they control the distribution of the parameter 
\begin_inset Formula $\mu$
\end_inset

.
\end_layout

\begin_layout Standard
The posterior distribution of 
\begin_inset Formula $\mu$
\end_inset

 is now obtained by multiplying the beta prior 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Beta[mu|a,b]"

\end_inset

 by the binomial likelihood function 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Bin(m|N,mu)"

\end_inset

 and normalizing.
 Keep only the factors that depend on 
\begin_inset Formula $\mu$
\end_inset

, we see that this posterior distribution has the form
\begin_inset Formula 
\begin{equation}
p(\mu|m,l,a,b)\propto\mu^{m+a-1}(1-\mu)^{l+b-1}\label{eq:p(mu|m,l,a,b)}
\end{equation}

\end_inset

where 
\begin_inset Formula $l=N-m$
\end_inset

.
 We see thats 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(mu|m,l,a,b)"

\end_inset

 has the same functional dependence on 
\begin_inset Formula $\mu$
\end_inset

 as the prior distribution, reflecting the conjugacy properties of the prior
 with respect to the likelihood function.
 Indeed, it is simply another beta distribution
\begin_inset Formula 
\begin{equation}
p(\mu|m,l,a,b)=\frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)}\mu^{m+a-1}(1-\mu)^{l+b-1}\label{eq:p(mu|m,l,a,b)=}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This allows us to provide a simple interpretation of the hyperparameters
 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 in the prior as an 
\emph on
effective number of observations
\emph default
 of 
\begin_inset Formula $x=1$
\end_inset

 and 
\begin_inset Formula $x=0$
\end_inset

, respectively.
 Note that 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 need not be integers.
 Furthermore, the posterior distribution can act as the prior if we subsequently
 observe additional data.
 An additional observation of 
\begin_inset Formula $x=1$
\end_inset

 simply corresponds to incrementing the value of a by 
\begin_inset Formula $1$
\end_inset

, whereas for an observation of 
\begin_inset Formula $x=0$
\end_inset

 we increment 
\begin_inset Formula $b$
\end_inset

 by 
\begin_inset Formula $1$
\end_inset

.
\end_layout

\begin_layout Standard
We see that this 
\emph on
sequential
\emph default
 approach to learning arises naturally when we adopt a Bayesian viewpoint.
 It is independent of the choice of prior and of the likelihood function
 and depends only on the assumption of i.i.d.
 data.
 Sequential methods make use of observations one at a time, or in small
 batches, and then discard them before the next observations are used.
 They can be used, for example, in real-time learning scenarios where a
 steady stream of data is arriving, and predictions must be made before
 all of the data is seen.
 Because they do not require the whole data set to be stored or loaded into
 memory, sequential methods are also useful for large data sets.
 Maximum likelihood methods can also be cast into a sequential framework.
\end_layout

\begin_layout Standard
Given the observed data set 
\begin_inset Formula ${\cal D}$
\end_inset

, we can predict the next 
\begin_inset Formula $x$
\end_inset

 by the form
\begin_inset Formula 
\[
p(x=1|{\cal D})=\int_{0}^{1}p(x=1|\mu)p(\mu|{\cal D})\d\mu=\int_{0}^{1}\mu p(\mu|{\cal D})\d\mu=\EE[\mu|{\cal D}]
\]

\end_inset

Using result 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(mu|m,l,a,b)="

\end_inset

 together with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:E[mu] for beta"

\end_inset

, we obtain
\begin_inset Formula 
\[
p(x=1|{\cal D})=\frac{m+a}{m+a+l+b}
\]

\end_inset

which has a simple interpretation as the total fraction of observations
 (both real observations and fictitious prior observations).
\end_layout

\begin_layout Standard
As the number of observations increases, the posterior distribution becomes
 more sharply peaked, in which the variance goes to zero for 
\begin_inset Formula $a\to\infty$
\end_inset

 or 
\begin_inset Formula $b\to\infty$
\end_inset

.
 In fact, we might wonder whether it is a general property of Bayesian learning
 that, as we observe more and more data, the uncertainty represented by
 the posterior distribution will steadily decrease.
\end_layout

\begin_layout Standard
To address this, we can take a frequentist view of Bayesian learning and
 show that, on average, such a property does indeed hold.
 Consider a general Bayesian inference problem for a parameter 
\begin_inset Formula $\theta$
\end_inset

 for which we have observed a data set 
\begin_inset Formula ${\cal D}$
\end_inset

, described by the joint distribution 
\begin_inset Formula $p(\theta,{\cal D})$
\end_inset

.
 The following result
\begin_inset Formula 
\[
\EE_{\bm{\theta}}[\bm{\theta}]=\EE_{{\cal D}}[\EE_{\bm{\theta}}[\bm{\theta}|{\cal D}]]
\]

\end_inset

where
\begin_inset Formula 
\begin{eqnarray}
\EE_{\bm{\theta}}[\bm{\theta}] & \equiv & \int p(\bm{\theta})\bm{\theta}\d\bm{\theta}\\
\EE_{{\cal D}}[\EE_{\bm{\theta}}[\bm{\theta}|{\cal D}]] & \equiv & \int\left\{ \int\bm{\theta}p({\bf \bm{\theta}|{\cal D})\d\bm{\theta}}\right\} p({\cal D})\d{\cal D}
\end{eqnarray}

\end_inset

says that the posterior mean of 
\begin_inset Formula $\bm{\theta}$
\end_inset

, averaged over the distribution generating the data, is equal to the prior
 mean of 
\begin_inset Formula $\bm{\theta}$
\end_inset

.
 Similarly, we can show that
\begin_inset Formula 
\begin{equation}
\var_{\bm{\theta}}[\bm{\theta}]=\EE_{{\cal D}}[\var_{\bm{\theta}}[\bm{\theta}|{\cal D}]]+\var_{{\cal D}}[\EE_{\bm{\theta}}[\bm{\theta}|{\cal D}]]\label{eq:E_theta[theta]==}
\end{equation}

\end_inset

The term on the left-hand side of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:E_theta[theta]=="

\end_inset

 is the prior variance of 
\begin_inset Formula $\bm{\theta}$
\end_inset

.
 On the right-hand side, the first term is the average posterior variance
 of 
\begin_inset Formula $\bm{\theta}$
\end_inset

, and the second term measures the variance in the posterior mean of 
\begin_inset Formula $\bm{\theta}$
\end_inset

.
 Because this variance is a positive quantity, this result shows that, on
 average, the posterior variance of 
\begin_inset Formula $\bm{\theta}$
\end_inset

 is smaller than the prior variance.
 The reduction in variance is greater if the variance in the posterior mean
 is greater.
 Note, however, that this result only holds on average, and that for a particula
r observed data set it is possible for the posterior variance to be larger
 than the prior variance.
\end_layout

\begin_layout Section
Multinomial Variables
\end_layout

\begin_layout Standard
Often, we encounter discrete variables that can take on one of 
\begin_inset Formula $K$
\end_inset

 possible mutually exclusive states.
 One particularly convenient representation to express such variables is
 the 1-of-
\begin_inset Formula $K$
\end_inset

 scheme, in which, the variable is represented by a 
\begin_inset Formula $K$
\end_inset

-dimensional vector 
\begin_inset Formula ${\bf x}$
\end_inset

 in which one of the elements 
\begin_inset Formula $x_{k}$
\end_inset

 equals 
\begin_inset Formula $1$
\end_inset

, and all remaining elements equal 
\begin_inset Formula $0$
\end_inset

.
 Note that such vectors satisfy 
\begin_inset Formula $\sum_{k=1}^{K}x_{k}=1$
\end_inset

.
 If we denote the probability of 
\begin_inset Formula $x_{k}=1$
\end_inset

 by the parameter 
\begin_inset Formula $\mu_{k}$
\end_inset

, then the distribution of 
\begin_inset Formula ${\bf x}$
\end_inset

 is given by
\begin_inset Formula 
\begin{equation}
p({\bf x}|\bm{\mu})=\prod_{k=1}^{K}\mu_{k}^{x_{k}}\label{eq:p(x|mu)}
\end{equation}

\end_inset

where 
\begin_inset Formula $\bm{\mu}=(\mu_{1},\ldots,\mu_{K})\trans$
\end_inset

, and the parameters 
\begin_inset Formula $\mu_{k}$
\end_inset

 are constrained to satisfy 
\begin_inset Formula $\mu_{k}\geq0$
\end_inset

 and 
\begin_inset Formula $\sum_{k}\mu_{k}=1$
\end_inset

.
 The distribution 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x|mu)"

\end_inset

 can be regarded as a generalization of the Bernoulli distribution to more
 than two outcomes.
 Its mean is given by
\begin_inset Formula 
\[
\EE[{\bf x}|\bm{\mu}]=\sum_{x}p({\bf x}|\bm{\mu}){\bf x}=(\mu_{1},\ldots,\mu_{K})\trans=\bm{\mu}
\]

\end_inset


\end_layout

\begin_layout Standard
Now consider a data set 
\begin_inset Formula ${\cal D}$
\end_inset

 of 
\begin_inset Formula $N$
\end_inset

 independent observations 
\begin_inset Formula ${\bf x}_{1},\ldots,{\bf x}_{N}$
\end_inset

.
 The corresponding likelihood function takes the form
\begin_inset Formula 
\begin{equation}
p({\cal D}|\bm{\mu})=\prod_{n=1}^{N}\prod_{k=1}^{K}\mu_{k}^{x_{nk}}=\prod_{k=1}^{K}\mu_{k}^{(\sum_{n}x_{nk})}=\prod_{k=1}^{K}\mu_{k}^{m_{k}}\label{eq:p(D|mu)}
\end{equation}

\end_inset

which depends on the 
\begin_inset Formula $N$
\end_inset

 data points only through the 
\begin_inset Formula $K$
\end_inset

 quantities
\begin_inset Formula 
\[
m_{k}=\sum_{n}x_{nk}
\]

\end_inset

which represents the number of observations of 
\begin_inset Formula $x_{k}=1$
\end_inset

.
 These are called the 
\emph on
sufficient statistics
\emph default
 for this distribution.
\end_layout

\begin_layout Standard
The maximum likelihood solution for 
\begin_inset Formula $\bm{\mu}$
\end_inset

 taking account of its constraint, which can be solved using Lagrange multiplier
, is in the form
\begin_inset Formula 
\[
\mu_{k}^{{\rm ML}}=\frac{m_{k}}{N}
\]

\end_inset

which is the fraction of the 
\begin_inset Formula $N$
\end_inset

 observations for which 
\begin_inset Formula $x_{k}=1$
\end_inset

.
\end_layout

\begin_layout Standard
We can consider the joint distribution of the quantities 
\begin_inset Formula $m_{1},\ldots,m_{K}$
\end_inset

, conditioned on the parameters 
\begin_inset Formula $\bm{\mu}$
\end_inset

 and on the total number 
\begin_inset Formula $N$
\end_inset

 of observations.
 From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(D|mu)"

\end_inset

 this takes the form
\begin_inset Formula 
\begin{equation}
\Mult(m_{1},m_{2},\ldots,m_{K}|\bm{\mu},N)=\binom{N}{m_{1}\: m_{2}\:\ldots\: m_{K}}\prod_{k=1}^{K}\mu_{k}^{m_{k}}\label{eq:Mult(m_1...m_K|mu,N)}
\end{equation}

\end_inset

which is known as the 
\emph on
multinomial
\emph default
 distribution.
 Note that the variables 
\begin_inset Formula $m_{k}$
\end_inset

 are subject to the constraint
\begin_inset Formula 
\[
\sum_{k=1}^{K}m_{k}=N
\]

\end_inset


\end_layout

\begin_layout Subsection
The Dirichlet distribution
\end_layout

\begin_layout Standard
By inspecting the form of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Mult(m_1...m_K|mu,N)"

\end_inset

, we see that the conjugate prior is given by
\begin_inset Formula 
\[
p(\bm{\mu}|\bm{\alpha})\propto\prod_{k=1}^{K}\mu_{k}^{\alpha_{k}-1}
\]

\end_inset

where 
\begin_inset Formula $0\leq\mu_{k}\leq1$
\end_inset

 and 
\begin_inset Formula $\sum_{k}\mu_{k}=1$
\end_inset

.
 Here 
\begin_inset Formula $\bm{\alpha}=(\alpha_{1},\ldots,\alpha_{K})\trans$
\end_inset

 are the parameters of the distribution.
 Note that, because of the summation constraint, the distribution over the
 space of the 
\begin_inset Formula $\{\mu_{k}\}$
\end_inset

 is confined to a 
\emph on
simplex
\emph default
 of dimensionality 
\begin_inset Formula $K-1$
\end_inset

.
\end_layout

\begin_layout Standard
The normalized form for this distribution is by
\begin_inset Formula 
\begin{equation}
\Dir(\bm{\mu}|\bm{\alpha})=\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{1})\ldots\Gamma(\alpha_{K})}\prod_{k=1}^{K}\mu_{k}^{\alpha_{k}-1}\label{eq:Dir(mu|alpha)}
\end{equation}

\end_inset

which is called the 
\emph on
Dirichlet
\emph default
 distribution, here 
\begin_inset Formula $\alpha_{0}=\sum_{k=1}^{N}\alpha_{K}$
\end_inset

.
\end_layout

\begin_layout Standard
Multiplying the prior 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Dir(mu|alpha)"

\end_inset

 by the likelihood function 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Mult(m_1...m_K|mu,N)"

\end_inset

, we obtain the posterior distribution for the parameters 
\begin_inset Formula $\{\mu_{k}\}$
\end_inset

 in the form
\begin_inset Formula 
\[
p(\bm{\mu}|{\cal D},\bm{\alpha})=\Dir(\bm{\mu}|\bm{\alpha}+{\bf m})
\]

\end_inset

where we have denoted 
\begin_inset Formula ${\bf m}=(m_{1},\ldots,m_{K})\trans$
\end_inset

.
\end_layout

\begin_layout Section
The Gaussian Distribution
\end_layout

\begin_layout Standard
The Gaussian, also known as the normal distribution, is a widely used model
 for the distribution of continuous variables.
 In the case of a single variable x, the Gaussian distribution can be written
 in the form
\begin_inset Formula 
\[
{\cal N}(x|\mu,\sigma^{2})=\frac{1}{(2\pi\sigma^{2})^{1/2}}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} 
\]

\end_inset

where 
\begin_inset Formula $\mu$
\end_inset

 is the mean and 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is the variance.
 For a 
\begin_inset Formula $D$
\end_inset

-dimensional vector 
\begin_inset Formula ${\bf x}$
\end_inset

, the multivariate Gaussian distribution takes the form
\begin_inset Formula 
\[
{\cal N}({\bf x}|\bm{\mu},\bm{\Sigma})=\frac{1}{(2\pi)^{D/2}}\frac{1}{\left|\bm{\Sigma}\right|^{1/2}}\exp\left\{ -\frac{1}{2}({\bf x}-\bm{\mu})\trans\bm{\Sigma}^{-1}({\bf x}-\bm{\mu})\right\} 
\]

\end_inset

 where 
\begin_inset Formula $\bm{\mu}$
\end_inset

 is a 
\begin_inset Formula $D$
\end_inset

-dimensional mean vector, 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 is a 
\begin_inset Formula $D\times D$
\end_inset

 covariance matrix, and 
\begin_inset Formula $\left|\bm{\Sigma}\right|$
\end_inset

 denotes the determinant of 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

.
\end_layout

\begin_layout Standard
The Gaussian distribution arises in many different contexts and can be motivated
 from a variety of different perspectives.
 For example, we have already seen that for a single real variable, the
 distribution that maximizes the entropy is the Gaussian.
 This property applies also to the multivariate Gaussian.
\end_layout

\begin_layout Standard
Another situation in which the Gaussian distribution arises is when we consider
 the sum of multiple random variables.
 The 
\emph on
central limit theorem
\emph default
 tells us that, subject to certain mild conditions, the sum of a set of
 random variables, which is of course itself a random variable, has a distributi
on that becomes increasingly Gaussian as the number of terms in the sum
 increases.
\end_layout

\begin_layout Standard
The Gaussian distribution has many important analytical properties, and
 we shall consider several of these in detail.
 We begin by considering the geometrical form of the Gaussian distribution.
 The functional dependence of the Gaussian on 
\begin_inset Formula ${\bf x}$
\end_inset

 is through the quadratic form
\begin_inset Formula 
\begin{equation}
\Delta^{2}=({\bf x}-\bm{\mu})\trans\bm{\Sigma}^{-1}({\bf x}-\bm{\mu})\label{eq:Delta^2 in Gaussian}
\end{equation}

\end_inset

which appears in the exponent.
 The quantity 
\begin_inset Formula $\Delta$
\end_inset

 is called the 
\emph on
Mahalanobis distance
\emph default
 from 
\begin_inset Formula $\bm{\mu}$
\end_inset

 to 
\begin_inset Formula ${\bf x}$
\end_inset

 and reduces to the Euclidian distance when 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 is the identity matrix.
 The Gaussian distribution will be constant on surfaces in 
\begin_inset Formula ${\bf x}$
\end_inset

-space for which this quadratic form is constant.
\end_layout

\begin_layout Standard
First of all, we note that the matrix 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 can be taken to be symmetric, without loss of generality, because any antisymme
tric component would disappear from the exponent.
 Now consider the eigenvector equation for the covariance matrix
\begin_inset Formula 
\[
\bm{\Sigma}{\bf u}_{i}=\lambda_{i}{\bf u}_{i}
\]

\end_inset

where 
\begin_inset Formula $i=1,\ldots,D$
\end_inset

.
 Because 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 is a real, symmetric matrix its eigenvalues will be real, and its eigenvectors
 can be chosen to form an orthonormal set, so that
\begin_inset Formula 
\begin{equation}
{\bf u}_{i}\trans{\bf u}_{j}=I_{ij}\label{eq:u_i^T u_j = I_ij}
\end{equation}

\end_inset

The covariance matrix 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 can be expressed as an expansion in terms of its eigenvectors in the form
\begin_inset Formula 
\[
\bm{\Sigma}=\sum_{i=1}^{D}\lambda_{i}{\bf u}_{i}{\bf u}_{i}\trans
\]

\end_inset

 and similarly the inverse covariance matrix 
\begin_inset Formula $\bm{\Sigma}^{-1}$
\end_inset

 can be expressed as
\begin_inset Formula 
\begin{equation}
\bm{\Sigma}^{-1}=\sum_{i=1}^{D}\frac{1}{\lambda_{i}}{\bf u}_{i}{\bf u_{i}}\trans\label{eq:Sigma^-1 expressed using eigenvectors}
\end{equation}

\end_inset

Substituting 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Sigma^-1 expressed using eigenvectors"

\end_inset

 into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Delta^2 in Gaussian"

\end_inset

, the quadratic form becomes
\begin_inset Formula 
\[
\Delta^{2}=\sum_{i=1}^{D}\frac{y_{i}^{2}}{\lambda_{i}}
\]

\end_inset

 where we have defined
\begin_inset Formula 
\begin{equation}
y_{i}={\bf u}_{i}\trans({\bf x}-\bm{\mu})\label{eq:y_i for Gaussian}
\end{equation}

\end_inset

 We can interpret 
\begin_inset Formula $\{y_{i}\}$
\end_inset

 as a new coordinate system defined by the orthonormal vectors 
\begin_inset Formula ${\bf u}_{i}$
\end_inset

 that are shifted and rotate with respect to the original 
\begin_inset Formula $x_{i}$
\end_inset

 coordinates.
 Forming the factor 
\begin_inset Formula ${\bf y}=(y_{1},\ldots,y_{D})\trans$
\end_inset

, we have
\begin_inset Formula 
\[
{\bf y}={\bf U}({\bf x}-\bm{\mu})
\]

\end_inset

where 
\begin_inset Formula ${\bf U}$
\end_inset

 is a matrix whose rows are given by 
\begin_inset Formula ${\bf u}_{i}\trans$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:u_i^T u_j = I_ij"

\end_inset

 
\begin_inset Formula ${\bf U}$
\end_inset

 is an 
\emph on
orthogonal matrix
\emph default
, i.e.
 
\begin_inset Formula ${\bf UU}\trans={\bf U}\trans{\bf U}={\bf I}$
\end_inset

.
\end_layout

\begin_layout Standard
The quadratic form, and hence the Gaussian density, will be constant on
 surfaces for which 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:y_i for Gaussian"

\end_inset

 is constant.
 If all of the eigenvalues 
\begin_inset Formula $\lambda_{i}$
\end_inset

 are positive, then these surfaces represent ellipsoids, with there centers
 at 
\begin_inset Formula $\bm{\mu}$
\end_inset

 and their axes oriented along 
\begin_inset Formula ${\bf u}_{i}$
\end_inset

, and with scaling factors in the directions of the axes given by 
\begin_inset Formula $\lambda_{i}^{1/2}$
\end_inset

.
\end_layout

\begin_layout Standard
For the Gaussian distribution to be well defined, it is necessary for all
 of the eigenvalues 
\begin_inset Formula $\lambda_{i}$
\end_inset

 of the covariance matrix to be strictly positive, i.e.
 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 to be positive definite, otherwise the distribution cannot be properly
 normalized.
 In Chapter 12, we will encounter Gaussian distributions for which one or
 more of the eigenvalues are zero, in which case the distribution is singular
 and is confined to a subspace of lower dimensionality.
\end_layout

\begin_layout Standard
Now consider the form of the Gaussian distribution in the new coordinate
 system defined by the 
\begin_inset Formula $y_{i}$
\end_inset

.
 In going from the 
\begin_inset Formula ${\bf x}$
\end_inset

 to the 
\begin_inset Formula ${\bf y}$
\end_inset

 coordinate system, we have a Jacobian matrix 
\begin_inset Formula ${\bf J}$
\end_inset

 with elements given by
\begin_inset Formula 
\[
J_{ij}=\frac{\partial x_{i}}{\partial y_{j}}=U_{ji}
\]

\end_inset

where 
\begin_inset Formula $U_{ji}$
\end_inset

 are the elements of the matrix 
\begin_inset Formula ${\bf U}\trans$
\end_inset

.
 Using the orthonormality property of the matrix 
\begin_inset Formula ${\bf U}$
\end_inset

, we see that the square of the determinant of the Jacobian matrix is
\begin_inset Formula 
\[
|{\bf J}|^{2}=|{\bf U}\trans|^{2}=|{\bf U}\trans||{\bf U}|=|{\bf U}\trans{\bf U}|=|{\bf I}|=1
\]

\end_inset

and hence 
\begin_inset Formula $|{\bf J}|=1$
\end_inset

.
 Also, the determinant 
\begin_inset Formula $|\bm{\Sigma}|$
\end_inset

 of the covariance matrix can be written as the product of its eigenvalues,
 and hence
\begin_inset Formula 
\[
|\bm{\Sigma}|^{1/2}=\prod_{j=1}^{D}\lambda_{j}^{1/2}
\]

\end_inset

Thus in the 
\begin_inset Formula $y_{j}$
\end_inset

 coordinate system, the Gaussian distribution takes the form
\begin_inset Formula 
\[
p({\bf y})=p({\bf x})|{\bf J}|=\prod_{j=1}^{D}\frac{1}{(2\pi\lambda_{j})^{1/2}}\exp\left\{ -\frac{y_{j}^{2}}{2\lambda_{j}}\right\} 
\]

\end_inset

which is the product of 
\begin_inset Formula $D$
\end_inset

 independent univariate Gaussian distributions.
 The eigenvectors therefore define a new set of shifted and rotated coordinates
 with respect to which the joint probability distribution factorizes into
 a product of independent distributions.
\end_layout

\begin_layout Standard
It can be proved
\begin_inset Formula 
\[
\EE[{\bf x}]=\bm{\mu}
\]

\end_inset

and
\begin_inset Formula 
\[
\EE[{\bf xx}\trans]=\bm{\mu\mu}\trans+\bm{\Sigma}
\]

\end_inset

Therefore
\begin_inset Formula 
\begin{eqnarray}
\cov[{\bf x}] & = & \EE\left[({\bf x}-\EE[{\bf x}])({\bf x}-\EE[{\bf x}])\trans\right]\nonumber \\
 & = & \bm{\Sigma}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Although the Gaussian distribution is widely used as a density model, it
 suffers from some significant limitations.
 Its number of parameters, 
\begin_inset Formula $D(D+3)/2$
\end_inset

, grows quadratically with 
\begin_inset Formula $D$
\end_inset

.
 With large 
\begin_inset Formula $D$
\end_inset

, the computational task of manipulating and inverting large matrices can
 become prohibitive.
 One way to address this problem is to use restricted forms of the covariance
 matrix.
 If we restrict 
\begin_inset Formula $\bm{\Sigma}=\diag(\sigma_{i}^{2})$
\end_inset

, then the total of independent parameters is 
\begin_inset Formula $2D$
\end_inset

 and the corresponding contours of constant density are given by axis-aligned
 ellipsoids.
 We could further restrict 
\begin_inset Formula $\bm{\Sigma}=\sigma^{2}{\bf I}$
\end_inset

, known as an 
\emph on
isotropic
\emph default
 covariance, giving 
\begin_inset Formula $D+1$
\end_inset

 independent parameters and spherical surfaces of constant density.
 Unfortunately,whereas such approaches limit the number of degrees of freedom
 in the distribution and make inversion of the covariance matrix a much
 faster operation, they also greatly restrict the form of the probability
 density and limit its ability to capture interesting correlations in the
 data.
\end_layout

\begin_layout Standard
A further limitation of the Gaussian distribution is that it is intrinsically
 unimodal (i.e., has a single maximum) and so is unable to provide a good
 approximation to multimodal distributions.
 Thus the Gaussian distribution can be both too flexible, in the sense of
 having too many parameters, while also being too limited in the range of
 distributions that it can adequately represent.
 We will see later that the introduction of 
\emph on
latent
\emph default
 variables, also called 
\emph on
hidden
\emph default
 variables or 
\emph on
unobserved
\emph default
 variables, allows both of these problems to be addressed.
\end_layout

\begin_layout Subsection
Conditional Gaussian distributions
\end_layout

\begin_layout Standard
Given a joint Gaussian distribution 
\begin_inset Formula ${\cal N}({\bf x}|\bm{\mu},\bm{\Sigma})$
\end_inset

 with 
\begin_inset Formula $\bm{\Lambda}\equiv\bm{\Sigma}^{-1}$
\end_inset

 and
\begin_inset Formula 
\[
{\bf x}=\begin{pmatrix}{\bf x}_{a}\\
{\bf x}_{b}
\end{pmatrix},\quad\bm{\mu}=\begin{pmatrix}\bm{\mu}_{a}\\
\bm{\mu}_{b}
\end{pmatrix}
\]

\end_inset


\begin_inset Formula 
\[
\bm{\Sigma}=\begin{pmatrix}\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab}\\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{pmatrix},\quad\bm{\Lambda}=\begin{pmatrix}\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab}\\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{pmatrix}
\]

\end_inset

The conditional distribution 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $p({\bf x}_{a}|{\bf x}_{b})$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 is given by
\begin_inset Formula 
\begin{eqnarray}
p({\bf x}_{a}|{\bf x}_{b}) & = & {\cal N}({\bf x}|\bm{\mu}_{a|b},\bm{\Lambda}_{aa}^{-1})\\
\bm{\mu}_{a|b} & = & \bm{\mu}_{a}-\bm{\Lambda}_{aa}^{-1}\bm{\Lambda}_{ab}({\bf x}_{a}-{\bf x}_{b})
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Subsection
Marginal Gaussian distributions
\end_layout

\begin_layout Standard
Same conditions as above, the marginal distribution 
\begin_inset Formula $p({\bf x}_{a})$
\end_inset

 is given by
\begin_inset Formula 
\[
p({\bf x}_{a})={\cal N}({\bf x}_{a}|\bm{\mu}_{a},\bm{\Sigma}_{aa})
\]

\end_inset


\end_layout

\begin_layout Subsection
Bayesâ€™ theorem for Gaussian variables
\end_layout

\begin_layout Standard
Given a marginal Gaussian distribution for 
\begin_inset Formula ${\bf x}$
\end_inset

 and a conditional Gaussian distribution for 
\begin_inset Formula ${\bf y}$
\end_inset

 given 
\begin_inset Formula ${\bf x}$
\end_inset

 in the form
\begin_inset Formula 
\begin{eqnarray}
p({\bf x}) & = & {\cal N}({\bf x}|\bm{\mu},\bm{\Lambda}^{-1})\\
p({\bf y}|{\bf x}) & = & {\cal N}({\bf y}|{\bf Ax}+{\bf b},{\bf L}^{-1})
\end{eqnarray}

\end_inset

the marginal distribution of 
\begin_inset Formula ${\bf y}$
\end_inset

 and the conditional distribution of 
\begin_inset Formula ${\bf x}$
\end_inset

 given 
\begin_inset Formula ${\bf y}$
\end_inset

 are given by
\begin_inset Formula 
\begin{eqnarray}
p({\bf y}) & = & {\cal N}({\bf y}|{\bf A}\bm{\mu}+{\bf b},{\bf L}^{-1}+{\bf A}\bm{\Lambda}^{-1}{\bf A}\trans\\
p({\bf x}|{\bf y}) & = & {\cal N}({\bf x}|\bm{\Sigma}\{{\bf A}\trans{\bf L}({\bf y}-{\bf b})+\bm{\Lambda\mu}\},\bm{\Sigma})
\end{eqnarray}

\end_inset

where
\begin_inset Formula 
\[
\bm{\Sigma}=(\bm{\Lambda}+{\bf A}\trans{\bf LA})^{-1}
\]

\end_inset


\end_layout

\begin_layout Subsection
Maximum likelihood for the Gaussian
\end_layout

\begin_layout Standard
Given a data set 
\begin_inset Formula ${\bf X}=({\bf x}_{1},\ldots,{\bf x}_{N})\trans$
\end_inset

 in which the observations 
\begin_inset Formula $\{{\bf x}_{n}\}$
\end_inset

 are assumed to be drawn independently from a multivariate Gaussian distribution.
 The log likelihood function is given by
\begin_inset Formula 
\begin{equation}
\ln p({\bf X}|\bm{\mu},\bm{\Sigma})=-\frac{ND}{2}\ln(2\pi)-\frac{N}{2}\ln|\bm{\Sigma}|-\frac{1}{2}\sum_{n=1}^{N}({\bf x}_{n}-\bm{\mu})\trans\bm{\Sigma}^{-1}({\bf x}_{n}-\bm{\mu})\label{eq:ln p(X | mu, Sigma)}
\end{equation}

\end_inset

 It depends on the data set only through the two quantities
\begin_inset Formula 
\[
\sum_{n=1}^{N}{\bf x}_{n},\qquad\sum_{n=1}^{N}{\bf x}_{n}{\bf x}_{n}\trans
\]

\end_inset

These are known as the 
\emph on
sufficient statistics
\emph default
 for the Gaussian distributions.
 The derivative of the log likelihood with respect to 
\begin_inset Formula $\bm{\mu}$
\end_inset

 is given by
\begin_inset Formula 
\[
\frac{\partial}{\partial\bm{\mu}}\ln p({\bf X}|\bm{\mu},\bm{\Sigma})=\sum_{n=1}^{N}\bm{\Sigma}^{-1}({\bf x}_{n}-\bm{\mu})
\]

\end_inset

and setting this derivative to zero, we obtain the solution
\begin_inset Formula 
\[
\bm{\mu}_{{\rm ML}}=\frac{1}{N}\sum_{n=1}^{N}{\bf x}_{n}
\]

\end_inset

The maximization of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(X | mu, Sigma)"

\end_inset

 is more involved and the result is given by
\begin_inset Formula 
\[
\bm{\Sigma}_{{\rm ML}}=\frac{1}{N}\sum_{n=1}^{N}({\bf x}_{n}-\bm{\mu}_{{\rm ML}})({\bf x}_{n}-\bm{\mu}_{{\rm ML}})\trans
\]

\end_inset


\end_layout

\begin_layout Standard
If we evaluate the expectations of the maximum likelihood solutions under
 the true distribution, we obtain the following results
\begin_inset Formula 
\begin{eqnarray*}
\EE[\bm{\mu}_{{\rm ML}}] & = & \bm{\mu}\\
\EE[\bm{\Sigma}_{{\rm ML}}] & = & \frac{N-1}{N}\bm{\Sigma}
\end{eqnarray*}

\end_inset

Hence the maximum likelihood estimate for the covariance is biased.
 We can correct this bias by defining a different estimator 
\begin_inset Formula $\widetilde{\bm{\Sigma}}$
\end_inset

 given by
\begin_inset Formula 
\[
\widetilde{\bm{\Sigma}}=\frac{1}{N-1}\sum_{n=1}^{N}({\bf x}_{n}-\bm{\mu}_{{\rm ML}})({\bf x}_{n}-\bm{\mu}_{{\rm ML}})\trans
\]

\end_inset


\end_layout

\begin_layout Subsection
Sequential estimation
\end_layout

\begin_layout Standard
Our discussion of the maximum likelihood solution for the parameters of
 a Gaussian distribution provides a convenient opportunity to give a more
 general discussion of the topic of sequential estimation for maximum likelihood.
 Sequential methods allow data points to be processed one at a time and
 then discarded and are important for on-line applications, and also where
 large data sets are involved so that batch processing of all data points
 at once is infeasible.
\end_layout

\begin_layout Standard
If we denote the maximum likelihood estimator of the mean 
\begin_inset Formula $\bm{\mu}_{{\rm ML}}^{(N)}$
\end_inset

 when it is based on 
\begin_inset Formula $N$
\end_inset

 observations, it can be simply shown
\begin_inset Formula 
\[
\bm{\mu}_{{\rm ML}}^{(N)}=\bm{\mu}_{{\rm ML}}^{(N-1)}+\frac{1}{N}({\bf x}_{N}-\bm{\mu}_{{\rm ML}}^{(N-1)})
\]

\end_inset

 This result has a nice interpretation.
 After observing 
\begin_inset Formula $N-1$
\end_inset

 data points we have estimate 
\begin_inset Formula $\bm{\mu}$
\end_inset

 by 
\begin_inset Formula $\bm{\mu}_{{\rm ML}}^{(N-1)}$
\end_inset

.
 We now observe data point 
\begin_inset Formula ${\bf x}_{N}$
\end_inset

, and we obtain our revised estimate 
\begin_inset Formula $\bm{\mu}_{{\rm ML}}^{(N)}$
\end_inset

 by moving the old estimate a small amount, proportional to 
\begin_inset Formula $1/N$
\end_inset

, in the direction of the `error signal' 
\begin_inset Formula $({\bf x}_{N}-\bm{\mu}_{{\rm ML}}^{(N-1)})$
\end_inset

.
 Note that, as 
\begin_inset Formula $N$
\end_inset

 increases, so the contribution from successive data points gets smaller.
\end_layout

\begin_layout Standard
The 
\emph on
Robbins-Monro
\emph default
 algorithm is a more general formulation of sequential leading.
 Consider a pair of random variable 
\begin_inset Formula $\theta$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

 governed by a joint distribution 
\begin_inset Formula $p(z,\theta)$
\end_inset

.
 The conditional expectation of 
\begin_inset Formula $z$
\end_inset

 given 
\begin_inset Formula $\theta$
\end_inset

 defines a deterministic function 
\begin_inset Formula $f(\theta)$
\end_inset

 that is given by
\begin_inset Formula 
\[
f(\theta)\equiv\EE[z|\theta]=\int zp(z|\theta)\d z
\]

\end_inset

The Robbins-Monro can find the root 
\begin_inset Formula $\theta^{\star}$
\end_inset

 at which 
\begin_inset Formula $f(\theta^{\star})=0$
\end_inset

 sequentially.
\end_layout

\begin_layout Subsection
Beyesian inference for the Gaussian
\end_layout

\begin_layout Standard
Now we develop a Bayesian treatment by introducing prior distributions over
 
\begin_inset Formula $\bm{\mu}$
\end_inset

 and 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

.
\end_layout

\begin_layout Standard
Suppose the covariance is known and mean is unknown, the conjugate prior
 of the mean is simply another Gaussian.
 In fact, the Bayesian paradigm leads very naturally to a sequential view
 of the inference problem.
\begin_inset Formula 
\[
p(\bm{\mu}|D)=\left[p(\bm{\mu})\prod_{n=1}^{N-1}(p({\bf x}_{n}|\bm{\mu})\right]p({\bf x}_{N}|\bm{\mu})
\]

\end_inset


\end_layout

\begin_layout Standard
The term in square brackets is (up to a normalization coefficient) just
 the posterior distribution after observing 
\begin_inset Formula $N-1$
\end_inset

 data points.
 We see that this can be viewed as a prior distribution, which is combined
 using Bayesâ€™ theorem with the likelihood function associated with data
 point 
\begin_inset Formula ${\bf x}_{N}$
\end_inset

 to arrive at the posterior distribution after observing 
\begin_inset Formula $N$
\end_inset

 data points.
 This sequential view of Bayesian inference is very general and applies
 to any problem in which the observed data are assumed to be independent
 and identically distributed.
\end_layout

\begin_layout Standard
Assume the mean is known and we wish to infer the variance.
 It's more convenient to work with the precision 
\begin_inset Formula $\lambda\equiv1/\sigma^{2}$
\end_inset

.
 The likelihood function for 
\begin_inset Formula $\lambda$
\end_inset

 takes the form
\begin_inset Formula 
\begin{equation}
p({\bf X}|\lambda)=\prod_{n=1}^{N}{\cal N}(x_{n}|\mu,\lambda^{-1})\propto\lambda^{N/2}\exp\left\{ -\frac{\lambda}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{2}\right\} \label{eq:p(X | lambda)}
\end{equation}

\end_inset

The corresponding conjugate prior should therefore be proportional to the
 product of a power of 
\begin_inset Formula $\lambda$
\end_inset

 and the exponential of a linear function of 
\begin_inset Formula $\lambda$
\end_inset

.
 This corresponds to the 
\emph on
gamma
\emph default
 distribution which is defined by
\begin_inset Formula 
\[
\Gam(\lambda|a,b)=\frac{1}{\Gamma(a)}b^{a}\lambda^{a-1}\exp(-b\lambda)
\]

\end_inset

The gamma distribution has a finite integral if 
\begin_inset Formula $a>0$
\end_inset

, and the distribution itself is finite if 
\begin_inset Formula $a\geq1$
\end_inset

.
 The mean and variance are given by
\begin_inset Formula 
\begin{eqnarray}
\EE[\lambda] & = & \frac{a}{b}\\
\var[\lambda] & = & \frac{a}{b^{2}}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Consider a prior distribution 
\begin_inset Formula $\Gam(\lambda|a_{0},b_{0})$
\end_inset

.
 If we multiply by the likelihood function 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(X | lambda)"

\end_inset

, then we obtain a posterior distribution
\begin_inset Formula 
\[
p(\lambda|{\bf X})\propto\lambda^{a_{0}-1}\lambda^{N/2}\exp\left\{ -b_{0}\lambda-\frac{\lambda}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{2}\right\} 
\]

\end_inset

we can find 
\begin_inset Formula $p(\mu|\lambda)$
\end_inset

 and 
\begin_inset Formula $p(\lambda)$
\end_inset

 by inspection.
 In particular, we see that 
\begin_inset Formula $p($
\end_inset

which we recognize as a gamma distribution of the form 
\begin_inset Formula $\Gam(\lambda|a_{N},b_{N})$
\end_inset

 where
\begin_inset Formula 
\begin{eqnarray}
a_{N} & = & a_{0}+\frac{N}{2}\label{eq:a_N for Gam(lambda | a_N, b_N)}\\
b_{N} & = & b_{0}+\frac{1}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{2}=b_{0}+\frac{N}{2}\sigma_{{\rm ML}}^{2}\label{eq:b_N for Gam(lambda | a_N, b_N)}
\end{eqnarray}

\end_inset

where 
\begin_inset Formula $\sigma_{{\rm ML}}^{2}$
\end_inset

 is the maximum likelihood estimator of the variance.
\end_layout

\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:a_N for Gam(lambda | a_N, b_N)"

\end_inset

, we see that the effect of observing 
\begin_inset Formula $N$
\end_inset

 data points is to increase the value of the coefficient 
\begin_inset Formula $a$
\end_inset

 by 
\begin_inset Formula $N/2$
\end_inset

.
 Thus we can interpret the parameter 
\begin_inset Formula $a_{0}$
\end_inset

 in the prior in terms of 
\begin_inset Formula $2a_{0}$
\end_inset

 `effective' prior observations.
 Similarly, from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:b_N for Gam(lambda | a_N, b_N)"

\end_inset

 we can interpret the parameter 
\begin_inset Formula $b_{0}$
\end_inset

 in the prior as arising from the 
\begin_inset Formula $2a_{0}$
\end_inset

 `effective' prior observations having variance 
\begin_inset Formula $b_{0}/a_{0}$
\end_inset

.
 Recall that we made an analogous interpretation for the Dirichlet prior.
 These distributions are examples of the exponential family, and we shall
 see that the interpretation of a conjugate prior in terms of effective
 fictitious data points is a general one for the exponential family of distribut
ions.
\end_layout

\begin_layout Standard
Instead of working with the precision, we can consider the variance itself.
 The conjugate prior in this case is called the 
\emph on
inverse gamma
\emph default
 distribution.
\end_layout

\begin_layout Standard
Now suppose that both the mean and the precision are unknown.
 To find a conjugate prior, we consider the dependence of the likelihood
 function on 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\lambda$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
p({\bf X}|\mu,\lambda) & = & \prod_{n=1}^{N}\left(\frac{\lambda}{2\pi}\right)^{1/2}\exp\left\{ -\frac{\lambda}{2}(x_{n}-\mu)^{2}\right\} \\
 & \propto & \left[\lambda^{1/2}\exp\left(-\frac{\lambda\mu^{2}}{2}\right)\right]^{N}\exp\left\{ \lambda\mu\sum_{n=1}^{N}x_{n}-\frac{\lambda}{2}\sum_{n=1}^{N}x_{n}^{2}\right\} 
\end{eqnarray*}

\end_inset


\begin_inset Formula 
\[
\]

\end_inset

We now wish to identify a prior distribution 
\begin_inset Formula $p(\mu,\lambda)$
\end_inset

 that has the same functional dependence on 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\lambda$
\end_inset

 as the likelihood function and that should therefore take the form
\begin_inset Formula 
\begin{eqnarray*}
p(\mu,\lambda) & \propto & \left[\lambda^{1/2}\exp\left(-\frac{\lambda\mu^{2}}{2}\right)\right]^{\beta}\exp\left\{ c\lambda\mu-d\lambda\right\} \\
 & = & \exp\left\{ -\frac{\beta\lambda}{2}(\mu-c/\beta)^{2}\right\} \lambda^{\beta/2}\exp\left\{ -\left(d-\frac{c^{2}}{2\beta}\right)\lambda\right\} 
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $c$
\end_inset

, 
\begin_inset Formula $d$
\end_inset

, and 
\begin_inset Formula $\beta$
\end_inset

 are constants.
 Since we can always write 
\begin_inset Formula $p(\mu,\lambda)=p(\mu|\lambda)p(\lambda)$
\end_inset

, we can find 
\begin_inset Formula $p(\mu|\lambda)$
\end_inset

 and 
\begin_inset Formula $p(\lambda)$
\end_inset

 by inspection, and we already know 
\begin_inset Formula $p(\mu|\lambda)$
\end_inset

 is a Gaussian whose precision is a linear function of 
\begin_inset Formula $\lambda$
\end_inset

 and that 
\begin_inset Formula $p(\lambda)$
\end_inset

 is a gamma distribution, so that the normalized prior takes the form
\begin_inset Formula 
\begin{equation}
p(\mu,\lambda)={\cal N}(\mu|\mu_{0},(\beta\lambda)^{-1})\Gam(\lambda|a,b)\label{eq:p(mu, lambda)}
\end{equation}

\end_inset

where we have defined new constants given by 
\begin_inset Formula $\mu_{0}=c/\beta$
\end_inset

, 
\begin_inset Formula $a=1+\beta/2$
\end_inset

, 
\begin_inset Formula $b=d-c^{2}/2\beta$
\end_inset

.
 The distribution 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(mu, lambda)"

\end_inset

 is called the 
\emph on
normal-gamma
\emph default
 or 
\emph on
Gaussian-gamma
\emph default
 distribution.
 Note that this is not simply the product of an independent Gaussian prior
 over 
\begin_inset Formula $\mu$
\end_inset

 and a gamma prior over 
\begin_inset Formula $\lambda$
\end_inset

, because the precision of 
\begin_inset Formula $\mu$
\end_inset

 is a linear function of 
\begin_inset Formula $\lambda$
\end_inset

.
 Even if we chose a prior in which 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $\lambda$
\end_inset

 were independent, the posterior distribution would exhibit a coupling between
 them.
\end_layout

\begin_layout Standard
In the case of the multivariate Gaussian distribution 
\begin_inset Formula ${\cal N}({\bf x}|\bm{\mu},\bm{\Lambda}^{-1})$
\end_inset

 for a 
\begin_inset Formula $D$
\end_inset

-dimensional variable 
\begin_inset Formula ${\bf x}$
\end_inset

, the conjugate prior distribution for the mean 
\begin_inset Formula $\bm{\mu}$
\end_inset

, assuming the precision is known, is again a Gaussian.
 For known mean and unknown precision matrix 
\begin_inset Formula $\bm{\Lambda}$
\end_inset

, the conjugate prior is the 
\emph on
Wishart
\emph default
 distribution given by
\begin_inset Formula 
\[
{\cal W}(\bm{\Lambda}|{\bf W},\nu)=B\left|\bm{\Lambda}\right|^{(\nu-D-1)/2}\exp\left(-\frac{1}{2}\Tr({\bf W}^{-1}\bm{\Lambda})\right)
\]

\end_inset

where 
\begin_inset Formula $\nu$
\end_inset

 is called the number of 
\emph on
degree of freedom
\emph default
 of the distribution, 
\begin_inset Formula ${\bf W}$
\end_inset

 is a 
\begin_inset Formula $D\times D$
\end_inset

 scale matrix, and the normalization constant 
\begin_inset Formula $B$
\end_inset

 is given by
\begin_inset Formula 
\[
B({\bf W},\nu)=\left|{\bf W}\right|^{\nu/2}\left(2^{\nu D/2}\pi^{D(D-1)/4}\prod_{i=1}^{D}\Gamma\left(\frac{\nu+1-i}{2}\right)\right)^{-1}
\]

\end_inset

Again, it is also possible to define a conjugate prior over the covariance
 matrix itself, rather than over the precision matrix, which leads to the
 
\emph on
inverse Wishart
\emph default
 distribution, although we shall not discuss this further.
 If both the mean and the precision are unknown, then, following a similar
 line of reasoning to the univariate case, the conjugate prior is given
 by
\begin_inset Formula 
\[
p(\bm{\mu},\bm{\Lambda}|\bm{\mu}_{0},\beta,{\bf W},\nu)={\cal N}(\bm{\mu}|\bm{\mu}_{0},(\beta\bm{\Lambda})^{-1}){\cal W}(\bm{\Lambda}|{\bf W},\nu)
\]

\end_inset

 which is known as the 
\emph on
normal-Wishart
\emph default
 or 
\emph on
Gaussian-Wishart
\emph default
 distribution.
\end_layout

\begin_layout Subsection
Student's t-distribution
\end_layout

\begin_layout Standard
We have seen that the conjugate prior for the precision of a Gaussian is
 given by a gamma distribution.
 If we have a univariate Gaussian 
\begin_inset Formula ${\cal N}(x|\mu,\tau^{-1})$
\end_inset

 together with a Gamma prior 
\begin_inset Formula $\Gam(\tau|a,b)$
\end_inset

 and we integrate out the precision, we obtain the marginal distribution
 of 
\begin_inset Formula $x$
\end_inset

 in the form
\begin_inset Formula 
\begin{eqnarray}
p(x|\mu,a,b) & = & \int_{0}^{\infty}{\cal N}(x|\mu,\tau^{-1})\Gam(\tau|a,b)\d\tau\label{eq:p(x|mu, a, b)=}\\
 & = & \frac{b^{a}}{\Gamma(a)}\left(\frac{1}{2\pi}\right)^{1/2}\left[b+\frac{(x-\mu)^{2}}{2}\right]^{-a-1/2}\Gamma(a+1/2)\nonumber 
\end{eqnarray}

\end_inset

By convention we define new parameters given by 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\nu=2a$
\end_inset

 and 
\begin_inset Formula $\lambda=a/b$
\end_inset

, in terms of which the distribution 
\begin_inset Formula $p(x|\mu,a,b)$
\end_inset

 takes the form
\begin_inset Formula 
\[
\St(x|\mu,\lambda,\nu)=\frac{\Gamma(\nu/2+1/2)}{\Gamma(\nu/2)}\left(\frac{\lambda}{\pi\nu}\right)^{1/2}\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\nu/2-1/2}
\]

\end_inset

which is known as 
\emph on
Student's t-distribution
\emph default
.
 The parameter 
\begin_inset Formula $\lambda$
\end_inset

 is sometimes called the 
\emph on
precision
\emph default
 of the t-distribution, even though it is not in general equal to the inverse
 of the variance.
 The parameter 
\begin_inset Formula $\nu$
\end_inset

 is called the 
\emph on
degrees of freedom
\emph default
.
 For the particular case of 
\begin_inset Formula $\nu=1$
\end_inset

, the t-distribution reduces to the 
\emph on
Cauchy
\emph default
 distribution, while in the limit 
\begin_inset Formula $\nu\to\infty$
\end_inset

 the t-distribution 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\St(x|\mu,\lambda,\nu)$
\end_inset

 becomes a Gaussian 
\begin_inset Formula ${\cal N}(x|\mu,\lambda^{-1})$
\end_inset

.
\end_layout

\begin_layout Standard
From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x|mu, a, b)="

\end_inset

, we see that Student's t-distribution is obtained by adding up an infinite
 number of Gaussian distributions having the same mean but different precisions.
 This can be interpreted as an infinite mixture of Gaussians.
 he result is a distribution that in general has longer `tails' than a Gaussian.
 This gives the t-distribution an important property called 
\emph on
robustness
\emph default
, which means that it is much less sensitive than the Gaussian to the presence
 of a few data points which are 
\emph on
outliers
\emph default
.
 Note that the maximum likelihood solution for the t-distribution can be
 found using the expectation-maximization (EM) algorithm.
 Outliers can arise in practical applications either because the process
 that generates the data corresponds to a distribution having a heavy tail
 or simply through mislabeled data.
 Robustness is also an important property for regression problems.
 Unsurprisingly, the least squares approach to regression does not exhibit
 robustness, because it corresponds to maximum likelihood under a (conditional)
 Gaussian distribution.
 By basing a regression model on a heavy-tailed distribution such as a t-distrib
ution, we obtain a more robust model.
\end_layout

\begin_layout Standard
If we go back to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x|mu, a, b)="

\end_inset

 and substitute the alternative parameters 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $\nu=2a$
\end_inset

, 
\begin_inset Formula $\lambda=a/b$
\end_inset

, and 
\begin_inset Formula $\eta=\tau b/a$
\end_inset

, we see that the t-distribution can be written in the form
\begin_inset Formula 
\[
\St(x|\mu,\lambda,\nu)=\int_{0}^{\infty}{\cal N}(x|\mu,(\eta\lambda)^{-1})\Gam(\eta|\nu/2,\nu/2)\d\eta
\]

\end_inset

We can generalize this to a multivariate Gaussian 
\begin_inset Formula ${\cal N}({\bf x}|\bm{\mu},\bm{\Lambda})$
\end_inset

 to obtain the corresponding multivariate Student's t-distribution in the
 form
\begin_inset Formula 
\[
\St({\bf x}|\bm{\mu},\bm{\Lambda},\nu)=\int_{0}^{\infty}{\cal N}({\bf x}|\bm{\mu},(\eta\bm{\Lambda})^{-1})\Gam(\eta|\nu/2,\nu/2)\d\eta
\]

\end_inset

We can evaluate this integral to give
\begin_inset Formula 
\[
\St({\bf x}|\bm{\mu},\bm{\Lambda},\nu)=\frac{\Gamma(D/2+\nu/2)}{\Gamma(\nu/2)}\frac{|\bm{\Lambda}|^{1/2}}{(\pi\nu)^{D/2}}\left[1+\frac{\Delta^{2}}{\nu}\right]^{-D/2-\nu/2}
\]

\end_inset

where 
\begin_inset Formula $D$
\end_inset

 is the dimensionality of 
\begin_inset Formula ${\bf x}$
\end_inset

, and 
\begin_inset Formula $\Delta^{2}$
\end_inset

 is the squared Mahalanobis distance defined by
\begin_inset Formula 
\[
\Delta^{2}=({\bf x}-\bm{\mu})\trans\bm{\Lambda}({\bf x}-\bm{\mu})
\]

\end_inset

The multivariate form of Student's t-distribution satisfies the following
 properties
\begin_inset Formula 
\begin{align}
\EE[{\bf x}] & =\bm{\mu} & \text{if}\quad\nu>1\\
\cov[{\bf x}] & =\frac{\nu}{(\nu-2)}\bm{\Lambda^{-1}} & \text{if}\quad\nu>2\\
\mode[{\bf x}] & =\bm{\mu}
\end{align}

\end_inset

with corresponding results for the univariate case.
\end_layout

\begin_layout Subsection
Periodic variables
\end_layout

\begin_layout Standard
Gaussian distributions are not always applicable for continuous variables,
 e.g., for periodic variables, which can conveniently be represented using
 an angular (polar) coordinate 
\begin_inset Formula $0\leq\theta<2\pi$
\end_inset

.
\end_layout

\begin_layout Standard
We might be tempted to treat periodic variables by choosing some direction
 as the origin and then applying a conventional distribution such as the
 Gaussian.
 Such an approach, however, would give results that were strongly dependent
 on the arbitrary choice of origin.
 For example, for two observations at 
\begin_inset Formula $\theta_{1}=1^{\circ}$
\end_inset

 and 
\begin_inset Formula $\theta_{2}=359^{\circ}$
\end_inset

, if we choose the origin at 
\begin_inset Formula $0^{\circ}$
\end_inset

 then the sample mean will be 
\begin_inset Formula $180^{\circ}$
\end_inset

 with the standard deviation 
\begin_inset Formula $179^{\circ}$
\end_inset

, whereas if we choose the origin at 
\begin_inset Formula $180^{\circ}$
\end_inset

, then the mean will be 
\begin_inset Formula $0^{\circ}$
\end_inset

 and the standard deviation will be 
\begin_inset Formula $1^{\circ}$
\end_inset

.
 We clearly need to develop a special approach for the treatment of periodic
 variables.
\end_layout

\begin_layout Standard
For a set of observations 
\begin_inset Formula ${\cal D}=\{\theta_{1},\ldots,\theta_{N}\}$
\end_inset

, to find an invariant measure of the mean, we note that the observations
 can be viewed as points on the unit circle and can therefore be described
 instead by two-dimensional unit vectors 
\begin_inset Formula ${\bf x}_{1},\ldots,{\bf x}_{N}$
\end_inset

 where 
\begin_inset Formula $\left\Vert {\bf x}_{n}\right\Vert =1$
\end_inset

 for 
\begin_inset Formula $n=1,\ldots,N$
\end_inset

.
 We can average the vectors 
\begin_inset Formula $\{{\bf x}_{n}\}$
\end_inset

 to give
\begin_inset Formula 
\[
\overline{{\bf x}}=\frac{1}{N}\sum_{n=1}^{N}{\bf x}_{n}
\]

\end_inset

and then find the corresponding angle 
\begin_inset Formula $\overline{\theta}$
\end_inset

 of this average.
 We can solve for 
\begin_inset Formula $\overline{\theta}$
\end_inset

 to give
\begin_inset Formula 
\begin{equation}
\overline{\theta}=\tan^{-1}\left\{ \frac{\sum_{n}\sin\theta_{n}}{\sum_{n}\cos\theta_{n}}\right\} \label{eq:overline theta}
\end{equation}

\end_inset

Shortly, we shall see how this result arises naturally as the maximum likelihood
 estimator for an appropriately defined distribution over a periodic variable.
\end_layout

\begin_layout Standard
We now consider a periodic generalization of the Gaussian called the 
\emph on
von Mises
\emph default
 distribution.
 Here we shall limit our attention to univariate distributions, although
 periodic distributions can also be found over hyperspheres of arbitrary
 dimension.
\end_layout

\begin_layout Standard
By convention, we will consider 
\begin_inset Formula $p(\theta)$
\end_inset

 that have period 
\begin_inset Formula $2\pi$
\end_inset

 which must satisfy the three conditions
\begin_inset Formula 
\begin{eqnarray}
p(\theta) & \geq & 0\\
\int_{0}^{2\pi}p(\theta)\d\theta & = & 1\\
p(\theta+2\pi) & = & p(\theta)
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
We can easily obtain a Gaussian-like distribution that satisfies these three
 properties as follows.
 Consider a Gaussian distribution over two variables 
\begin_inset Formula ${\bf x}=(x_{1},x_{2})$
\end_inset

 having mean 
\begin_inset Formula $\bm{\mu}=(\mu_{1},\mu_{2})$
\end_inset

 and a covariance matrix 
\begin_inset Formula $\bm{\Sigma}=\sigma{\bf I}_{2}$
\end_inset

, so that
\begin_inset Formula 
\[
p(x_{1},x_{2})=\frac{1}{2\pi\sigma^{2}}\exp\left\{ -\frac{(x_{1}-\mu_{1})^{2}+(x_{2}-\mu_{2})^{2}}{2\sigma^{2}}\right\} 
\]

\end_inset

By transforming from Cartesian coordinates 
\begin_inset Formula $(x_{1},x_{2})$
\end_inset

 to polar coordinates 
\begin_inset Formula $(r,\theta)$
\end_inset

 and 
\begin_inset Formula $(\mu_{1},\mu_{2})$
\end_inset

 to 
\begin_inset Formula $(r_{0},\theta_{0})$
\end_inset

, then condition on the unit circle 
\begin_inset Formula $r=1$
\end_inset

, we obtain our final expression for the distribution of 
\begin_inset Formula $p(\theta)$
\end_inset

 along the unit circle 
\begin_inset Formula $r=1$
\end_inset

 in the form
\begin_inset Formula 
\begin{equation}
p(\theta|\theta_{0},m)=\frac{1}{2\pi I_{0}(m)}\exp\left\{ m\cos(\theta-\theta_{0})\right\} \label{eq:p(theta | theta_0, m)}
\end{equation}

\end_inset

which is called the 
\emph on
von Mises
\emph default
 distribution, or the 
\emph on
circular normal
\emph default
.
 Here 
\begin_inset Formula $\theta_{0}$
\end_inset

 corresponds to the mean of the distribution, while 
\begin_inset Formula $m$
\end_inset

, which is known as the 
\emph on
concentration
\emph default
 parameter, is analogous to the precision for the Gaussian.
 The normalization coefficient for 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(theta | theta_0, m)"

\end_inset

 is expressed in terms of 
\begin_inset Formula $I_{0}(m)$
\end_inset

, which is the zeroth-order Bessel function of the first kind and is defined
 by
\begin_inset Formula 
\[
I_{0}(m)=\frac{1}{2\pi}\int_{0}^{2\pi}\exp\{m\cos\theta\}\d\theta
\]

\end_inset

For large 
\begin_inset Formula $m$
\end_inset

, the distribution becomes approximately Gaussian.
\end_layout

\begin_layout Standard
Now consider the maximum likelihood estimators for the parameters 
\begin_inset Formula $\theta_{0}$
\end_inset

 and 
\begin_inset Formula $m$
\end_inset

.
 The log likelihood functions is given by
\begin_inset Formula 
\begin{equation}
\ln p({\cal D}|\theta_{0},m)=-N\ln(2\pi)-N\ln I_{0}(m)+m\sum_{n=1}^{N}\cos(\theta_{n}-\theta_{0})\label{eq:ln p(D | theta_0, m)}
\end{equation}

\end_inset

 Setting the derivative with respect to 
\begin_inset Formula $\theta_{0}$
\end_inset

 equal to zero gives
\begin_inset Formula 
\[
\sum_{n=1}^{N}\sin(\theta_{n}-\theta_{0})=0
\]

\end_inset

from which we obtain
\begin_inset Formula 
\[
\theta_{0}^{{\rm ML}}=\tan^{-1}\left\{ \frac{\sum_{n}\sin\theta_{n}}{\sum_{n}\cos\theta_{n}}\right\} 
\]

\end_inset

which we recognized as the result 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:overline theta"

\end_inset

.
\end_layout

\begin_layout Standard
Similarly, maximizing 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ln p(D | theta_0, m)"

\end_inset

 with respect to 
\begin_inset Formula $m$
\end_inset

, and make use of 
\begin_inset Formula $I_{0}^{\prime}(m)=I_{1}(m)$
\end_inset

, we have
\begin_inset Formula 
\begin{equation}
A(m)=\frac{1}{N}\sum_{n=1}^{N}\cos(\theta_{n}-\theta_{0}^{{\rm ML}})\label{eq:A(m)}
\end{equation}

\end_inset

where we have defined
\begin_inset Formula 
\[
A(m)=\frac{I_{1}(m)}{I_{0}(m)}
\]

\end_inset

We can write 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:A(m)"

\end_inset

 in the form
\begin_inset Formula 
\begin{equation}
A(m_{{\rm ML}})=\left(\frac{1}{N}\sum_{n=1}^{N}\cos\theta_{n}\right)\cos\theta_{0}^{{\rm ML}}-\left(\frac{1}{N}\sum_{n=1}^{N}\sin\theta_{n}\right)\sin\theta_{0}^{{\rm ML}}\label{eq:A(m_ML)}
\end{equation}

\end_inset

The right hand side of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:A(m_ML)"

\end_inset

 is easily evaluated, and the function 
\begin_inset Formula $A(m)$
\end_inset

 can be inverted numerically.
\end_layout

\begin_layout Standard
One limitation of the von Mises distribution is that it is unimodal.
 By forming 
\emph on
mixtures
\emph default
 of von Mises distributions, we obtain a flexible framework for modeling
 periodic variables that can handle multimodality.
\end_layout

\begin_layout Subsection
Mixtures of Gaussians
\end_layout

\begin_layout Standard
The Gaussian distribution suffers from significant limitations when it comes
 to modeling real data sets, for example, the `Old Faithful' data set, which
 forms two dominant clumps, whereas a linear superposition of two Gaussians
 gives a better characterization of the data set.
\end_layout

\begin_layout Standard
Such superpositions, formed by taking linear combinations of more basic
 distributions such as Gaussians, can be formulated as probabilistic models
 known as 
\emph on
mixture distributions
\emph default
.
 By using a sufficient number of Gaussians, and by adjusting their means
 and covariances as well as the coefficients in the linear combination,
 almost any continuous density can be approximated to arbitrary accuracy.
\end_layout

\begin_layout Standard
We therefore consider a superposition of 
\begin_inset Formula $K$
\end_inset

 Gaussian densities of the form
\begin_inset Formula 
\begin{equation}
p({\bf x})=\sum_{k=1}^{K}\pi_{k}{\cal N}({\bf x}|\bm{\mu}_{k},\bm{\Sigma}_{k})\label{eq:p(x) mixture of Gaussians}
\end{equation}

\end_inset

which is called a 
\emph on
mixture of Gaussians
\emph default
.
 Each Gaussian density 
\begin_inset Formula ${\cal N}({\bf x}|\bm{\mu}_{k},\bm{\Sigma}_{k})$
\end_inset

 is called a 
\emph on
component
\emph default
 of the mixture and has its own mean 
\begin_inset Formula $\bm{\mu}_{k}$
\end_inset

 and covariance 
\begin_inset Formula $\bm{\Sigma}_{k}$
\end_inset

.
\end_layout

\begin_layout Standard
In this section we shall consider Gaussian components to illustrate the
 framework of mixture models.
 More generally, mixture models can comprise linear combinations of other
 distributions.
\end_layout

\begin_layout Standard
The parameters 
\begin_inset Formula $\pi_{k}$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x) mixture of Gaussians"

\end_inset

 are called 
\emph on
mixing coefficients
\emph default
.
 It's easy to verify they should satisfy 
\begin_inset Formula $\sum_{k=1}^{K}\pi_{k}=1$
\end_inset

 and 
\begin_inset Formula $0\leq p({\bf x})\leq1$
\end_inset

 for 
\begin_inset Formula $p({\bf x})$
\end_inset

 to be probability.
\end_layout

\begin_layout Standard
From the sum and product rules, the marginal density is given by
\begin_inset Formula 
\[
p({\bf x})=\sum_{k=1}^{K}p(k)p({\bf x}|k)
\]

\end_inset

which is equivalent to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x) mixture of Gaussians"

\end_inset

 in which we can view 
\begin_inset Formula $\pi_{k}=p(k)$
\end_inset

 as the prior probability of picking the 
\begin_inset Formula $k^{{\rm th}}$
\end_inset

 component, and the density 
\begin_inset Formula ${\cal N}({\bf x}|\bm{\mu}_{k},\bm{\Sigma}_{k})=p({\bf x}|k)$
\end_inset

 as the probability of 
\begin_inset Formula ${\bf x}$
\end_inset

 conditioned on 
\begin_inset Formula $k$
\end_inset

.
 An important role is played by the posterior probabilities 
\begin_inset Formula $p(k|{\bf x})$
\end_inset

, which are also known as 
\emph on
responsibilities
\emph default
.
 From Bayes' theorem these are given by
\begin_inset Formula 
\begin{eqnarray}
\gamma_{k}({\bf x}) & \equiv & p(k|{\bf x})\nonumber \\
 & = & \frac{p(k)p({\bf x}|k)}{\sum_{l}p(l)p({\bf x}|l)}\nonumber \\
 & = & \frac{\pi_{k}{\cal N}({\bf x}|\bm{\mu}_{k},\bm{\Sigma}_{k})}{\sum_{l}\pi_{l}{\cal N}({\bf x}|\bm{\mu}_{l},\bm{\Sigma}_{l})}
\end{eqnarray}

\end_inset

We shall discuss the probabilistic interpretation of the mixture distribution
 in greater detail in Chapter 9.
\end_layout

\begin_layout Standard
The form of the Gaussian mixture distribution is governed by the parameters
 
\begin_inset Formula $\bm{\pi}$
\end_inset

, 
\begin_inset Formula $\bm{\mu}$
\end_inset

 and 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

, where we have used the notation 
\begin_inset Formula $\bm{\pi}\equiv\{\pi_{1},\ldots,\pi_{K}\}$
\end_inset

, 
\begin_inset Formula $\bm{\mu}\equiv\{\bm{\mu}_{1},\ldots,\bm{\mu}_{K}\}$
\end_inset

 and 
\begin_inset Formula $\bm{\Sigma}\equiv\{\bm{\Sigma}_{1},\ldots,\bm{\Sigma}_{K}\}$
\end_inset

.
 One way to set the values of these parameters is to use maximum likelihood.
 From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x) mixture of Gaussians"

\end_inset

 the log of the likelihood function is given by
\begin_inset Formula 
\[
\ln p({\bf X}|\bm{\pi},\bm{\mu},\bm{\Sigma})=\sum_{n=1}^{N}\ln\left\{ \sum_{k=1}^{K}\pi_{k}{\cal N}({\bf x}_{n}|\bm{\mu}_{k},\bm{\Sigma}_{k})\right\} 
\]

\end_inset

where 
\begin_inset Formula ${\bf X}=\{{\bf x}_{1},\ldots,{\bf x}_{N}\}$
\end_inset

.
 We immediately see that the situation is now much more complex than with
 a single Gaussian, due to the presence of the summation over k inside the
 logarithm.
 As a result, the maximum likelihood solution for the parameters no longer
 has a closed-form analytical solution.
 One approach to maximizing the likelihood function is to use iterative
 numerical optimization techniques.
 Alternatively we can employ a powerful framework called 
\emph on
expectation maximization
\emph default
, which will be discussed at length in Chapter 9.
\end_layout

\begin_layout Section
The Exponential Family
\end_layout

\begin_layout Standard
The probability distributions that we have studied so far in this chapter
 (with the exception of the Gaussian mixture) are specific examples of a
 broad class of distributions called the 
\emph on
exponential family
\emph default
.
 Members of the exponential family have many important properties in common,
 and it is illuminating to discuss these properties in some generality.
\end_layout

\begin_layout Standard
The exponential family of distributions over 
\begin_inset Formula ${\bf x}$
\end_inset

, given parameters 
\begin_inset Formula $\bm{\eta}$
\end_inset

, is defined to be the set of distributions of the form
\begin_inset Formula 
\begin{equation}
p({\bf x}|\bm{\eta})=h({\bf x})g(\bm{\eta})\exp\left\{ \bm{\eta}\trans{\bf u}({\bf x})\right\} \label{eq:p(x | eta)}
\end{equation}

\end_inset

where 
\begin_inset Formula ${\bf x}$
\end_inset

 may be scalar or vector, and may be discrete or continuous.
 Here 
\begin_inset Formula $\bm{\eta}$
\end_inset

 are called the 
\emph on
natural parameters
\emph default
 of the distribution, and 
\begin_inset Formula ${\bf u}({\bf x})$
\end_inset

 is some function of 
\begin_inset Formula ${\bf x}$
\end_inset

.
 The function 
\begin_inset Formula $g(\bm{\eta})$
\end_inset

 can be interpreted as the coefficient that ensures that the distribution
 is normalized and therefore satisfies
\begin_inset Formula 
\begin{equation}
g(\bm{\eta})\int h({\bf x})\exp\left\{ \bm{\eta}\trans{\bf u}({\bf x})\right\} \d{\bf x}=1\label{eq:condition of g(eta)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Consider first the Bernoulli distribution
\begin_inset Formula 
\[
p(x|\mu)=\Bern(x|\mu)=\mu^{x}(1-\mu)^{1-x}
\]

\end_inset

Expressing the right-hand side as the exponential of the logarithm we have
\begin_inset Formula 
\begin{eqnarray}
p(x|\mu) & = & \exp\left\{ x\ln\mu+(1-x)\ln(1-\mu)\right\} \nonumber \\
 & = & (1-\mu)\exp\left\{ \ln\left(\frac{\mu}{1-\mu}\right)x\right\} 
\end{eqnarray}

\end_inset

Comparison with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x | eta)"

\end_inset

 allows us to identify
\begin_inset Formula 
\[
\eta=\ln\left(\frac{\mu}{1-\mu}\right)
\]

\end_inset

which can solve for 
\begin_inset Formula $\mu$
\end_inset

 to give 
\begin_inset Formula $\mu=\sigma(\eta)$
\end_inset

, where
\begin_inset Formula 
\[
\sigma(\eta)=\frac{1}{1+\exp(-\eta)}
\]

\end_inset

is called the 
\emph on
logistic sigmoid
\emph default
 function.
 Thus we can write the Bernoulli distribution using the standard representation
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x | eta)"

\end_inset

 in the form
\begin_inset Formula 
\[
p(x|\eta)=\sigma(-\eta)\exp(\eta x)
\]

\end_inset

Comparison with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x | eta)"

\end_inset

 shows that
\begin_inset Formula 
\begin{eqnarray}
u(x) & = & x\\
h(x) & = & 1\\
g(\eta) & = & \sigma(-\eta)
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Next consider the multinomial distribution that, for a single observation
 
\begin_inset Formula ${\bf x}$
\end_inset

, takes the form
\begin_inset Formula 
\[
p({\bf x}|\bm{\mu})=\prod_{k=1}^{M}\mu_{k}^{x_{k}}=\exp\left\{ \sum_{k=1}^{M}x_{k}\ln\mu_{k}\right\} 
\]

\end_inset

where 
\begin_inset Formula ${\bf x}=(x_{1},\ldots,x_{N})\trans$
\end_inset

.
 We can write this in the standard representation so that
\begin_inset Formula 
\[
p({\bf x}|\bm{\eta})=\exp(\bm{\eta}\trans{\bf x})
\]

\end_inset

where 
\begin_inset Formula $\eta_{k}=\ln\mu_{k}$
\end_inset

 and 
\begin_inset Formula $\bm{\eta}=(\eta_{1},\ldots,\eta_{M})\trans$
\end_inset

.
 Again, comparing with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x | eta)"

\end_inset

 we have
\begin_inset Formula 
\begin{eqnarray}
{\bf u}({\bf x}) & = & {\bf x}\\
h({\bf x}) & = & 1\\
g(\bm{\eta}) & = & 1
\end{eqnarray}

\end_inset

Note that the parameters 
\begin_inset Formula $\eta_{k}$
\end_inset

 are not independent because the parameters 
\begin_inset Formula $\mu_{k}$
\end_inset

 are subject to the constraint
\begin_inset Formula 
\begin{equation}
\sum_{k=1}^{M}\mu_{k}=1\label{eq:u_k constraint}
\end{equation}

\end_inset

In some circumstances, it will be convenient to remove this constraint by
 expressing the distribution in terms of only 
\begin_inset Formula $M-1$
\end_inset

 parameters.
 Making use of the constraint 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:u_k constraint"

\end_inset

 to give
\begin_inset Formula 
\[
\mu_{k}=\frac{\exp(\eta_{k})}{1+\sum_{j}\exp(\eta_{j})}
\]

\end_inset

This is called the 
\emph on
softmax
\emph default
 function, or the 
\emph on
normalized exponential
\emph default
.
 In this representation, the multinomial distribution therefore takes the
 form
\begin_inset Formula 
\[
p({\bf x}|\bm{\eta})=\left(1+\sum_{k=1}^{M-1}\exp(\eta_{k})\right)^{-1}\exp(\bm{\eta}\trans{\bf x})
\]

\end_inset

This is the standard form of the exponential family, with parameter vector
 
\begin_inset Formula $\bm{\eta}=(\eta_{1},\ldots,\eta_{M-1})\trans$
\end_inset

 in which
\begin_inset Formula 
\begin{eqnarray}
{\bf u}({\bf x}) & = & {\bf x}\\
h({\bf x}) & = & 1\\
g(\bm{\eta}) & = & \left(1+\sum_{k=1}^{M-1}\exp(\eta_{k})\right)^{-1}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Finally, let us consider the Gaussian distribution.
 For the univariate Gaussian, we have
\begin_inset Formula 
\begin{eqnarray}
p(x|\mu,\sigma^{2}) & = & \frac{1}{(2\pi\sigma^{2})^{1/2}}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} \nonumber \\
 & = & \frac{1}{(2\pi\sigma^{2})^{1/2}}\exp\left\{ -\frac{1}{2\sigma^{2}}x^{2}+\frac{\mu}{\sigma^{2}}x-\frac{1}{2\sigma^{2}}\mu^{2}\right\} 
\end{eqnarray}

\end_inset

which, after some rearrangement, can be cast in the standard exponential
 family form 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x | eta)"

\end_inset

 with
\begin_inset Formula 
\begin{eqnarray}
\bm{\eta} & = & \begin{pmatrix}\mu/\sigma^{2}\\
-1/2\sigma^{2}
\end{pmatrix}\\
{\bf u}(x) & = & \begin{pmatrix}x\\
x^{2}
\end{pmatrix}\\
h({\bf x)} & = & (2\pi)^{-1/2}\\
g(\bm{\eta}) & = & (-2\eta_{2})^{1/2}\exp\left(\frac{\eta_{1}^{2}}{4\eta_{2}}\right)
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Subsection
Maximum likelihood and sufficient statistics
\end_layout

\begin_layout Standard
Let us now consider the problem of estimating the parameter vector 
\begin_inset Formula $\bm{\eta}$
\end_inset

 in the general exponential family distribution 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x | eta)"

\end_inset

 using the technique of maximum likelihood.
 Taking the gradient of both sides of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:condition of g(eta)"

\end_inset

 with respect to 
\begin_inset Formula $\bm{\eta}$
\end_inset

, and use 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x | eta)"

\end_inset

, we can obtain the result
\begin_inset Formula 
\[
-\nabla\ln g(\bm{\eta})=\EE[{\bf u}({\bf x})]
\]

\end_inset

Note that the covariance of 
\begin_inset Formula ${\bf u}({\bf x})$
\end_inset

 can be expressed in terms of the second derivatives of 
\begin_inset Formula $g(\bm{\eta})$
\end_inset

, and similarly for higher order moments.
 Thus, provided we can normalize a distribution from the exponential family,
 we can always find its moments by simple differentiation.
\end_layout

\begin_layout Standard
Now consider a set of i.i.d.
 data denoted by 
\begin_inset Formula ${\bf X}=\{{\bf x}_{1},\ldots,{\bf x}_{n}\}$
\end_inset

, for which the likelihood function is given by
\begin_inset Formula 
\begin{equation}
p({\bf X}|\bm{\eta})=\left(\prod_{n=1}^{N}h({\bf x}_{n})\right)g(\bm{\eta})^{N}\exp\left\{ \bm{\eta}\trans\sum_{n=1}^{N}{\bf u}({\bf x}_{n})\right\} \label{eq:p(X | eta)}
\end{equation}

\end_inset

Setting the gradient of 
\begin_inset Formula $\ln p({\bf X}|\bm{\eta})$
\end_inset

 with respect to 
\begin_inset Formula $\bm{\eta}$
\end_inset

 to zero, we get the following condition to be satisfied by the maximum
 likelihood estimator 
\begin_inset Formula $\bm{\eta}_{{\rm ML}}$
\end_inset


\begin_inset Formula 
\[
-\nabla\ln g(\bm{\eta}_{{\rm ML}})=\frac{1}{N}\sum_{n=1}^{N}{\bf u}({\bf x}_{n})
\]

\end_inset

which can in principle be solved to obtain 
\begin_inset Formula $\bm{\eta}_{{\rm ML}}$
\end_inset

.
 We see that the 
\emph on
sufficient statistic
\emph default
 of the distribution 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x | eta)"

\end_inset

 is 
\begin_inset Formula $\sum_{n}{\bf u}({\bf x}_{n})$
\end_inset

.
 For example, for the Gaussian 
\begin_inset Formula ${\bf u}(x)=(x,x^{2})\trans$
\end_inset

, so we should keep both the sum of 
\begin_inset Formula $\{x_{n}\}$
\end_inset

 and the sum of 
\begin_inset Formula $\{x_{n}^{2}\}$
\end_inset

.
\end_layout

\begin_layout Subsection
Conjugate priors
\end_layout

\begin_layout Standard
In general, for a given probability distribution 
\begin_inset Formula $p({\bf x}|\bm{\eta})$
\end_inset

, we can seek a prior 
\begin_inset Formula $p(\bm{\eta})$
\end_inset

 that is conjugate to the likelihood function, so that the posterior distributio
n has the same functional form as the prior.
 For any member of the exponential family 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x | eta)"

\end_inset

, there exists a conjugate prior that can be written in the form
\begin_inset Formula 
\begin{equation}
p(\bm{\eta}|\bm{\chi},\nu)=f(\bm{\chi},\nu)g(\bm{\eta})^{\nu}\exp\left\{ \nu\bm{\eta}\trans\bm{\chi}\right\} \label{eq:p(eta | chi, nu)}
\end{equation}

\end_inset

where 
\begin_inset Formula $f(\bm{\chi},\nu)$
\end_inset

 is a normalization coefficient, and 
\begin_inset Formula $g(\bm{\eta})$
\end_inset

 is the same function as appears in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x | eta)"

\end_inset

.
 To multiply the prior 
\begin_inset Formula $ $
\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(eta | chi, nu)"

\end_inset

 by the likelihood function 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(X | eta)"

\end_inset

 will obtain the posterior distribution, up to a normalization coefficient,
 in the form
\begin_inset Formula 
\[
p(\bm{\eta}|{\bf X},\bm{\chi},\nu)\propto g(\bm{\eta})^{\nu+N}\exp\left\{ \bm{\eta}\trans\left(\sum_{n=1}^{N}{\bf u}({\bf x}_{n})+\nu\bm{\chi}\right)\right\} 
\]

\end_inset

This again takes the same functional form as the prior 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(eta | chi, nu)"

\end_inset

, confirming conjugacy.
 Furthermore, we see that the parameter 
\begin_inset Formula $\nu$
\end_inset

 can be interpreted as an effective number of pseudo-observations in the
 prior, each of which has a value for the sufficient statistic 
\begin_inset Formula ${\bf u}({\bf x})$
\end_inset

 given by 
\begin_inset Formula $\bm{\chi}$
\end_inset

.
\end_layout

\begin_layout Subsection
Noninformative priors
\end_layout

\begin_layout Standard
In many cases, we may have little idea of what form the distribution should
 like.
 We may then seek a form of prior distribution, called a 
\emph on
noninformative prior
\emph default
, which is intended to have as little influence on the posterior distribution
 as possible.
 This is sometimes referred to as `letting the data speak for themselves'.
\end_layout

\begin_layout Standard
If we have a distribution 
\begin_inset Formula $p(x|\lambda)$
\end_inset

 governed by a parameter 
\begin_inset Formula $\lambda$
\end_inset

, we might be tempted to propose a prior distribution 
\begin_inset Formula $p(\lambda)=\text{const}$
\end_inset

.
 If 
\begin_inset Formula $\lambda$
\end_inset

 is a discrete variable with 
\begin_inset Formula $K$
\end_inset

 states, simply set 
\begin_inset Formula $p(\lambda)=1/K$
\end_inset

.
 In the case of continuous parameters, there are two difficulties.
 First, if the domain of 
\begin_inset Formula $\lambda$
\end_inset

 is unbounded, the prior distribution cannot be correctly normalized.
 Such priors are called 
\emph on
improper
\emph default
.
 In practice, improper priors can often be used provided the corresponding
 posterior distribution is 
\emph on
proper
\emph default
, i.e., that it can be correctly normalized.
\end_layout

\begin_layout Standard
A second difficulty arises from the transformation behavior of a probability
 density under a nonlinear change of variables, given by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p_y(y)=p_x(..."

\end_inset

.
 If a function 
\begin_inset Formula $h(\lambda)$
\end_inset

 is constant and 
\begin_inset Formula $\lambda=g(\eta)$
\end_inset

, 
\begin_inset Formula $ $
\end_inset

then 
\begin_inset Formula $\widehat{h}(\eta)=h(g(\eta))$
\end_inset

 will also be constant.
 However, if we choose the density 
\begin_inset Formula $p_{\lambda}(\lambda)$
\end_inset

 to be constant, then the density of 
\begin_inset Formula $\eta$
\end_inset

 will be given, from 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p_y(y)=p_x(..."

\end_inset

, by
\begin_inset Formula 
\[
p_{\eta}(\eta)=p_{\lambda}(\lambda)\left|\frac{\d\lambda}{\d\eta}\right|=p_{\lambda}(g(\eta))\left|g'(\eta)\right|\propto\left|g'(\eta)\right|
\]

\end_inset

and so the density over 
\begin_inset Formula $\eta$
\end_inset

 will not be constant.
 This issue does not arise when we use maximum likelihood, because the likelihoo
d function 
\begin_inset Formula $p(x|\lambda)$
\end_inset

 is a simple function of 
\begin_inset Formula $\lambda$
\end_inset

 and so we are free to use any convenient parameterization.
 If, however, we are to choose a prior distribution that is constant, we
 must take care to use an appropriate representation for the parameters.
\end_layout

\begin_layout Standard
Here we consider two simple examples of noninformative priors.
 First of all, if a density takes the form
\begin_inset Formula 
\[
p(x|\mu)=f(x-\mu)
\]

\end_inset

then the parameter 
\begin_inset Formula $\mu$
\end_inset

 is known as a 
\emph on
location parameter
\emph default
.
 This family of densities exhibits 
\emph on
translation invariance
\emph default
 between if we shift 
\begin_inset Formula $x$
\end_inset

 by a constant to give 
\begin_inset Formula $\widehat{x}=x+c$
\end_inset

, then
\begin_inset Formula 
\[
p(\widehat{x}|\widehat{\mu})=f(\widehat{x}-\widehat{\mu})
\]

\end_inset

where we have defined 
\begin_inset Formula $\widehat{\mu}=\mu+c$
\end_inset

.
 Thus the density takes the same form in the new variable as in the original
 one, and so the density is independent of the choice of origin.
 We would like to choose a prior distribution that reflects this translation
 invariance property, which can be proved 
\begin_inset Formula $p(\mu)$
\end_inset

 is constant.
 An example of a location parameter would be the mean 
\begin_inset Formula $\mu$
\end_inset

 of a Gaussian distribution.
 As we have seen, the conjugate prior distribution for 
\begin_inset Formula $\mu$
\end_inset

 in this case is a Gaussian 
\begin_inset Formula $p(\mu|\mu_{0},\sigma_{0}^{2})={\cal N}(\mu|\mu_{0},\sigma_{0}^{2})$
\end_inset

, and we obtain a noninformative prior by taking the limit 
\begin_inset Formula $\sigma_{0}^{2}\to\infty$
\end_inset

.
\end_layout

\begin_layout Standard
As a second example, consider a density of the form
\begin_inset Formula 
\[
p(x|\sigma)=\frac{1}{\sigma}f\left(\frac{x}{\sigma}\right)
\]

\end_inset

where 
\begin_inset Formula $\sigma>0$
\end_inset

.
 The parameter 
\begin_inset Formula $\sigma$
\end_inset

 is known as a 
\emph on
scale parameter
\emph default
, and the density exhibits 
\emph on
scale invariance
\emph default
 because if we scale 
\begin_inset Formula $x$
\end_inset

 by a constant to give 
\begin_inset Formula $\widehat{x}=cx$
\end_inset

, then
\begin_inset Formula 
\[
p(\widehat{x}|\widehat{\sigma})=\frac{1}{\widehat{\sigma}}f\left(\frac{\widehat{x}}{\widehat{\sigma}}\right)
\]

\end_inset

where we have defined 
\begin_inset Formula $\widehat{\sigma}=c\sigma$
\end_inset

.
 We can use this property to prove 
\begin_inset Formula $p(\sigma)\propto1/\sigma$
\end_inset

.
 Note that again this is an improper prior because its integral is divergent.
 It is sometimes also convenient to think of the prior distribution for
 a scale parameter in terms of the density of the log of the parameter,
 so 
\begin_inset Formula $p(\ln\sigma)=\text{const}$
\end_inset

.
 An example of a scale parameter would be the standard deviation 
\begin_inset Formula $\sigma$
\end_inset

 of a Gaussian distribution.
\end_layout

\begin_layout Section
Nonparametric Methods
\end_layout

\begin_layout Standard
Throughout this chapter, we have focussed on the use of probability distribution
s having specific functional forms governed by a small number of parameters
 whose values are to be determined from a data set.
 This is called the 
\emph on
parametric
\emph default
 approach to density modeling.
 An important limitation of this approach is that the chosen density might
 be a poor model of the distribution that generates the data, which can
 result in poor predictive performance.
 For instance, if the process that generates the data is multimodal, then
 this aspect of the distribution can never be captured by a Gaussian, which
 is necessarily unimodal.
\end_layout

\begin_layout Standard
In this section, we consider some 
\emph on
nonparametric
\emph default
 approaches to density estimation that make few assumptions about the form
 of the distribution.
 Here we shall focus mainly on simple frequentist methods.
 The reader should be aware, however, that nonparametric Bayesian methods
 are attracting increasing interest.
\end_layout

\begin_layout Standard
The histogram method simply partition random variable 
\begin_inset Formula $x$
\end_inset

 into distinct bins of width 
\begin_inset Formula $\Delta_{i}$
\end_inset

 and then count the number 
\begin_inset Formula $n_{i}$
\end_inset

 of observations of 
\begin_inset Formula $x$
\end_inset

 falling in bin 
\begin_inset Formula $i$
\end_inset

.
 The probability values for each bin is simply given by
\begin_inset Formula 
\[
p_{i}=\frac{n_{i}}{N\Delta_{i}}
\]

\end_inset

This gives a model for the density 
\begin_inset Formula $p(x)$
\end_inset

 that is constant over the width of each bin, and often the bins are chosen
 to have the same width 
\begin_inset Formula $\Delta_{i}=\Delta$
\end_inset

.
 The result of histogram density model is highly dependent on choosing the
 best value of 
\begin_inset Formula $\Delta$
\end_inset

, not too large or too small.
\end_layout

\begin_layout Standard
In practice, the histogram technique can be useful for obtaining a quick
 visualization of data in one or two dimensions but is unsuited to most
 density estimation applications.
\end_layout

\begin_layout Standard
The histogram approach to density estimation does, however, teach us two
 important lessons.
 First, to estimate the probability density at a particular location, we
 should consider the data points that lie within some local neighborhood
 of that point.
 For histograms, this neighborhood property was defined by the bins, and
 there is a natural `smoothing' parameter describing the spatial extent
 of the local region, in this case the bin width.
 Second, the value of the smoothing parameter should be neither too large
 nor too small in order to obtain good results.
\end_layout

\begin_layout Subsection
Kernel density estimators
\end_layout

\begin_layout Standard
Let us suppose observations are being drawn from some unknown probability
 density 
\begin_inset Formula $p({\bf x})$
\end_inset

 in Euclidean 
\begin_inset Formula $D$
\end_inset

-dimensional space, and we wish to estimate the value of 
\begin_inset Formula $p({\bf x})$
\end_inset

.
 From our earlier discussion of locality, let us consider some small region
 
\begin_inset Formula ${\cal R}$
\end_inset

 containing 
\begin_inset Formula ${\bf x}$
\end_inset

.
 The probability mass associated with this region is given by
\begin_inset Formula 
\[
P=\int_{{\cal R}}p({\bf x})\d{\bf x}
\]

\end_inset

 Now suppose that we have collected a data set comprising 
\begin_inset Formula $N$
\end_inset

 observations drawn from 
\begin_inset Formula $p({\bf x})$
\end_inset

.
 Because each data point has a probability 
\begin_inset Formula $P$
\end_inset

 of falling within 
\begin_inset Formula ${\cal R}$
\end_inset

, the total number 
\begin_inset Formula $K$
\end_inset

 of points that lie inside 
\begin_inset Formula ${\cal R}$
\end_inset

 will be distributed according to the binomial distribution
\begin_inset Formula 
\[
\Bin(K|N,P)=\frac{N!}{K!(N-K)!}P^{K}(1-P)^{1-K}
\]

\end_inset

Using 
\begin_inset Formula $ $
\end_inset


\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:E[m] for binomial"

\end_inset

, we see that the mean fraction of points falling inside the region is 
\begin_inset Formula $\EE[K/N]=P$
\end_inset

, and similarly using 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:var[m] for binomial"

\end_inset

 the variance around this mean is 
\begin_inset Formula $\var[K/N]=P(1-P)/N$
\end_inset

.
 For large 
\begin_inset Formula $N$
\end_inset

, this distribution will be sharply peaked around the mean and so
\begin_inset Formula 
\begin{equation}
K\simeq NP\label{eq:K simeq NP}
\end{equation}

\end_inset

If, however, we also assume that the region 
\begin_inset Formula ${\cal R}$
\end_inset

 is sufficiently small that the probability density 
\begin_inset Formula $p({\bf x})$
\end_inset

 is roughly constant over the region, then we have
\begin_inset Formula 
\begin{equation}
P\simeq p({\bf x})V\label{eq:P simeq p(x)V}
\end{equation}

\end_inset

when 
\begin_inset Formula $V$
\end_inset

 is the volume of 
\begin_inset Formula ${\cal R}$
\end_inset

.
 Combining 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:K simeq NP"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:P simeq p(x)V"

\end_inset

, we obtain density estimate in the form
\begin_inset Formula 
\begin{equation}
p({\bf x})=\frac{K}{NV}\label{eq:p(x)=K/NV}
\end{equation}

\end_inset

Note that the validity of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x)=K/NV"

\end_inset

 depends on two contradictory assumptions, namely that the region 
\begin_inset Formula ${\cal R}$
\end_inset

 be sufficiently small that the density is approximately constantly over
 the region and yet sufficiently large (in relation to the value of that
 density) that the number 
\begin_inset Formula $K$
\end_inset

 of points falling inside the region is sufficient for the binomial distribution
 to be sharply peaked.
\end_layout

\begin_layout Standard
We can exploit the result 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x)=K/NV"

\end_inset

 in two different ways.
 Either we can fix 
\begin_inset Formula $K$
\end_inset

 and determine the value of 
\begin_inset Formula $V$
\end_inset

 from the data, which gives rise to the 
\begin_inset Formula $K$
\end_inset

nearest-neighbor technique discussed shortly, or we can fix 
\begin_inset Formula $V$
\end_inset

 and determine 
\begin_inset Formula $K$
\end_inset

 from the data, giving rise to the kernel approach.
\end_layout

\begin_layout Standard
We begin by discussing the kernel method in detail, and to start with we
 take the region 
\begin_inset Formula ${\cal R}$
\end_inset

 to be a small hypercube centered on the point 
\begin_inset Formula ${\bf x}$
\end_inset

 at which we wish to determine the probability density.
 In order to count the number 
\begin_inset Formula $K$
\end_inset

 of points falling within the region, it is convenient to define the following
 function
\begin_inset Formula 
\begin{equation}
k({\bf u})=\begin{cases}
1, & |u_{i}|\leq1/2,\quad i=1,\ldots,D\\
0, & \text{otherwise}
\end{cases}\label{eq:k(u)=1 or 0}
\end{equation}

\end_inset

which represents a unit cube centered on the origin.
 The function 
\begin_inset Formula $k({\bf u})$
\end_inset

 is an example of a 
\emph on
kernel function
\emph default
, and in this context is also called a 
\emph on
Parzen window
\emph default
.
 From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:k(u)=1 or 0"

\end_inset

, the total number of data points lying inside this cube of side 
\begin_inset Formula $h$
\end_inset

 will be
\begin_inset Formula 
\[
K=\sum_{n=1}^{N}k\left(\frac{{\bf x}-{\bf x}_{n}}{h}\right)
\]

\end_inset

Substituting this expression into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x)=K/NV"

\end_inset

 then gives the following result for the estimated density at 
\begin_inset Formula ${\bf x}$
\end_inset


\begin_inset Formula 
\begin{equation}
p({\bf x})=\frac{1}{N}\sum_{n=1}^{N}\frac{1}{h^{D}}k\left(\frac{{\bf x}-{\bf x}_{n}}{h}\right)\label{eq:p(x) using a kernel function}
\end{equation}

\end_inset

where we have used 
\begin_inset Formula $V=h^{D}$
\end_inset

.
 Using the symmetry of the function 
\begin_inset Formula $k({\bf u})$
\end_inset

, we can now re-interpret this equation, not as a single cube centered on
 
\begin_inset Formula ${\bf x}$
\end_inset

 but as the sum over 
\begin_inset Formula $N$
\end_inset

 cubes centered on the 
\begin_inset Formula $N$
\end_inset

 data points 
\begin_inset Formula ${\bf x}_{n}$
\end_inset

.
\end_layout

\begin_layout Standard
The kernel density estimator 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x) using a kernel function"

\end_inset

 will suffer from one of the same problems that the histogram method suffered
 from, namely the presence of artificial discontinuities, in this case at
 the boundaries of the cubes.
 We can obtain a smoother density model if we choose a smoother kernel function,
 and a common choice is the Gaussian, which gives rise to the following
 kernel density model
\begin_inset Formula 
\[
p({\bf x})=\frac{1}{N}\sum_{n=1}^{N}\frac{1}{(2\pi h^{2})^{1/2}}\exp\left\{ -\frac{\left\Vert {\bf x}-{\bf x}_{n}\right\Vert ^{2}}{2h^{2}}\right\} 
\]

\end_inset

where 
\begin_inset Formula $h$
\end_inset

 represents the standard deviation of the Gaussian components.
 Thus our density model is obtained by placing a Gaussian over each data
 point and then adding up the contributions over the whole data set, and
 then dividing by 
\begin_inset Formula $N$
\end_inset

 so that the density is correctly normalized.
\end_layout

\begin_layout Standard
We can choose any other other kernel function 
\begin_inset Formula $k({\bf u})$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x) using a kernel function"

\end_inset

 subject to the conditions
\begin_inset Formula 
\begin{eqnarray}
k({\bf u}) & \geq & 0\\
\int k({\bf u})\d{\bf u} & = & 1
\end{eqnarray}

\end_inset

which ensure that the resulting probability distribution is nonnegative
 everywhere and integrates to one.
 The class of density model given by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x) using a kernel function"

\end_inset

 is called a kernel density estimator, or 
\emph on
Parzen
\emph default
 estimator.
 It has a great merit that there is no computation involved in the `training'
 phase because this simply requires storage of the training set.
 However, this is also one of its great weaknesses because the computational
 cost of evaluating the density grows linearly with the size of the data
 set.
\end_layout

\begin_layout Subsection
Nearest-neighbor methods
\end_layout

\begin_layout Standard
One of the difficulties with the kernel approach to density estimation is
 that the parameter 
\begin_inset Formula $h$
\end_inset

 governing the kernel width is fixed for all kernels.
 In regions of high data density, a large value of 
\begin_inset Formula $h$
\end_inset

 may lead to over-smoothing and a washing out of structure that might otherwise
 be extracted from the data.
 However, reducing 
\begin_inset Formula $h$
\end_inset

 may lead to noisy estimates elsewhere in data space where the density is
 smaller.
 Thus the optimal choice for 
\begin_inset Formula $h$
\end_inset

 may be dependent on location within the data space.
 This issue is addressed by nearest-neighbor methods for density estimation.
\end_layout

\begin_layout Standard
We therefore return to our general result 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x)=K/NV"

\end_inset

 for local density estimation, and instead of fixing 
\begin_inset Formula $V$
\end_inset

 and determining the value of 
\begin_inset Formula $K$
\end_inset

 from the data, we consider a fixed value of 
\begin_inset Formula $K$
\end_inset

 and use the data to find an appropriate value of 
\begin_inset Formula $V$
\end_inset

.
 To do this, we consider a small sphere centered on the point 
\begin_inset Formula ${\bf x}$
\end_inset

 at which we wish to estimate the density 
\begin_inset Formula $p({\bf x})$
\end_inset

, and we allow the radius of the sphere to grow until it contains precisely
 
\begin_inset Formula $K$
\end_inset

 data points.
 The estimate of the density 
\begin_inset Formula $p({\bf x})$
\end_inset

 is then given by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x)=K/NV"

\end_inset

 with 
\begin_inset Formula $V$
\end_inset

 set to the volume of the resulting sphere.
 This technique is known as 
\emph on

\begin_inset Formula $K$
\end_inset

 nearest neighbors
\emph default
.
 We see that the value of 
\begin_inset Formula $K$
\end_inset

 now governs the degree of smoothing and that again there is an optimum
 choice for 
\begin_inset Formula $K$
\end_inset

 that is neither too large nor too small.
 Note that the model produced by 
\begin_inset Formula $K$
\end_inset

 nearest neighbors is not a true density model because the integral over
 all space diverges.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $K$
\end_inset

-nearest-neighbor technique for density estimation can be extended to the
 problem of classification.
 To do this, we apply the 
\begin_inset Formula $K$
\end_inset

-nearest-neighbor density estimation technique to each class separately
 and then make use of Bayesâ€™ theorem.
 Suppose that we have a data set comprising 
\begin_inset Formula $N_{k}$
\end_inset

 point in class 
\begin_inset Formula ${\cal C}_{k}$
\end_inset

 with 
\begin_inset Formula $N$
\end_inset

 points in total.
 If we wish to classify a new point 
\begin_inset Formula ${\bf x}$
\end_inset

, we draw a sphere centered on 
\begin_inset Formula ${\bf x}$
\end_inset

 containing precisely 
\begin_inset Formula $K$
\end_inset

 points irrespective of their class.
 Suppose this sphere has volume 
\begin_inset Formula $V$
\end_inset

 and contains 
\begin_inset Formula $K_{k}$
\end_inset

 points from class 
\begin_inset Formula ${\cal C}_{k}$
\end_inset

.
 Then 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x)=K/NV"

\end_inset

 provides an estimate of the density associated with each class
\begin_inset Formula 
\[
p({\bf x}|{\cal C}_{k})=\frac{K_{k}}{N_{k}V}
\]

\end_inset

Combining with the unconditional density 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x)=K/NV"

\end_inset

, and the class priors are given by
\begin_inset Formula 
\[
p({\cal C}_{k})=\frac{N_{k}}{N}
\]

\end_inset

we can use Bayes' theorem to obtain the posterior probability of class membershi
p
\begin_inset Formula 
\[
p({\cal C}_{k}|{\bf x})=\frac{p({\bf x}|{\cal C}_{k})p({\cal C}_{k})}{p({\bf x})}=\frac{K_{k}}{K}
\]

\end_inset


\begin_inset Formula $K$
\end_inset

 controls the degree of smoothing, so that small 
\begin_inset Formula $K$
\end_inset

 produces many small regions of each class, whereas large 
\begin_inset Formula $K$
\end_inset

 leads to fewer larger regions.
 The particular case of 
\begin_inset Formula $K=1$
\end_inset

 is called the nearest-neighbor rule, because a test point is simply assigned
 to the same class as the nearest point from the training set.
\end_layout

\begin_layout Standard
As discussed so far, both the 
\begin_inset Formula $K$
\end_inset

-nearest-neighbor method, and the kernel density estimator, require the
 entire training data set to be stored, leading to expensive computation
 if the data set is large.
 This effect can be offset, at the expense of some additional one-off computatio
n, by constructing tree-based search structures to allow (approximate) near
 neighbors to be found efficiently without doing an exhaustive search of
 the data set.
 Nevertheless, these nonparametric methods are still severely limited.
 On the other hand, we have seen that simple parametric models are very
 restricted in terms of the forms of distribution that they can represent.
 We therefore need to find density models that are very flexible and yet
 for which the complexity of the models can be controlled independently
 of the size of the training set, and we shall see in subsequent chapters
 how to achieve this.
\end_layout

\end_body
\end_document
