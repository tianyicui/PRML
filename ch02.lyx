#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass scrbook
\use_default_options true
\master note.lyx
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset CommandInset include
LatexCommand input
filename "macros.lyx"

\end_inset


\end_layout

\begin_layout Chapter
Probability Distributions
\end_layout

\begin_layout Standard
In this chapter, we'll discuss different probability distributions.
 They're used as building blocks for more complex models, and to provide
 the opportunity to discuss some key statistical concepts.
\end_layout

\begin_layout Standard
The problem known as 
\emph on
density estimation
\emph default
 is to model the probability distribution 
\begin_inset Formula $p({\bf x})$
\end_inset

 of a random variable 
\begin_inset Formula ${\bf x}$
\end_inset

, given a finite set 
\begin_inset Formula ${\bf x}_{1},\ldots,{\bf x}_{N}$
\end_inset

 of observations.
 For the purpose of this chapter, we shall assume that the data points are
 i.i.d.
 It should be emphasized that the problem of density estimation is fundamentally
 ill-posed, because any distribution 
\begin_inset Formula $p({\bf x})$
\end_inset

 that is nonzero at each of the data points is a potential candidate.
 The issue of choosing an appropriate distribution relates to the problem
 of model selection and is a central issue in pattern recognition.
\end_layout

\begin_layout Standard
To apply 
\emph on
parametric
\emph default
 distributions to density estimation, we need a procedure for determining
 suitable values for the parameters, given an observed data set.
 In a frequentist treatment, we choose specific values for the parameters
 by optimizing some criterion, such as the likelihood function.
 In a Bayesian treatment we introduce prior distributions over the parameters
 and then use Bayesâ€™ theorem to compute the corresponding posterior distribution
 given the observed data.
\end_layout

\begin_layout Standard

\emph on
Conjugate
\emph default
 priors lead to posterior distributions having the same functional form
 as the priors, and therefore lead to a greatly simplified Bayesian analysis.
\end_layout

\begin_layout Standard
One limitation of the parametric approach is that it assumes a specific
 functional form for the distribution, which may turn out to be inappropriate
 for a particular application.
 An alternative approach is given by 
\emph on
nonparametric
\emph default
 density estimation methods in which the form of the distribution typically
 depends on the size of the data set.
 Such models still contain parameters, but these control the model complexity
 rather than the form of the distribution.
\end_layout

\begin_layout Section
Binary Variables
\end_layout

\begin_layout Standard
The distribution of single binary variable 
\begin_inset Formula $x\in\{0,1\}$
\end_inset

, with a single parameter 
\begin_inset Formula $\mu$
\end_inset

 given by 
\begin_inset Formula $p(x=1)=\mu$
\end_inset

, can be written in the form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Bern(x|\mu)=\mu^{x}(1-\mu)^{1-x}
\]

\end_inset

where 
\begin_inset Formula $0\leq\mu\leq1$
\end_inset

, It is known as the 
\emph on
Bernoulli
\emph default
 distribution.
 Its mean and variance are given by
\begin_inset Formula 
\begin{eqnarray}
\EE[x] & = & \mu\\
\var[x] & = & \mu(1-\mu)
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Suppose we have a data set 
\begin_inset Formula ${\cal D}=\{x_{1},\ldots,x_{N}\}$
\end_inset

 of observed values of 
\begin_inset Formula $x$
\end_inset

.
 By maximizing the likelihood function over 
\begin_inset Formula $\mu$
\end_inset

, we obtain the maximum likelihood estimator
\begin_inset Formula 
\begin{equation}
\mu_{{\rm ML}}=\frac{1}{N}\sum_{n=1}^{N}x_{n}\label{eq:mu_ML for Bern}
\end{equation}

\end_inset

 which is also known as the 
\emph on
sample mean
\emph default
.
 If we denote the number of observations of 
\begin_inset Formula $x=1$
\end_inset

 within the data set by 
\begin_inset Formula $m$
\end_inset

, then we can write 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:mu_ML for Bern"

\end_inset

 in the form
\begin_inset Formula 
\[
\mu_{{\rm ML}}=\frac{m}{N}
\]

\end_inset

so that the probability of landing heads is given, in this maximum likelihood
 framework, by the fraction of observations of heads in the data set.
\end_layout

\begin_layout Standard
If we flip a coin 
\begin_inset Formula $3$
\end_inset

 times and happen to observe 
\begin_inset Formula $3$
\end_inset

 heads.
 Then 
\begin_inset Formula $N=m=3$
\end_inset

 and 
\begin_inset Formula $\mu_{{\rm ML}}=1$
\end_inset

.
 In this case, the maximum likelihood result would predict that all future
 observations should give heads.
 This is an extreme example of the over-fitting associated with maximum
 likelihood.
 We shall see shortly how to arrive at more sensible conclusions through
 the introduction of a prior distribution over 
\begin_inset Formula $\mu$
\end_inset

.
\end_layout

\begin_layout Standard
We can also work out the distribution of the number 
\begin_inset Formula $m$
\end_inset

 of observations of 
\begin_inset Formula $x=1$
\end_inset

, in a data set which has size 
\begin_inset Formula $N$
\end_inset

.
 This is called the 
\emph on
binomial
\emph default
 distribution
\begin_inset Formula 
\begin{equation}
\Bin(m|N,\mu)=\binom{N}{m}\mu^{m}(1-\mu)^{N-m}\label{eq:Bin(m|N,mu)}
\end{equation}

\end_inset

which has mean and variance
\begin_inset Formula 
\begin{eqnarray}
\EE[m] & = & N_{\mu}\\
\var[m] & = & N\mu(1-\mu)
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Subsection
The beta distribution
\end_layout

\begin_layout Standard
Here we consider a form of prior distribution of the parameter 
\begin_inset Formula $\mu$
\end_inset

 in the Bernoulli distribution, which has a simple interpretation as well
 as some useful analytical properties.
 To motivate this prior, we note that the likelihood function takes the
 form of the product of factors of the form 
\begin_inset Formula $\mu^{x}(1-\mu)^{1-x}$
\end_inset

.
 If we choose a prior to be proportional to powers of 
\begin_inset Formula $\mu$
\end_inset

 and 
\begin_inset Formula $(1-\mu)$
\end_inset

, then the posterior distribution will have the same functional form as
 the prior.
 This property is called 
\emph on
conjugacy
\emph default
.
 We therefore choose a prior, called the 
\emph on
beta
\emph default
 distribution, given by
\begin_inset Formula 
\begin{equation}
\Beta[\mu|a,b]=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\label{eq:Beta[mu|a,b]}
\end{equation}

\end_inset

where 
\begin_inset Formula $\Gamma(x)$
\end_inset

 is the gamma function defined by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Gamma(x)\equiv\int_{0}^{\infty}u^{x-1}e^{-u}\d u
\]

\end_inset

and the coefficients in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Beta[mu|a,b]"

\end_inset

 ensures that the beta distribution is normalized.
\end_layout

\begin_layout Standard
The mean and variance of the beta distribution are given by
\begin_inset Formula 
\begin{eqnarray}
\EE[\mu] & = & \frac{a}{a+b}\label{eq:E[mu] for beta}\\
\var[\mu] & = & \frac{ab}{(a+b)^{2}(a+b+1)}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
The parameters 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 are often called 
\emph on
hyperparameters
\emph default
 because they control the distribution of the parameter 
\begin_inset Formula $\mu$
\end_inset

.
\end_layout

\begin_layout Standard
The posterior distribution of 
\begin_inset Formula $\mu$
\end_inset

 is now obtained by multiplying the beta prior 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Beta[mu|a,b]"

\end_inset

 by the binomial likelihood function 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Bin(m|N,mu)"

\end_inset

 and normalizing.
 Keep only the factors that depend on 
\begin_inset Formula $\mu$
\end_inset

, we see that this posterior distribution has the form
\begin_inset Formula 
\begin{equation}
p(\mu|m,l,a,b)\propto\mu^{m+a-1}(1-\mu)^{l+b-1}\label{eq:p(mu|m,l,a,b)}
\end{equation}

\end_inset

where 
\begin_inset Formula $l=N-m$
\end_inset

.
 We see thats 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(mu|m,l,a,b)"

\end_inset

 has the same functional dependence on 
\begin_inset Formula $\mu$
\end_inset

 as the prior distribution, reflecting the conjugacy properties of the prior
 with respect to the likelihood function.
 Indeed, it is simply another beta distribution
\begin_inset Formula 
\begin{equation}
p(\mu|m,l,a,b)=\frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)}\mu^{m+a-1}(1-\mu)^{l+b-1}\label{eq:p(mu|m,l,a,b)=}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This allows us to provide a simple interpretation of the hyperparameters
 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 in the prior as an 
\emph on
effective number of observations
\emph default
 of 
\begin_inset Formula $x=1$
\end_inset

 and 
\begin_inset Formula $x=0$
\end_inset

, respectively.
 Note that 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 need not be integers.
 Furthermore, the posterior distribution can act as the prior if we subsequently
 observe additional data.
 An additional observation of 
\begin_inset Formula $x=1$
\end_inset

 simply corresponds to incrementing the value of a by 
\begin_inset Formula $1$
\end_inset

, whereas for an observation of 
\begin_inset Formula $x=0$
\end_inset

 we increment 
\begin_inset Formula $b$
\end_inset

 by 
\begin_inset Formula $1$
\end_inset

.
\end_layout

\begin_layout Standard
We see that this 
\emph on
sequential
\emph default
 approach to learning arises naturally when we adopt a Bayesian viewpoint.
 It is independent of the choice of prior and of the likelihood function
 and depends only on the assumption of i.i.d.
 data.
 Sequential methods make use of observations one at a time, or in small
 batches, and then discard them before the next observations are used.
 They can be used, for example, in real-time learning scenarios where a
 steady stream of data is arriving, and predictions must be made before
 all of the data is seen.
 Because they do not require the whole data set to be stored or loaded into
 memory, sequential methods are also useful for large data sets.
 Maximum likelihood methods can also be cast into a sequential framework.
\end_layout

\begin_layout Standard
Given the observed data set 
\begin_inset Formula ${\cal D}$
\end_inset

, we can predict the next 
\begin_inset Formula $x$
\end_inset

 by the form
\begin_inset Formula 
\[
p(x=1|{\cal D})=\int_{0}^{1}p(x=1|\mu)p(\mu|{\cal D})\d\mu=\int_{0}^{1}\mu\, p(\mu|{\cal D})\d\mu=\EE[\mu|{\cal D}]
\]

\end_inset

Using result 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(mu|m,l,a,b)="

\end_inset

 together with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:E[mu] for beta"

\end_inset

, we obtain
\begin_inset Formula 
\[
p(x=1|{\cal D})=\frac{m+a}{m+a+l+b}
\]

\end_inset

which has a simple interpretation as the total fraction of observations
 (both real observations and fictitious prior observations).
\end_layout

\begin_layout Standard
As the number of observations increases, the posterior distribution becomes
 more sharply peaked, in which the variance goes to zero for 
\begin_inset Formula $a\to\infty$
\end_inset

 or 
\begin_inset Formula $b\to\infty$
\end_inset

.
 In fact, we might wonder whether it is a general property of Bayesian learning
 that, as we observe more and more data, the uncertainty represented by
 the posterior distribution will steadily decrease.
\end_layout

\begin_layout Standard
To address this, we can take a frequentist view of Bayesian learning and
 show that, on average, such a property does indeed hold.
 Consider a general Bayesian inference problem for a parameter 
\begin_inset Formula $\theta$
\end_inset

 for which we have observed a data set 
\begin_inset Formula ${\cal D}$
\end_inset

, described by the joint distribution 
\begin_inset Formula $p(\theta,{\cal D})$
\end_inset

.
 The following result
\begin_inset Formula 
\[
\EE_{\bm{\theta}}[\bm{\theta}]=\EE_{{\cal D}}[\EE_{\bm{\theta}}[\bm{\theta}|{\cal D}]]
\]

\end_inset

where
\begin_inset Formula 
\begin{eqnarray}
\EE_{\bm{\theta}}[\bm{\theta}] & \equiv & \int p(\bm{\theta})\bm{\theta}\d\bm{\theta}\\
\EE_{{\cal D}}[\EE_{\bm{\theta}}[\bm{\theta}|{\cal D}]] & \equiv & \int\left\{ \int\bm{\theta}\, p({\bf \bm{\theta}|{\cal D})\d\bm{\theta}}\right\} p({\cal D})\d{\cal D}
\end{eqnarray}

\end_inset

says that the posterior mean of 
\begin_inset Formula $\bm{\theta}$
\end_inset

, averaged over the distribution generating the data, is equal to the prior
 mean of 
\begin_inset Formula $\bm{\theta}$
\end_inset

.
 Similarly, we can show that
\begin_inset Formula 
\begin{equation}
\var_{\bm{\theta}}[\bm{\theta}]=\EE_{{\cal D}}[\var_{\bm{\theta}}[\bm{\theta}|{\cal D}]]+\var_{{\cal D}}[\EE_{\bm{\theta}}[\bm{\theta}|{\cal D}]]\label{eq:E_theta[theta]==}
\end{equation}

\end_inset

The term on the left-hand side of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:E_theta[theta]=="

\end_inset

 is the prior variance of 
\begin_inset Formula $\bm{\theta}$
\end_inset

.
 On the right-hand side, the first term is the average posterior variance
 of 
\begin_inset Formula $\bm{\theta}$
\end_inset

, and the second term measures the variance in the posterior mean of 
\begin_inset Formula $\bm{\theta}$
\end_inset

.
 Because this variance is a positive quantity, this result shows that, on
 average, the posterior variance of 
\begin_inset Formula $\bm{\theta}$
\end_inset

 is smaller than the prior variance.
 The reduction in variance is greater if the variance in the posterior mean
 is greater.
 Note, however, that this result only holds on average, and that for a particula
r observed data set it is possible for the posterior variance to be larger
 than the prior variance.
\end_layout

\begin_layout Section
Multinomial Variables
\end_layout

\begin_layout Standard
Often, we encounter discrete variables that can take on one of 
\begin_inset Formula $K$
\end_inset

 possible mutually exclusive states.
 One particularly convenient representation to express such variables is
 the 1-of-
\begin_inset Formula $K$
\end_inset

 scheme, in which, the variable is represented by a 
\begin_inset Formula $K$
\end_inset

-dimensional vector 
\begin_inset Formula ${\bf x}$
\end_inset

 in which one of the elements 
\begin_inset Formula $x_{k}$
\end_inset

 equals 
\begin_inset Formula $1$
\end_inset

, and all remaining elements equal 
\begin_inset Formula $0$
\end_inset

.
 Note that such vectors satisfy 
\begin_inset Formula $\sum_{k=1}^{K}x_{k}=1$
\end_inset

.
 If we denote the probability of 
\begin_inset Formula $x_{k}=1$
\end_inset

 by the parameter 
\begin_inset Formula $\mu_{k}$
\end_inset

, then the distribution of 
\begin_inset Formula ${\bf x}$
\end_inset

 is given by
\begin_inset Formula 
\begin{equation}
p({\bf x}|\bm{\mu})=\prod_{k=1}^{K}\mu_{k}^{x_{k}}\label{eq:p(x|mu)}
\end{equation}

\end_inset

where 
\begin_inset Formula $\bm{\mu}=(\mu_{1},\ldots,\mu_{K})\trans$
\end_inset

, and the parameters 
\begin_inset Formula $\mu_{k}$
\end_inset

 are constrained to satisfy 
\begin_inset Formula $\mu_{k}\geq0$
\end_inset

 and 
\begin_inset Formula $\sum_{k}\mu_{k}=1$
\end_inset

.
 The distribution 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(x|mu)"

\end_inset

 can be regarded as a generalization of the Bernoulli distribution to more
 than two outcomes.
 Its mean is given by
\begin_inset Formula 
\[
\EE[{\bf x}|\bm{\mu}]=\sum_{x}p({\bf x}|\bm{\mu}){\bf x}=(\mu_{1},\ldots,\mu_{K})\trans=\bm{\mu}
\]

\end_inset


\end_layout

\begin_layout Standard
Now consider a data set 
\begin_inset Formula ${\cal D}$
\end_inset

 of 
\begin_inset Formula $N$
\end_inset

 independent observations 
\begin_inset Formula ${\bf x}_{1},\ldots,{\bf x}_{N}$
\end_inset

.
 The corresponding likelihood function takes the form
\begin_inset Formula 
\begin{equation}
p({\cal D}|\bm{\mu})=\prod_{n=1}^{N}\prod_{k=1}^{K}\mu_{k}^{x_{nk}}=\prod_{k=1}^{K}\mu_{k}^{(\sum_{n}x_{nk})}=\prod_{k=1}^{K}\mu_{k}^{m_{k}}\label{eq:p(D|mu)}
\end{equation}

\end_inset

which depends on the 
\begin_inset Formula $N$
\end_inset

 data points only through the 
\begin_inset Formula $K$
\end_inset

 quantities
\begin_inset Formula 
\[
m_{k}=\sum_{n}x_{nk}
\]

\end_inset

which represents the number of observations of 
\begin_inset Formula $x_{k}=1$
\end_inset

.
 These are called the 
\emph on
sufficient statistics
\emph default
 for this distribution.
\end_layout

\begin_layout Standard
The maximum likelihood solution for 
\begin_inset Formula $\bm{\mu}$
\end_inset

 taking account of its constraint, which can be solved using Lagrange multiplier
, is in the form
\begin_inset Formula 
\[
\mu_{k}^{{\rm ML}}=\frac{m_{k}}{N}
\]

\end_inset

which is the fraction of the 
\begin_inset Formula $N$
\end_inset

 observations for which 
\begin_inset Formula $x_{k}=1$
\end_inset

.
\end_layout

\begin_layout Standard
We can consider the joint distribution of the quantities 
\begin_inset Formula $m_{1},\ldots,m_{K}$
\end_inset

, conditioned on the parameters 
\begin_inset Formula $\bm{\mu}$
\end_inset

 and on the total number 
\begin_inset Formula $N$
\end_inset

 of observations.
 From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:p(D|mu)"

\end_inset

 this takes the form
\begin_inset Formula 
\begin{equation}
\Mult(m_{1},m_{2},\ldots,m_{K}|\bm{\mu},N)=\binom{N}{m_{1}\: m_{2}\:\ldots\: m_{K}}\prod_{k=1}^{K}\mu_{k}^{m_{k}}\label{eq:Mult(m_1...m_K|mu,N)}
\end{equation}

\end_inset

which is known as the 
\emph on
multinomial
\emph default
 distribution.
 Note that the variables 
\begin_inset Formula $m_{k}$
\end_inset

 are subject to the constraint
\begin_inset Formula 
\[
\sum_{k=1}^{K}m_{k}=N
\]

\end_inset


\end_layout

\begin_layout Subsection
The Dirichlet distribution
\end_layout

\begin_layout Standard
By inspecting the form of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Mult(m_1...m_K|mu,N)"

\end_inset

, we see that the conjugate prior is given by
\begin_inset Formula 
\[
p(\bm{\mu}|\bm{\alpha})\propto\prod_{k=1}^{K}\mu_{k}^{\alpha_{k}-1}
\]

\end_inset

where 
\begin_inset Formula $0\leq\mu_{k}\leq1$
\end_inset

 and 
\begin_inset Formula $\sum_{k}\mu_{k}=1$
\end_inset

.
 Here 
\begin_inset Formula $\bm{\alpha}=(\alpha_{1},\ldots,\alpha_{K})\trans$
\end_inset

 are the parameters of the distribution.
 Note that, because of the summation constraint, the distribution over the
 space of the 
\begin_inset Formula $\{\mu_{k}\}$
\end_inset

 is confined to a 
\emph on
simplex
\emph default
 of dimensionality 
\begin_inset Formula $K-1$
\end_inset

.
\end_layout

\begin_layout Standard
The normalized form for this distribution is by
\begin_inset Formula 
\begin{equation}
\Dir(\bm{\mu}|\bm{\alpha})=\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{1})\ldots\Gamma(\alpha_{K})}\prod_{k=1}^{K}\mu_{k}^{\alpha_{k}-1}\label{eq:Dir(mu|alpha)}
\end{equation}

\end_inset

which is called the 
\emph on
Dirichlet
\emph default
 distribution, here 
\begin_inset Formula $\alpha_{0}=\sum_{k=1}^{N}\alpha_{K}$
\end_inset

.
\end_layout

\begin_layout Standard
Multiplying the prior 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Dir(mu|alpha)"

\end_inset

 by the likelihood function 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Mult(m_1...m_K|mu,N)"

\end_inset

, we obtain the posterior distribution for the parameters 
\begin_inset Formula $\{\mu_{k}\}$
\end_inset

 in the form
\begin_inset Formula 
\[
p(\bm{\mu}|{\cal D},\bm{\alpha})=\Dir(\bm{\mu}|\bm{\alpha}+{\bf m})
\]

\end_inset

where we have denoted 
\begin_inset Formula ${\bf m}=(m_{1},\ldots,m_{K})\trans$
\end_inset

.
\end_layout

\begin_layout Section
The Gaussian Distribution
\end_layout

\begin_layout Standard
The Gaussian, also known as the normal distribution, is a widely used model
 for the distribution of continuous variables.
 In the case of a single variable x, the Gaussian distribution can be written
 in the form
\begin_inset Formula 
\[
{\cal N}(x|\mu,\sigma^{2})=\frac{1}{(2\pi\sigma^{2})^{1/2}}\exp\left\{ -\frac{1}{2\sigma^{2}}(x-\mu)^{2}\right\} 
\]

\end_inset

where 
\begin_inset Formula $\mu$
\end_inset

 is the mean and 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is the variance.
 For a 
\begin_inset Formula $D$
\end_inset

-dimensional vector 
\begin_inset Formula ${\bf x}$
\end_inset

, the multivariate Gaussian distribution takes the form
\begin_inset Formula 
\[
{\cal N}({\bf x}|\bm{\mu},\bm{\Sigma})=\frac{1}{(2\pi)^{D/2}}\frac{1}{\left|\bm{\Sigma}\right|^{1/2}}\exp\left\{ -\frac{1}{2}({\bf x}-\bm{\mu})\trans\bm{\Sigma}^{-1}({\bf x}-\bm{\mu})\right\} 
\]

\end_inset

 where 
\begin_inset Formula $\bm{\mu}$
\end_inset

 is a 
\begin_inset Formula $D$
\end_inset

-dimensional mean vector, 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 is a 
\begin_inset Formula $D\times D$
\end_inset

 covariance matrix, and 
\begin_inset Formula $\left|\bm{\Sigma}\right|$
\end_inset

 denotes the determinant of 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

.
\end_layout

\begin_layout Standard
The Gaussian distribution arises in many different contexts and can be motivated
 from a variety of different perspectives.
 For example, we have already seen that for a single real variable, the
 distribution that maximizes the entropy is the Gaussian.
 This property applies also to the multivariate Gaussian.
\end_layout

\begin_layout Standard
Another situation in which the Gaussian distribution arises is when we consider
 the sum of multiple random variables.
 The 
\emph on
central limit theorem
\emph default
 tells us that, subject to certain mild conditions, the sum of a set of
 random variables, which is of course itself a random variable, has a distributi
on that becomes increasingly Gaussian as the number of terms in the sum
 increases.
\end_layout

\begin_layout Standard
The Gaussian distribution has many important analytical properties, and
 we shall consider several of these in detail.
 We begin by considering the geometrical form of the Gaussian distribution.
 The functional dependence of the Gaussian on 
\begin_inset Formula ${\bf x}$
\end_inset

 is through the quadratic form
\begin_inset Formula 
\begin{equation}
\Delta^{2}=({\bf x}-\bm{\mu})\trans\bm{\Sigma}^{-1}({\bf x}-\bm{\mu})\label{eq:Delta^2 in Gaussian}
\end{equation}

\end_inset

which appears in the exponent.
 The quantity 
\begin_inset Formula $\Delta$
\end_inset

 is called the 
\emph on
Mahalanobis distance
\emph default
 from 
\begin_inset Formula $\bm{\mu}$
\end_inset

 to 
\begin_inset Formula ${\bf x}$
\end_inset

 and reduces to the Euclidian distance when 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 is the identity matrix.
 The Gaussian distribution will be constant on surfaces in 
\begin_inset Formula ${\bf x}$
\end_inset

-space for which this quadratic form is constant.
\end_layout

\begin_layout Standard
First of all, we note that the matrix 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 can be taken to be symmetric, without loss of generality, because any antisymme
tric component would disappear from the exponent.
 Now consider the eigenvector equation for the covariance matrix
\begin_inset Formula 
\[
\bm{\Sigma}{\bf u}_{i}=\lambda_{i}{\bf u}_{i}
\]

\end_inset

where 
\begin_inset Formula $i=1,\ldots,D$
\end_inset

.
 Because 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 is a real, symmetric matrix its eigenvalues will be real, and its eigenvectors
 can be chosen to form an orthonormal set, so that
\begin_inset Formula 
\begin{equation}
{\bf u}_{i}\trans{\bf u}_{j}=I_{ij}\label{eq:u_i^T u_j = I_ij}
\end{equation}

\end_inset

The covariance matrix 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 can be expressed as an expansion in terms of its eigenvectors in the form
\begin_inset Formula 
\[
\bm{\Sigma}=\sum_{i=1}^{D}\lambda_{i}{\bf u}_{i}{\bf u}_{i}\trans
\]

\end_inset

 and similarly the inverse covariance matrix 
\begin_inset Formula $\bm{\Sigma}^{-1}$
\end_inset

 can be expressed as
\begin_inset Formula 
\begin{equation}
\bm{\Sigma}^{-1}=\sum_{i=1}^{D}\frac{1}{\lambda_{i}}{\bf u}_{i}{\bf u_{i}}\trans\label{eq:Sigma^-1 expressed using eigenvectors}
\end{equation}

\end_inset

Substituting 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Sigma^-1 expressed using eigenvectors"

\end_inset

 into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:Delta^2 in Gaussian"

\end_inset

, the quadratic form becomes
\begin_inset Formula 
\[
\Delta^{2}=\sum_{i=1}^{D}\frac{y_{i}^{2}}{\lambda_{i}}
\]

\end_inset

 where we have defined
\begin_inset Formula 
\begin{equation}
y_{i}={\bf u}_{i}\trans({\bf x}-\bm{\mu})\label{eq:y_i for Gaussian}
\end{equation}

\end_inset

 We can interpret 
\begin_inset Formula $\{y_{i}\}$
\end_inset

 as a new coordinate system defined by the orthonormal vectors 
\begin_inset Formula ${\bf u}_{i}$
\end_inset

 that are shifted and rotate with respect to the original 
\begin_inset Formula $x_{i}$
\end_inset

 coordinates.
 Forming the factor 
\begin_inset Formula ${\bf y}=(y_{1},\ldots,y_{D})\trans$
\end_inset

, we have
\begin_inset Formula 
\[
{\bf y}={\bf U}({\bf x}-\bm{\mu})
\]

\end_inset

where 
\begin_inset Formula ${\bf U}$
\end_inset

 is a matrix whose rows are given by 
\begin_inset Formula ${\bf u}_{i}\trans$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:u_i^T u_j = I_ij"

\end_inset

 
\begin_inset Formula ${\bf U}$
\end_inset

 is an 
\emph on
orthogonal matrix
\emph default
, i.e.
 
\begin_inset Formula ${\bf UU}\trans={\bf U}\trans{\bf U}={\bf I}$
\end_inset

.
\end_layout

\begin_layout Standard
The quadratic form, and hence the Gaussian density, will be constant on
 surfaces for which 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:y_i for Gaussian"

\end_inset

 is constant.
 If all of the eigenvalues 
\begin_inset Formula $\lambda_{i}$
\end_inset

 are positive, then these surfaces represent ellipsoids, with there centers
 at 
\begin_inset Formula $\bm{\mu}$
\end_inset

 and their axes oriented along 
\begin_inset Formula ${\bf u}_{i}$
\end_inset

, and with scaling factors in the directions of the axes given by 
\begin_inset Formula $\lambda_{i}^{1/2}$
\end_inset

.
\end_layout

\begin_layout Standard
For the Gaussian distribution to be well defined, it is necessary for all
 of the eigenvalues 
\begin_inset Formula $\lambda_{i}$
\end_inset

 of the covariance matrix to be strictly positive, i.e.
 
\begin_inset Formula $\bm{\Sigma}$
\end_inset

 to be positive definite, otherwise the distribution cannot be properly
 normalized.
 In Chapter 12, we will encounter Gaussian distributions for which one or
 more of the eigenvalues are zero, in which case the distribution is singular
 and is confined to a subspace of lower dimensionality.
\end_layout

\begin_layout Standard
Now consider the form of the Gaussian distribution in the new coordinate
 system defined by the 
\begin_inset Formula $y_{i}$
\end_inset

.
 In going from the 
\begin_inset Formula ${\bf x}$
\end_inset

 to the 
\begin_inset Formula ${\bf y}$
\end_inset

 coordinate system, we have a Jacobian matrix 
\begin_inset Formula ${\bf J}$
\end_inset

 with elements given by
\begin_inset Formula 
\[
J_{ij}=\frac{\partial x_{i}}{\partial y_{j}}=U_{ji}
\]

\end_inset

where 
\begin_inset Formula $U_{ji}$
\end_inset

 are the elements of the matrix 
\begin_inset Formula ${\bf U}\trans$
\end_inset

.
 Using the orthonormality property of the matrix 
\begin_inset Formula ${\bf U}$
\end_inset

, we see that the square of the determinant of the Jacobian matrix is
\begin_inset Formula 
\[
|{\bf J}|^{2}=|{\bf U}\trans|^{2}=|{\bf U}\trans||{\bf U}|=|{\bf U}\trans{\bf U}|=|{\bf I}|=1
\]

\end_inset

and hence 
\begin_inset Formula $|{\bf J}|=1$
\end_inset

.
 Also, the determinant 
\begin_inset Formula $|\bm{\Sigma}|$
\end_inset

 of the covariance matrix can be written as the product of its eigenvalues,
 and hence
\begin_inset Formula 
\[
|\bm{\Sigma}|^{1/2}=\prod_{j=1}^{D}\lambda_{j}^{1/2}
\]

\end_inset

Thus in the 
\begin_inset Formula $y_{j}$
\end_inset

 coordinate system, the Gaussian distribution takes the form
\begin_inset Formula 
\[
p({\bf y})=p({\bf x})|{\bf J}|=\prod_{j=1}^{D}\frac{1}{(2\pi\lambda_{j})^{1/2}}\exp\left\{ -\frac{y_{j}^{2}}{2\lambda_{j}}\right\} 
\]

\end_inset

which is the product of 
\begin_inset Formula $D$
\end_inset

 independent univariate Gaussian distributions.
 The eigenvectors therefore define a new set of shifted and rotated coordinates
 with respect to which the joint probability distribution factorizes into
 a product of independent distributions.
\end_layout

\begin_layout Standard
It can be proved
\begin_inset Formula 
\[
\EE[{\bf x}]=\bm{\mu}
\]

\end_inset

and
\begin_inset Formula 
\[
\EE[{\bf xx}\trans]=\bm{\mu\mu}\trans+\bm{\Sigma}
\]

\end_inset

Therefore
\begin_inset Formula 
\begin{eqnarray}
\cov[{\bf x}] & = & \EE\left[({\bf x}-\EE[{\bf x}])({\bf x}-\EE[{\bf x}])\trans\right]\nonumber \\
 & = & \bm{\Sigma}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Although the Gaussian distribution is widely used as a density model, it
 suffers from some significant limitations.
 Its number of parameters, 
\begin_inset Formula $D(D+3)/2$
\end_inset

, grows quadratically with 
\begin_inset Formula $D$
\end_inset

.
 With large 
\begin_inset Formula $D$
\end_inset

, the computational task of manipulating and inverting large matrices can
 become prohibitive.
 One way to address this problem is to use restricted forms of the covariance
 matrix.
 If we restrict 
\begin_inset Formula $\bm{\Sigma}=\diag(\sigma_{i}^{2})$
\end_inset

, then the total of independent parameters is 
\begin_inset Formula $2D$
\end_inset

 and the corresponding contours of constant density are given by axis-aligned
 ellipsoids.
 We could further restrict 
\begin_inset Formula $\bm{\Sigma}=\sigma^{2}{\bf I}$
\end_inset

, known as an 
\emph on
isotropic
\emph default
 covariance, giving 
\begin_inset Formula $D+1$
\end_inset

 independent parameters and spherical surfaces of constant density.
 Unfortunately,whereas such approaches limit the number of degrees of freedom
 in the distribution and make inversion of the covariance matrix a much
 faster operation, they also greatly restrict the form of the probability
 density and limit its ability to capture interesting correlations in the
 data.
\end_layout

\begin_layout Standard
A further limitation of the Gaussian distribution is that it is intrinsically
 unimodal (i.e., has a single maximum) and so is unable to provide a good
 approximation to multimodal distributions.
 Thus the Gaussian distribution can be both too flexible, in the sense of
 having too many parameters, while also being too limited in the range of
 distributions that it can adequately represent.
 We will see later that the introduction of 
\emph on
latent
\emph default
 variables, also called 
\emph on
hidden
\emph default
 variables or 
\emph on
unobserved
\emph default
 variables, allows both of these problems to be addressed.
\end_layout

\begin_layout Subsection
Conditional Gaussian distributions
\end_layout

\begin_layout Standard
Given a joint Gaussian distribution 
\begin_inset Formula ${\cal N}({\bf x}|\bm{\mu},\bm{\Sigma})$
\end_inset

 with 
\begin_inset Formula $\bm{\Lambda}\equiv\bm{\Sigma}^{-1}$
\end_inset

 and
\begin_inset Formula 
\[
{\bf x}=\begin{pmatrix}{\bf x}_{a}\\
{\bf x}_{b}
\end{pmatrix},\quad\bm{\mu}=\begin{pmatrix}\bm{\mu}_{a}\\
\bm{\mu}_{b}
\end{pmatrix}
\]

\end_inset


\begin_inset Formula 
\[
\bm{\Sigma}=\begin{pmatrix}\bm{\Sigma}_{aa} & \bm{\Sigma}_{ab}\\
\bm{\Sigma}_{ba} & \bm{\Sigma}_{bb}
\end{pmatrix},\quad\bm{\Lambda}=\begin{pmatrix}\bm{\Lambda}_{aa} & \bm{\Lambda}_{ab}\\
\bm{\Lambda}_{ba} & \bm{\Lambda}_{bb}
\end{pmatrix}
\]

\end_inset

The conditional distribution 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $p({\bf x}_{a}|{\bf x}_{b})$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 is given by
\begin_inset Formula 
\begin{eqnarray}
p({\bf x}_{a}|{\bf x}_{b}) & = & {\cal N}({\bf x}|\bm{\mu}_{a|b},\bm{\Lambda}_{aa}^{-1})\\
\bm{\mu}_{a|b} & = & \bm{\mu}_{a}-\bm{\Lambda}_{aa}^{-1}\bm{\Lambda}_{ab}({\bf x}_{a}-{\bf x}_{b})
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Subsection
Marginal Gaussian distributions
\end_layout

\begin_layout Standard
Same conditions as above, the marginal distribution 
\begin_inset Formula $p({\bf x}_{a})$
\end_inset

 is given by
\begin_inset Formula 
\[
p({\bf x}_{a})={\cal N}({\bf x}_{a}|\bm{\mu}_{a},\bm{\Sigma}_{aa})
\]

\end_inset


\end_layout

\end_body
\end_document
